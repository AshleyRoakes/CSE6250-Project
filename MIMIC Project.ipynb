{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a331de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk import word_tokenize\n",
    "from nltk.internals import find_jars_within_path\n",
    "import nltk\n",
    "import sklearn.model_selection\n",
    "from collections import Counter\n",
    "import sklearn.feature_extraction.text as skt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pyhealth\n",
    "from pyhealth.medcode import InnerMap\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import gensim.models.word2vec as w2v\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import tensorflow as TF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c542936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#root = \"/Users/ashleyroakes/Desktop/\"\n",
    "root = \"./data/\"\n",
    "\n",
    "mim_root = root + \"mimic-iii-clinical-database-1.4/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b7b726",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "## Read in Discharge Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9078a11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of discharge summaries:  55177\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22532</td>\n",
       "      <td>167853.0</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2151-7-16**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13702</td>\n",
       "      <td>107527.0</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2118-6-2**]       Discharg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13702</td>\n",
       "      <td>167118.0</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2119-5-4**]              D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13702</td>\n",
       "      <td>196489.0</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2124-7-21**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26880</td>\n",
       "      <td>135453.0</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2162-3-3**]              D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID   HADM_ID           CATEGORY DESCRIPTION  \\\n",
       "0       22532  167853.0  Discharge summary      Report   \n",
       "1       13702  107527.0  Discharge summary      Report   \n",
       "2       13702  167118.0  Discharge summary      Report   \n",
       "3       13702  196489.0  Discharge summary      Report   \n",
       "4       26880  135453.0  Discharge summary      Report   \n",
       "\n",
       "                                                TEXT  \n",
       "0  Admission Date:  [**2151-7-16**]       Dischar...  \n",
       "1  Admission Date:  [**2118-6-2**]       Discharg...  \n",
       "2  Admission Date:  [**2119-5-4**]              D...  \n",
       "3  Admission Date:  [**2124-7-21**]              ...  \n",
       "4  Admission Date:  [**2162-3-3**]              D...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes = mim_root + \"NOTEEVENTS.csv.gz\"\n",
    "\n",
    "notes_df = pd.read_csv(notes, compression='gzip', error_bad_lines=False, \n",
    "                       usecols = ['SUBJECT_ID', 'HADM_ID', 'CATEGORY', 'DESCRIPTION','TEXT'])\\\n",
    "                      .query(\"CATEGORY == 'Discharge summary'\")\\\n",
    "                      .query(\"DESCRIPTION == 'Report'\")\n",
    "\n",
    "# Should be 55,177 records\n",
    "print(\"Number of discharge summaries: \", + len(notes_df))\n",
    "\n",
    "notes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757857a9",
   "metadata": {},
   "source": [
    "## Read in Patient Diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e16993f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>[25013, 3371, 5849, 5780, V5867, 25063, 5363, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>[53100, 2851, 07054, 5715, 45621, 53789, 4019,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100006</td>\n",
       "      <td>[49320, 51881, 486, 20300, 2761, 7850, 3090, V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100007</td>\n",
       "      <td>[56081, 5570, 9973, 486, 4019]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100009</td>\n",
       "      <td>[41401, 99604, 4142, 25000, 27800, V8535, 4148...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                          ICD9_CODE\n",
       "0   100001  [25013, 3371, 5849, 5780, V5867, 25063, 5363, ...\n",
       "1   100003  [53100, 2851, 07054, 5715, 45621, 53789, 4019,...\n",
       "2   100006  [49320, 51881, 486, 20300, 2761, 7850, 3090, V...\n",
       "3   100007                     [56081, 5570, 9973, 486, 4019]\n",
       "4   100009  [41401, 99604, 4142, 25000, 27800, V8535, 4148..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag = mim_root + \"DIAGNOSES_ICD.csv.gz\"\n",
    "\n",
    "diag_df = pd.read_csv(diag, compression='gzip', error_bad_lines=False)\\\n",
    "                    .dropna()\\\n",
    "                    .groupby('HADM_ID')['ICD9_CODE']\\\n",
    "                    .unique()\\\n",
    "                    .reset_index()\n",
    "\n",
    "diag_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5befa018",
   "metadata": {},
   "source": [
    "## Read in ICD9 Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e7e3472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>SHORT_TITLE</th>\n",
       "      <th>LONG_TITLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174</td>\n",
       "      <td>01166</td>\n",
       "      <td>TB pneumonia-oth test</td>\n",
       "      <td>Tuberculous pneumonia [any form], tubercle bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175</td>\n",
       "      <td>01170</td>\n",
       "      <td>TB pneumothorax-unspec</td>\n",
       "      <td>Tuberculous pneumothorax, unspecified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176</td>\n",
       "      <td>01171</td>\n",
       "      <td>TB pneumothorax-no exam</td>\n",
       "      <td>Tuberculous pneumothorax, bacteriological or h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>177</td>\n",
       "      <td>01172</td>\n",
       "      <td>TB pneumothorx-exam unkn</td>\n",
       "      <td>Tuberculous pneumothorax, bacteriological or h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>178</td>\n",
       "      <td>01173</td>\n",
       "      <td>TB pneumothorax-micro dx</td>\n",
       "      <td>Tuberculous pneumothorax, tubercle bacilli fou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROW_ID ICD9_CODE               SHORT_TITLE  \\\n",
       "0     174     01166     TB pneumonia-oth test   \n",
       "1     175     01170    TB pneumothorax-unspec   \n",
       "2     176     01171   TB pneumothorax-no exam   \n",
       "3     177     01172  TB pneumothorx-exam unkn   \n",
       "4     178     01173  TB pneumothorax-micro dx   \n",
       "\n",
       "                                          LONG_TITLE  \n",
       "0  Tuberculous pneumonia [any form], tubercle bac...  \n",
       "1              Tuberculous pneumothorax, unspecified  \n",
       "2  Tuberculous pneumothorax, bacteriological or h...  \n",
       "3  Tuberculous pneumothorax, bacteriological or h...  \n",
       "4  Tuberculous pneumothorax, tubercle bacilli fou...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icd = mim_root + \"D_ICD_DIAGNOSES.csv.gz\"\n",
    "icd_df = pd.read_csv(icd, compression='gzip', error_bad_lines=False)\n",
    "\n",
    "\n",
    "icd_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d7502",
   "metadata": {},
   "source": [
    "## Merge datasets by HADM_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb8a4e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55172"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(diag_df, notes_df, on='HADM_ID', how='inner')\n",
    "\n",
    "# Should be 55177-5 = 55172\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b193cbba",
   "metadata": {},
   "source": [
    "## Substitute special sequences & Filter HoPI sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cb0055",
   "metadata": {},
   "source": [
    "### Identify HOPI sections & Substitute Special Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f357cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_notes(st):\n",
    "    s  = \"History of Present Illness\"\n",
    "    s1 = \"HISTORY OF PRESENT ILLNESS:|HISTORY OF THE PRESENT ILLNESS:| \\\n",
    "          HISTORY OF PRESENT ILLNESS/CHIEF COMPLAINT:|HISTORY OF THE PRESENT ILLNESS/HOSPITAL COURSE:| \\\n",
    "          HISTORY OF THE PRESENT ILLNESS/HOSPITAL COURSE:|HISTORY OF PRESENT ILLNESS/HOSPITAL COURSE PRIOR TO TRANSFER:\"#|\\nHISTORY:\"\n",
    "\n",
    "    s2 = \"present illness:|Present Illness:|PRESENT ILLNESS:\"#|Chief Complaint:\"\n",
    "    \n",
    "    match  = re.search(s, st)\n",
    "    match1 = re.search(s1, st)\n",
    "    match2 = re.search(s2, st)\n",
    "    \n",
    "    if (match is not None) or (match1 is not None) or (match2 is not None):\n",
    "        if match is not None:\n",
    "            st = st.split(s, 1)[1]\n",
    "            e = \"\\n\\n\\n\"\n",
    "            n = st.split(e, 1)[0]\n",
    "            \n",
    "        elif match1 is not None: \n",
    "            st = st.split(match1[0], 1)[1]\n",
    "            #e = re.search(r\"\\n[\\s\\w]+:\", st)[0]\n",
    "            e = re.search(r\"\\b([A-Z]\\w*)+:\", st)[0]\n",
    "            n = st.split(e, 1)[0]\n",
    "            \n",
    "        elif match2 is not None: \n",
    "            st = st.split(match2[0], 1)[1]\n",
    "            #e = re.search(r\"\\n[\\s\\w]+:\", st)[0]\n",
    "            e = re.search(r\"\\b([A-Z]\\w*)+:\", st)[0]\n",
    "            n = st.split(e, 1)[0]\n",
    "        \n",
    "        # Replace special strings ([** **]) with \"\"\n",
    "\n",
    "        rep = re.findall(r\"\\[\\*\\*([a-zA-Z0-9\\s\\-\\(\\)]+)\\*\\*]\", n)\n",
    "\n",
    "        for i in range(len(rep)):\n",
    "            n = n.replace(rep[i], \"\")\n",
    "        \n",
    "        n = n.rsplit(' ', 1)[0]\n",
    "\n",
    "    else: \n",
    "        n = ''\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad9596a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addtitional Preprocessing\n",
    "#sort = df.sort_values(by=['trunc_len'])\n",
    "\n",
    "# Check which sequences are length of 1-5\n",
    "#short = sort[sort['trunc_len'] <= 10].reset_index(drop = True)\n",
    "\n",
    "# Remove these tokens: Past, Physical, Hospital, Dictated\n",
    "\n",
    "# Check which HOPI sections have this string: EAST HOSPITAL MEDICINE ATTENDING ADMISSION NOTE and remove them\n",
    "#df[df['HOPI'] == ' EAST HOSPITAL MEDICINE ATTENDING ADMISSION NOTE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "991b1670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs missing HOPI sections: 3689\n"
     ]
    }
   ],
   "source": [
    "df['HOPI'] = df[\"TEXT\"].map(lambda t: process_notes(t))\n",
    "\n",
    "df[\"HOPI\"] = df[\"HOPI\"].apply(lambda x: re.sub('\\[\\*\\*[^\\]]*\\*\\*\\]', '', x))\n",
    "df[\"HOPI\"] = df[\"HOPI\"].apply(lambda x: re.sub('<[^>]*>', '', x))\n",
    "df[\"HOPI\"] = df[\"HOPI\"].apply(lambda x: re.sub('[\\W]+', ' ', x))\n",
    "\n",
    "# Detect history of present illness in text (n = 2641 records without HoPI data)\n",
    "missing = len(df[df['HOPI'] == \"\"])\n",
    "print(\"Number of docs missing HOPI sections: \" + str(missing))\n",
    "\n",
    "df = df[df[\"HOPI\"] != \"\"].reset_index(drop = True)\n",
    "df['SENT_TOKENS'] = df[\"HOPI\"].map(lambda t: [p.lower() for p in nltk.RegexpTokenizer(r'\\w+').tokenize(t) if not p.isnumeric()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a132416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>HOPI</th>\n",
       "      <th>SENT_TOKENS</th>\n",
       "      <th>SENT_TOKENS_COMBO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>[25013, 3371, 5849, 5780, V5867, 25063, 5363, ...</td>\n",
       "      <td>58526</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2117-9-11**]              ...</td>\n",
       "      <td>35F w poorly controlled Type 1 diabetes melli...</td>\n",
       "      <td>[325mg, 35f, 3l, 3rd, 4mg, 5d, a, ag, also, am...</td>\n",
       "      <td>325mg 35f 3l 3rd 4mg 5d a ag also am an and an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>[53100, 2851, 07054, 5715, 45621, 53789, 4019,...</td>\n",
       "      <td>54610</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2150-4-17**]              ...</td>\n",
       "      <td>Mr is a 59M w HepC cirrhosis c b grade I II e...</td>\n",
       "      <td>[40mg, 4l, 59m, a, abdominal, about, abstain, ...</td>\n",
       "      <td>40mg 4l 59m a abdominal about abstain admissio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100006</td>\n",
       "      <td>[49320, 51881, 486, 20300, 2761, 7850, 3090, V...</td>\n",
       "      <td>9895</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2108-4-6**]       Discharg...</td>\n",
       "      <td>This is a 48 year old African American female...</td>\n",
       "      <td>[a, abdominal, abg, admitted, african, ago, al...</td>\n",
       "      <td>a abdominal abg admitted african ago albuterol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100007</td>\n",
       "      <td>[56081, 5570, 9973, 486, 4019]</td>\n",
       "      <td>23018</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2145-3-31**]              ...</td>\n",
       "      <td>Ms is a 73 year old female with a history of ...</td>\n",
       "      <td>[a, abdominal, and, appendectomy, back, began,...</td>\n",
       "      <td>a abdominal and appendectomy back began bowel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100009</td>\n",
       "      <td>[41401, 99604, 4142, 25000, 27800, V8535, 4148...</td>\n",
       "      <td>533</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2162-5-16**]              ...</td>\n",
       "      <td>60yo man with known coronary disease AMI in a...</td>\n",
       "      <td>[36mmhg, 60yo, a, admitted, ak, ami, and, angi...</td>\n",
       "      <td>36mmhg 60yo a admitted ak ami and angina anter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                          ICD9_CODE  SUBJECT_ID  \\\n",
       "0   100001  [25013, 3371, 5849, 5780, V5867, 25063, 5363, ...       58526   \n",
       "1   100003  [53100, 2851, 07054, 5715, 45621, 53789, 4019,...       54610   \n",
       "2   100006  [49320, 51881, 486, 20300, 2761, 7850, 3090, V...        9895   \n",
       "3   100007                     [56081, 5570, 9973, 486, 4019]       23018   \n",
       "4   100009  [41401, 99604, 4142, 25000, 27800, V8535, 4148...         533   \n",
       "\n",
       "            CATEGORY DESCRIPTION  \\\n",
       "0  Discharge summary      Report   \n",
       "1  Discharge summary      Report   \n",
       "2  Discharge summary      Report   \n",
       "3  Discharge summary      Report   \n",
       "4  Discharge summary      Report   \n",
       "\n",
       "                                                TEXT  \\\n",
       "0  Admission Date:  [**2117-9-11**]              ...   \n",
       "1  Admission Date:  [**2150-4-17**]              ...   \n",
       "2  Admission Date:  [**2108-4-6**]       Discharg...   \n",
       "3  Admission Date:  [**2145-3-31**]              ...   \n",
       "4  Admission Date:  [**2162-5-16**]              ...   \n",
       "\n",
       "                                                HOPI  \\\n",
       "0   35F w poorly controlled Type 1 diabetes melli...   \n",
       "1   Mr is a 59M w HepC cirrhosis c b grade I II e...   \n",
       "2   This is a 48 year old African American female...   \n",
       "3   Ms is a 73 year old female with a history of ...   \n",
       "4   60yo man with known coronary disease AMI in a...   \n",
       "\n",
       "                                         SENT_TOKENS  \\\n",
       "0  [325mg, 35f, 3l, 3rd, 4mg, 5d, a, ag, also, am...   \n",
       "1  [40mg, 4l, 59m, a, abdominal, about, abstain, ...   \n",
       "2  [a, abdominal, abg, admitted, african, ago, al...   \n",
       "3  [a, abdominal, and, appendectomy, back, began,...   \n",
       "4  [36mmhg, 60yo, a, admitted, ak, ami, and, angi...   \n",
       "\n",
       "                                   SENT_TOKENS_COMBO  \n",
       "0  325mg 35f 3l 3rd 4mg 5d a ag also am an and an...  \n",
       "1  40mg 4l 59m a abdominal about abstain admissio...  \n",
       "2  a abdominal abg admitted african ago albuterol...  \n",
       "3  a abdominal and appendectomy back began bowel ...  \n",
       "4  36mmhg 60yo a admitted ak ami and angina anter...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truncate to 500 tokens per HOPI\n",
    "df[\"SENT_TOKENS\"] = df[\"SENT_TOKENS\"].map(lambda c: np.unique(c)[0:500])\n",
    "\n",
    "df['SENT_TOKENS_COMBO'] = df[\"SENT_TOKENS\"].map(lambda t: \" \".join(t))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "194ab1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=200, alpha=0.025>', 'datetime': '2022-12-04T00:10:10.828053', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 1547918 words, keeping 33008 word types\n",
      "PROGRESS: at sentence #20000, processed 3096903 words, keeping 45077 word types\n",
      "PROGRESS: at sentence #30000, processed 4643864 words, keeping 54264 word types\n",
      "PROGRESS: at sentence #40000, processed 6211630 words, keeping 62260 word types\n",
      "PROGRESS: at sentence #50000, processed 7754844 words, keeping 69400 word types\n",
      "collected 70408 word types from a corpus of 7986792 raw words and 51483 sentences\n",
      "Creating a fresh vocabulary\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 21877 unique words (31.07% of original 70408, drops 48531)', 'datetime': '2022-12-04T00:10:12.220052', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 7914681 word corpus (99.10% of original 7986792, drops 72111)', 'datetime': '2022-12-04T00:10:12.220554', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "deleting the raw counts dictionary of 70408 items\n",
      "sample=0.001 downsamples 37 most-common words\n",
      "Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 7602769.587054234 word corpus (96.1%% of prior 7914681)', 'datetime': '2022-12-04T00:10:12.349054', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "estimated required memory for 21877 words and 200 dimensions: 45941700 bytes\n",
      "resetting layer weights\n",
      "Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-12-04T00:10:12.559053', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'training model with 4 workers on 21877 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-12-04T00:10:12.560053', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "EPOCH 0 - PROGRESS: at 21.08% examples, 1591798 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 0 - PROGRESS: at 43.39% examples, 1643246 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 0 - PROGRESS: at 64.45% examples, 1630705 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 0 - PROGRESS: at 86.45% examples, 1640776 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 0: training on 7986792 raw words (7603262 effective words) took 4.6s, 1647253 effective words/s\n",
      "EPOCH 1 - PROGRESS: at 21.06% examples, 1591867 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 42.63% examples, 1613749 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 63.74% examples, 1610022 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 84.50% examples, 1601946 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 1: training on 7986792 raw words (7602818 effective words) took 4.7s, 1611918 effective words/s\n",
      "EPOCH 2 - PROGRESS: at 20.34% examples, 1540900 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 41.80% examples, 1580760 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 61.10% examples, 1541270 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 81.55% examples, 1545634 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 2: training on 7986792 raw words (7603346 effective words) took 4.9s, 1543489 effective words/s\n",
      "EPOCH 3 - PROGRESS: at 19.85% examples, 1505065 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 41.20% examples, 1555793 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 62.49% examples, 1573093 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 82.54% examples, 1560241 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 3: training on 7986792 raw words (7602529 effective words) took 4.9s, 1549568 effective words/s\n",
      "EPOCH 4 - PROGRESS: at 20.48% examples, 1548569 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 4 - PROGRESS: at 42.06% examples, 1592493 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 4 - PROGRESS: at 63.98% examples, 1619142 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 4 - PROGRESS: at 84.99% examples, 1610643 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 4: training on 7986792 raw words (7602318 effective words) took 4.7s, 1619630 effective words/s\n",
      "Word2Vec lifecycle event {'msg': 'training on 39933960 raw words (38014273 effective words) took 23.9s, 1591923 effective words/s', 'datetime': '2022-12-04T00:10:36.440056', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "Word2Vec lifecycle event {'fname_or_handle': './model/model.w2v', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-12-04T00:10:36.440556', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'}\n",
      "not storing attribute cum_table\n",
      "saved ./model/model.w2v\n"
     ]
    }
   ],
   "source": [
    "sentences = df['SENT_TOKENS_COMBO'].map(lambda t: t.split()).values\n",
    "\n",
    "model = w2v.Word2Vec(vector_size=200, min_count=5, workers=4, epochs=5)\n",
    "\n",
    "model.build_vocab(sentences)\n",
    "\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "model.save('./model/model.w2v')\n",
    "\n",
    "wv = model.wv\n",
    "\n",
    "vocab = model.wv.key_to_index  \n",
    "\n",
    "ind2w = {i+1:w for i,w in enumerate(sorted(vocab))}\n",
    "w2ind = {w:i for i,w in ind2w.items()}\n",
    "\n",
    "PAD_CHAR = \"**PAD**\"\n",
    "\n",
    "def build_matrix(ind2w, wv):\n",
    "    W = np.zeros((len(ind2w)+1, len(wv.get_vector(wv.index_to_key[0])) ))\n",
    "    words = [PAD_CHAR]\n",
    "    W[0][:] = np.zeros(len(wv.get_vector(wv.index_to_key[0])))\n",
    "    for idx, word in ind2w.items():\n",
    "        if idx >= W.shape[0]:\n",
    "            break    \n",
    "        W[idx][:] = wv.get_vector(word)\n",
    "        words.append(word)\n",
    "    return W, words\n",
    "\n",
    "W, words = build_matrix(ind2w, wv)\n",
    "\n",
    "def save_embeddings(W, words, outfile):\n",
    "    with open(outfile, 'w') as o:\n",
    "        #pad token already included\n",
    "        for i in range(len(words)):\n",
    "            line = [words[i]]\n",
    "            line.extend([str(d) for d in W[i]])\n",
    "            o.write(\" \".join(line) + \"\\n\")\n",
    "\n",
    "outfile = './model/model.embed'\n",
    "save_embeddings(W, words, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4c34972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embed_file):\n",
    "    #also normalizes the embeddings\n",
    "    W = []\n",
    "    with open(embed_file) as ef:\n",
    "        for line in ef:\n",
    "            line = line.rstrip().split()\n",
    "            vec = np.array(line[1:]).astype(float)\n",
    "            vec = vec / float(np.linalg.norm(vec) + 1e-6)\n",
    "            W.append(vec)\n",
    "        vec = np.random.randn(len(W[-1]))\n",
    "        vec = vec / float(np.linalg.norm(vec) + 1e-6)\n",
    "        W.append(vec)\n",
    "    W = np.array(W)\n",
    "    return W\n",
    "\n",
    "loaded_embed = torch.Tensor(load_embeddings(outfile))\n",
    "\n",
    "embed = nn.Embedding(loaded_embed.size()[0], loaded_embed.size()[1], padding_idx=0)\n",
    "embed.weight.data = loaded_embed.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c110107a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>HOPI</th>\n",
       "      <th>SENT_TOKENS</th>\n",
       "      <th>SENT_TOKENS_COMBO</th>\n",
       "      <th>idx_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>[25013, 3371, 5849, 5780, V5867, 25063, 5363, ...</td>\n",
       "      <td>58526</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2117-9-11**]              ...</td>\n",
       "      <td>35F w poorly controlled Type 1 diabetes melli...</td>\n",
       "      <td>[325mg, 35f, 3l, 3rd, 4mg, 5d, a, ag, also, am...</td>\n",
       "      <td>325mg 35f 3l 3rd 4mg 5d a ag also am an and an...</td>\n",
       "      <td>[615, 640, 698, 715, 860, 995, 1424, 1881, 205...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>[53100, 2851, 07054, 5715, 45621, 53789, 4019,...</td>\n",
       "      <td>54610</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2150-4-17**]              ...</td>\n",
       "      <td>Mr is a 59M w HepC cirrhosis c b grade I II e...</td>\n",
       "      <td>[40mg, 4l, 59m, a, abdominal, about, abstain, ...</td>\n",
       "      <td>40mg 4l 59m a abdominal about abstain admissio...</td>\n",
       "      <td>[752, 853, 984, 1424, 1456, 1497, 1516, 1785, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100006</td>\n",
       "      <td>[49320, 51881, 486, 20300, 2761, 7850, 3090, V...</td>\n",
       "      <td>9895</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2108-4-6**]       Discharg...</td>\n",
       "      <td>This is a 48 year old African American female...</td>\n",
       "      <td>[a, abdominal, abg, admitted, african, ago, al...</td>\n",
       "      <td>a abdominal abg admitted african ago albuterol...</td>\n",
       "      <td>[1424, 1456, 1468, 1793, 1871, 1909, 1973, 205...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100007</td>\n",
       "      <td>[56081, 5570, 9973, 486, 4019]</td>\n",
       "      <td>23018</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2145-3-31**]              ...</td>\n",
       "      <td>Ms is a 73 year old female with a history of ...</td>\n",
       "      <td>[a, abdominal, and, appendectomy, back, began,...</td>\n",
       "      <td>a abdominal and appendectomy back began bowel ...</td>\n",
       "      <td>[1424, 1456, 2204, 2485, 3053, 3248, 3649, 459...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100009</td>\n",
       "      <td>[41401, 99604, 4142, 25000, 27800, V8535, 4148...</td>\n",
       "      <td>533</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2162-5-16**]              ...</td>\n",
       "      <td>60yo man with known coronary disease AMI in a...</td>\n",
       "      <td>[36mmhg, 60yo, a, admitted, ak, ami, and, angi...</td>\n",
       "      <td>36mmhg 60yo a admitted ak ami and angina anter...</td>\n",
       "      <td>[656, 1072, 1424, 1793, 1956, 2108, 2204, 2225...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                          ICD9_CODE  SUBJECT_ID  \\\n",
       "0   100001  [25013, 3371, 5849, 5780, V5867, 25063, 5363, ...       58526   \n",
       "1   100003  [53100, 2851, 07054, 5715, 45621, 53789, 4019,...       54610   \n",
       "2   100006  [49320, 51881, 486, 20300, 2761, 7850, 3090, V...        9895   \n",
       "3   100007                     [56081, 5570, 9973, 486, 4019]       23018   \n",
       "4   100009  [41401, 99604, 4142, 25000, 27800, V8535, 4148...         533   \n",
       "\n",
       "            CATEGORY DESCRIPTION  \\\n",
       "0  Discharge summary      Report   \n",
       "1  Discharge summary      Report   \n",
       "2  Discharge summary      Report   \n",
       "3  Discharge summary      Report   \n",
       "4  Discharge summary      Report   \n",
       "\n",
       "                                                TEXT  \\\n",
       "0  Admission Date:  [**2117-9-11**]              ...   \n",
       "1  Admission Date:  [**2150-4-17**]              ...   \n",
       "2  Admission Date:  [**2108-4-6**]       Discharg...   \n",
       "3  Admission Date:  [**2145-3-31**]              ...   \n",
       "4  Admission Date:  [**2162-5-16**]              ...   \n",
       "\n",
       "                                                HOPI  \\\n",
       "0   35F w poorly controlled Type 1 diabetes melli...   \n",
       "1   Mr is a 59M w HepC cirrhosis c b grade I II e...   \n",
       "2   This is a 48 year old African American female...   \n",
       "3   Ms is a 73 year old female with a history of ...   \n",
       "4   60yo man with known coronary disease AMI in a...   \n",
       "\n",
       "                                         SENT_TOKENS  \\\n",
       "0  [325mg, 35f, 3l, 3rd, 4mg, 5d, a, ag, also, am...   \n",
       "1  [40mg, 4l, 59m, a, abdominal, about, abstain, ...   \n",
       "2  [a, abdominal, abg, admitted, african, ago, al...   \n",
       "3  [a, abdominal, and, appendectomy, back, began,...   \n",
       "4  [36mmhg, 60yo, a, admitted, ak, ami, and, angi...   \n",
       "\n",
       "                                   SENT_TOKENS_COMBO  \\\n",
       "0  325mg 35f 3l 3rd 4mg 5d a ag also am an and an...   \n",
       "1  40mg 4l 59m a abdominal about abstain admissio...   \n",
       "2  a abdominal abg admitted african ago albuterol...   \n",
       "3  a abdominal and appendectomy back began bowel ...   \n",
       "4  36mmhg 60yo a admitted ak ami and angina anter...   \n",
       "\n",
       "                                          idx_tokens  \n",
       "0  [615, 640, 698, 715, 860, 995, 1424, 1881, 205...  \n",
       "1  [752, 853, 984, 1424, 1456, 1497, 1516, 1785, ...  \n",
       "2  [1424, 1456, 1468, 1793, 1871, 1909, 1973, 205...  \n",
       "3  [1424, 1456, 2204, 2485, 3053, 3248, 3649, 459...  \n",
       "4  [656, 1072, 1424, 1793, 1956, 2108, 2204, 2225...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding indexes\n",
    "df[\"idx_tokens\"] = df['SENT_TOKENS'].map(lambda t: [int(w2ind[w]) if w in w2ind else len(w2ind)+1 for w in t])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b841b468",
   "metadata": {},
   "source": [
    "## Plot a histogram of the Number of tokens in each HoPI document, after data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b72313d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"trunc_len\"] = df['SENT_TOKENS'].map(lambda c: len(c))\n",
    "\n",
    "plt.hist(df[\"trunc_len\"], np.arange(0, 501, 10))\n",
    "plt.xlabel(\"Number of Tokens\")\n",
    "plt.ylabel(\"Number of Documents\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f07c2f",
   "metadata": {},
   "source": [
    "## Split data in train, valid, and test sets\n",
    "### training (38,588 records, 69.9%), validation (5536 records, 10.0%) and testing (11,048 records, 20.0%) folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9cc17eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training: 36038, length of test: 10348, & length of valid: 5097\n"
     ]
    }
   ],
   "source": [
    "# Random split\n",
    "train, tst = sklearn.model_selection.train_test_split(df, test_size= 0.3, random_state=42)\n",
    "\n",
    "test, valid = sklearn.model_selection.train_test_split(tst, test_size= 0.33, random_state=42)\n",
    "\n",
    "train = train.reset_index(drop = True).drop([\"CATEGORY\", \"DESCRIPTION\"], axis = 1)\n",
    "test = test.reset_index(drop = True).drop([\"CATEGORY\", \"DESCRIPTION\"], axis = 1)\n",
    "valid = valid.reset_index(drop = True).drop([\"CATEGORY\", \"DESCRIPTION\"], axis = 1)\n",
    "\n",
    "print(f\"Length of training: {len(train)}, length of test: {len(test)}, & length of valid: {len(valid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6f8d07",
   "metadata": {},
   "source": [
    "## Count number of tokens in the training dataset (n = ~92,468 tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "790c81fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19017\n"
     ]
    }
   ],
   "source": [
    "# Count occurence of tokens that are in the training dataset\n",
    "n = len(np.unique(np.concatenate(train['SENT_TOKENS'].values)))\n",
    "occ = Counter(np.concatenate(train['SENT_TOKENS'].values))\n",
    "            \n",
    "# Tokens that occur >= 5 times are in the study vocabulary (should be ~19,503)\n",
    "vocab = [k for k,v in occ.items() if v >= 5]\n",
    "print(len(vocab))\n",
    "\n",
    "# Assign a unique integer ID for each token in the study vocabulary \n",
    "vocab_lookup = dict(zip(vocab, np.arange(0, len(vocab), 1)))\n",
    "\n",
    "# Convert each HoPI document to a 1D array of integers using this index\n",
    "train[\"trunc_idx\"] =  train['SENT_TOKENS'].map(lambda x: [vocab_lookup.get(i) for i in x if i in vocab_lookup.keys()])\n",
    "valid[\"trunc_idx\"] =  valid['SENT_TOKENS'].map(lambda x: [vocab_lookup.get(i) for i in x if i in vocab_lookup.keys()])\n",
    "test[\"trunc_idx\"] =  test['SENT_TOKENS'].map(lambda x: [vocab_lookup.get(i) for i in x if i in vocab_lookup.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b180a12e",
   "metadata": {},
   "source": [
    "# Document representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "04e11ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent clinical notes documents as TF-IDF weights\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def tfidf(df):\n",
    "    pipe = Pipeline([('count', CountVectorizer(vocabulary = vocab, lowercase = False)),('tfid', TfidfTransformer(use_idf=True))]).fit(df[\"HOPI\"].values)\n",
    "    pipe['count'].transform(df[\"HOPI\"].values).toarray()\n",
    "    pipe['tfid'].idf_\n",
    "\n",
    "    return pipe.transform(df[\"HOPI\"].values)\n",
    "\n",
    "# Create sparse matrix of size number of docs x words in vocab\n",
    "tfidf_train = tfidf(train)\n",
    "tfidf_valid = tfidf(valid)\n",
    "tfidf_test = tfidf(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e46938b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document embedding representation\n",
    "train['emb'] = train['idx_tokens'].map(lambda t: [np.array(embed.weight.data[w]) for w in t])\n",
    "valid['emb'] = valid['idx_tokens'].map(lambda t: [np.array(embed.weight.data[w]) for w in t])\n",
    "test['emb'] = test['idx_tokens'].map(lambda t: [np.array(embed.weight.data[w]) for w in t])\n",
    "\n",
    "# Document mean embedding representation\n",
    "train['x_mean_emb'] = train['idx_tokens'].map(lambda t: np.average([np.array(embed.weight.data[w]) for w in t], axis = 0))\n",
    "valid['x_mean_emb'] = valid['idx_tokens'].map(lambda t: np.average([np.array(embed.weight.data[w]) for w in t], axis = 0))\n",
    "test['x_mean_emb'] = test['idx_tokens'].map(lambda t: np.average([np.array(embed.weight.data[w]) for w in t], axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb2746c",
   "metadata": {},
   "source": [
    "# Label representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c0afc9",
   "metadata": {},
   "source": [
    "### Identify the hierarchical labels for each icd9 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e7305717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out the main ICD9 code (they are ordered for importance)\n",
    "train[\"ICD_Main\"] = train['ICD9_CODE'].map(lambda x: x[0])\n",
    "valid[\"ICD_Main\"] = valid['ICD9_CODE'].map(lambda x: x[0])\n",
    "test[\"ICD_Main\"] = test['ICD9_CODE'].map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9714dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pyhealth library to find code hierarchy for the train, test, and valid datasets\n",
    "icd9cm = InnerMap.load(\"ICD9CM\")\n",
    "\n",
    "def define_levels(x):\n",
    "    lvls = []\n",
    "    for l in x: \n",
    "        lvls.append(icd9cm.lookup(l))\n",
    "    return lvls\n",
    "\n",
    "def drop_top(X):\n",
    "    if '001-999.99' in X:\n",
    "        X.remove('001-999.99')\n",
    "    return X\n",
    "\n",
    "def get_codes(df):\n",
    "    #df['icd_desc'] = df['ICD_Main'].map(lambda x: icd9cm.lookup(x))\n",
    "    df['levels'] = df['ICD_Main'].map(lambda x:icd9cm.get_ancestors(x)[::-1])\n",
    "    df['levels'].map(lambda x: drop_top(x))\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        df['levels'][i].append(df['ICD_Main'][i])\n",
    "\n",
    "    df['levels_desc'] = df['levels'].map(lambda x: define_levels(x))\n",
    "\n",
    "get_codes(train)\n",
    "get_codes(valid)\n",
    "get_codes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "65bc18b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the code descriptions to the dfs\n",
    "train = pd.concat([train, train['levels_desc'].apply(pd.Series)], axis = 1).drop(\"levels_desc\", axis = 1)\\\n",
    "    .rename(columns={0: \"y_l1\", 1: \"y_l2\", 2: \"y_l3\", 3: \"y_l4\", 4:\"y_l5\", 5:\"y_l6\"})\n",
    "valid = pd.concat([valid, valid['levels_desc'].apply(pd.Series)], axis = 1).drop(\"levels_desc\", axis = 1)\\\n",
    "    .rename(columns={0: \"y_l1\", 1: \"y_l2\", 2: \"y_l3\", 3: \"y_l4\", 4:\"y_l5\", 5:\"y_l6\"})\n",
    "test = pd.concat([test, test['levels_desc'].apply(pd.Series)], axis = 1).drop(\"levels_desc\", axis = 1)\\\n",
    "    .rename(columns={0: \"y_l1\", 1: \"y_l2\", 2: \"y_l3\", 3: \"y_l4\" , 4:\"y_l5\", 5:\"y_l6\"})\n",
    "\n",
    "# train = pd.concat([train, train['levels_desc'].apply(pd.Series)], axis = 1)\\\n",
    "#     .rename(columns={0: \"y_l1\", 1: \"y_l2\", 2: \"y_l3\", 3: \"y_l4\", 4:\"y_l5\", 5:\"y_l6\"})\n",
    "# valid = pd.concat([valid, valid['levels_desc'].apply(pd.Series)], axis = 1)\\\n",
    "#     .rename(columns={0: \"y_l1\", 1: \"y_l2\", 2: \"y_l3\", 3: \"y_l4\", 4:\"y_l5\", 5:\"y_l6\"})\n",
    "# test = pd.concat([test, test['levels_desc'].apply(pd.Series)], axis = 1)\\\n",
    "#     .rename(columns={0: \"y_l1\", 1: \"y_l2\", 2: \"y_l3\", 3: \"y_l4\" , 4:\"y_l5\", 5:\"y_l6\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "24c6ec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['y_l5', 'y_l6']\n",
    "train.drop(cols, inplace = True, axis = 1)\n",
    "valid.drop(cols, inplace = True, axis = 1)\n",
    "test.drop(cols, inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3e04392a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2022-12-04T09:09:25.508831', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 48355 words, keeping 59 word types\n",
      "PROGRESS: at sentence #20000, processed 96549 words, keeping 59 word types\n",
      "PROGRESS: at sentence #30000, processed 144803 words, keeping 59 word types\n",
      "collected 59 word types from a corpus of 173763 raw words and 36038 sentences\n",
      "Creating a fresh vocabulary\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 59 unique words (100.00% of original 59, drops 0)', 'datetime': '2022-12-04T09:09:25.557333', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 173763 word corpus (100.00% of original 173763, drops 0)', 'datetime': '2022-12-04T09:09:25.557833', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "deleting the raw counts dictionary of 59 items\n",
      "sample=0.001 downsamples 36 most-common words\n",
      "Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 37108.00929953167 word corpus (21.4%% of prior 173763)', 'datetime': '2022-12-04T09:09:25.561332', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "estimated required memory for 59 words and 100 dimensions: 76700 bytes\n",
      "resetting layer weights\n",
      "Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-12-04T09:09:25.569832', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'training model with 4 workers on 59 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-12-04T09:09:25.571834', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "EPOCH 0: training on 173763 raw words (37096 effective words) took 0.1s, 716100 effective words/s\n",
      "EPOCH 1: training on 173763 raw words (37042 effective words) took 0.0s, 876391 effective words/s\n",
      "EPOCH 2: training on 173763 raw words (37196 effective words) took 0.0s, 772349 effective words/s\n",
      "EPOCH 3: training on 173763 raw words (37286 effective words) took 0.0s, 772322 effective words/s\n",
      "EPOCH 4: training on 173763 raw words (37152 effective words) took 0.0s, 765301 effective words/s\n",
      "Word2Vec lifecycle event {'msg': 'training on 868815 raw words (185772 effective words) took 0.5s, 370406 effective words/s', 'datetime': '2022-12-04T09:09:26.073733', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "Word2Vec lifecycle event {'fname_or_handle': './model/model_lvl1_100.w2v', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-12-04T09:09:26.074235', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'}\n",
      "not storing attribute cum_table\n",
      "saved ./model/model_lvl1_100.w2v\n",
      "Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2022-12-04T09:09:29.932585', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 48381 words, keeping 343 word types\n",
      "PROGRESS: at sentence #20000, processed 96561 words, keeping 368 word types\n",
      "PROGRESS: at sentence #30000, processed 145038 words, keeping 375 word types\n",
      "collected 377 word types from a corpus of 173892 raw words and 36038 sentences\n",
      "Creating a fresh vocabulary\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 326 unique words (86.47% of original 377, drops 51)', 'datetime': '2022-12-04T09:09:29.980586', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 173784 word corpus (99.94% of original 173892, drops 108)', 'datetime': '2022-12-04T09:09:29.981086', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "deleting the raw counts dictionary of 377 items\n",
      "sample=0.001 downsamples 70 most-common words\n",
      "Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 70448.84561050596 word corpus (40.5%% of prior 173784)', 'datetime': '2022-12-04T09:09:29.984587', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "estimated required memory for 326 words and 100 dimensions: 423800 bytes\n",
      "resetting layer weights\n",
      "Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-12-04T09:09:29.993587', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'training model with 4 workers on 326 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-12-04T09:09:29.994087', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "EPOCH 0: training on 173892 raw words (70435 effective words) took 0.1s, 1190683 effective words/s\n",
      "EPOCH 1: training on 173892 raw words (70489 effective words) took 0.1s, 1147915 effective words/s\n",
      "EPOCH 2: training on 173892 raw words (70727 effective words) took 0.1s, 1321941 effective words/s\n",
      "EPOCH 3: training on 173892 raw words (70588 effective words) took 0.1s, 1242604 effective words/s\n",
      "EPOCH 4: training on 173892 raw words (70351 effective words) took 0.1s, 1272667 effective words/s\n",
      "Word2Vec lifecycle event {'msg': 'training on 869460 raw words (352590 effective words) took 0.4s, 943046 effective words/s', 'datetime': '2022-12-04T09:09:30.368550', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "Word2Vec lifecycle event {'fname_or_handle': './model/model_lvl2_100.w2v', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-12-04T09:09:30.369050', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'}\n",
      "not storing attribute cum_table\n",
      "saved ./model/model_lvl2_100.w2v\n",
      "Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2022-12-04T09:09:34.169247', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 41591 words, keeping 848 word types\n",
      "PROGRESS: at sentence #20000, processed 83438 words, keeping 999 word types\n",
      "PROGRESS: at sentence #30000, processed 124673 words, keeping 1067 word types\n",
      "collected 1113 word types from a corpus of 149742 raw words and 36038 sentences\n",
      "Creating a fresh vocabulary\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 688 unique words (61.81% of original 1113, drops 425)', 'datetime': '2022-12-04T09:09:34.218747', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 148924 word corpus (99.45% of original 149742, drops 818)', 'datetime': '2022-12-04T09:09:34.219248', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "deleting the raw counts dictionary of 1113 items\n",
      "sample=0.001 downsamples 81 most-common words\n",
      "Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 81492.47130201414 word corpus (54.7%% of prior 148924)', 'datetime': '2022-12-04T09:09:34.226248', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "estimated required memory for 688 words and 100 dimensions: 894400 bytes\n",
      "resetting layer weights\n",
      "Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-12-04T09:09:34.240247', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'training model with 4 workers on 688 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-12-04T09:09:34.240753', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "EPOCH 0: training on 149742 raw words (81417 effective words) took 0.1s, 1264253 effective words/s\n",
      "EPOCH 1: training on 149742 raw words (81484 effective words) took 0.1s, 1167292 effective words/s\n",
      "EPOCH 2: training on 149742 raw words (81467 effective words) took 0.1s, 1321988 effective words/s\n",
      "EPOCH 3: training on 149742 raw words (81449 effective words) took 0.1s, 1389094 effective words/s\n",
      "EPOCH 4: training on 149742 raw words (81579 effective words) took 0.1s, 1179883 effective words/s\n",
      "Word2Vec lifecycle event {'msg': 'training on 748710 raw words (407396 effective words) took 0.4s, 983986 effective words/s', 'datetime': '2022-12-04T09:09:34.655248', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "Word2Vec lifecycle event {'fname_or_handle': './model/model_lvl3_100.w2v', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-12-04T09:09:34.656248', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'}\n",
      "not storing attribute cum_table\n",
      "saved ./model/model_lvl3_100.w2v\n",
      "Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2022-12-04T09:09:38.943747', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 48380 words, keeping 1411 word types\n",
      "PROGRESS: at sentence #20000, processed 96712 words, keeping 1691 word types\n",
      "PROGRESS: at sentence #30000, processed 144783 words, keeping 1866 word types\n",
      "collected 1934 word types from a corpus of 163035 raw words and 33824 sentences\n",
      "Creating a fresh vocabulary\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1018 unique words (52.64% of original 1934, drops 916)', 'datetime': '2022-12-04T09:09:38.999247', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 161370 word corpus (98.98% of original 163035, drops 1665)', 'datetime': '2022-12-04T09:09:39.000249', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "deleting the raw counts dictionary of 1934 items\n",
      "sample=0.001 downsamples 79 most-common words\n",
      "Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 98175.60209527746 word corpus (60.8%% of prior 161370)', 'datetime': '2022-12-04T09:09:39.009749', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "estimated required memory for 1018 words and 100 dimensions: 1323400 bytes\n",
      "resetting layer weights\n",
      "Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-12-04T09:09:39.026247', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'training model with 4 workers on 1018 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-12-04T09:09:39.027249', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "EPOCH 0: training on 163035 raw words (98238 effective words) took 0.1s, 1447384 effective words/s\n",
      "EPOCH 1: training on 163035 raw words (98040 effective words) took 0.1s, 1839614 effective words/s\n",
      "EPOCH 2: training on 163035 raw words (98203 effective words) took 0.1s, 1407319 effective words/s\n",
      "EPOCH 3: training on 163035 raw words (98169 effective words) took 0.1s, 1460842 effective words/s\n",
      "EPOCH 4: training on 163035 raw words (98321 effective words) took 0.1s, 1417411 effective words/s\n",
      "Word2Vec lifecycle event {'msg': 'training on 815175 raw words (490971 effective words) took 0.4s, 1174265 effective words/s', 'datetime': '2022-12-04T09:09:39.445748', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "Word2Vec lifecycle event {'fname_or_handle': './model/model_lvl4_100.w2v', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-12-04T09:09:39.446248', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'}\n",
      "not storing attribute cum_table\n",
      "saved ./model/model_lvl4_100.w2v\n",
      "Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.025>', 'datetime': '2022-12-04T09:09:43.767246', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 48318 words, keeping 59 word types\n",
      "PROGRESS: at sentence #20000, processed 96668 words, keeping 59 word types\n",
      "PROGRESS: at sentence #30000, processed 144809 words, keeping 59 word types\n",
      "collected 59 word types from a corpus of 163143 raw words and 33824 sentences\n",
      "Creating a fresh vocabulary\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 59 unique words (100.00% of original 59, drops 0)', 'datetime': '2022-12-04T09:09:43.812754', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 163143 word corpus (100.00% of original 163143, drops 0)', 'datetime': '2022-12-04T09:09:43.813247', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "deleting the raw counts dictionary of 59 items\n",
      "sample=0.001 downsamples 36 most-common words\n",
      "Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 34753.61183452127 word corpus (21.3%% of prior 163143)', 'datetime': '2022-12-04T09:09:43.815750', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "estimated required memory for 59 words and 300 dimensions: 171100 bytes\n",
      "resetting layer weights\n",
      "Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-12-04T09:09:43.820248', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'training model with 4 workers on 59 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-12-04T09:09:43.821249', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "EPOCH 0: training on 163143 raw words (34842 effective words) took 0.0s, 763082 effective words/s\n",
      "EPOCH 1: training on 163143 raw words (34905 effective words) took 0.1s, 651810 effective words/s\n",
      "EPOCH 2: training on 163143 raw words (34576 effective words) took 0.0s, 697710 effective words/s\n",
      "EPOCH 3: training on 163143 raw words (34898 effective words) took 0.0s, 759537 effective words/s\n",
      "EPOCH 4: training on 163143 raw words (34742 effective words) took 0.0s, 700101 effective words/s\n",
      "Word2Vec lifecycle event {'msg': 'training on 815715 raw words (173963 effective words) took 0.3s, 516199 effective words/s', 'datetime': '2022-12-04T09:09:44.158878', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "Word2Vec lifecycle event {'fname_or_handle': './model/model_lvl1_300.w2v', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-12-04T09:09:44.159377', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'}\n",
      "not storing attribute cum_table\n",
      "saved ./model/model_lvl1_300.w2v\n",
      "Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.025>', 'datetime': '2022-12-04T09:09:49.082394', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 49328 words, keeping 326 word types\n",
      "PROGRESS: at sentence #20000, processed 98587 words, keeping 347 word types\n",
      "PROGRESS: at sentence #30000, processed 148029 words, keeping 353 word types\n",
      "collected 355 word types from a corpus of 166543 raw words and 33824 sentences\n",
      "Creating a fresh vocabulary\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 306 unique words (86.20% of original 355, drops 49)', 'datetime': '2022-12-04T09:09:49.130893', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 166429 word corpus (99.93% of original 166543, drops 114)', 'datetime': '2022-12-04T09:09:49.131390', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "deleting the raw counts dictionary of 355 items\n",
      "sample=0.001 downsamples 71 most-common words\n",
      "Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 65705.66513856489 word corpus (39.5%% of prior 166429)', 'datetime': '2022-12-04T09:09:49.134891', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "estimated required memory for 306 words and 300 dimensions: 887400 bytes\n",
      "resetting layer weights\n",
      "Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-12-04T09:09:49.146390', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'training model with 4 workers on 306 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-12-04T09:09:49.147390', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "EPOCH 0: training on 166543 raw words (65519 effective words) took 0.1s, 1016463 effective words/s\n",
      "EPOCH 1: training on 166543 raw words (65404 effective words) took 0.1s, 1068757 effective words/s\n",
      "EPOCH 2: training on 166543 raw words (65557 effective words) took 0.1s, 1097701 effective words/s\n",
      "EPOCH 3: training on 166543 raw words (65712 effective words) took 0.1s, 958686 effective words/s\n",
      "EPOCH 4: training on 166543 raw words (65693 effective words) took 0.1s, 1047550 effective words/s\n",
      "Word2Vec lifecycle event {'msg': 'training on 832715 raw words (327885 effective words) took 0.4s, 827492 effective words/s', 'datetime': '2022-12-04T09:09:49.543892', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "Word2Vec lifecycle event {'fname_or_handle': './model/model_lvl2_300.w2v', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-12-04T09:09:49.544390', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'}\n",
      "not storing attribute cum_table\n",
      "saved ./model/model_lvl2_300.w2v\n",
      "Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.025>', 'datetime': '2022-12-04T09:09:55.747412', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 42209 words, keeping 793 word types\n",
      "PROGRESS: at sentence #20000, processed 84810 words, keeping 890 word types\n",
      "PROGRESS: at sentence #30000, processed 126918 words, keeping 957 word types\n",
      "collected 985 word types from a corpus of 142906 raw words and 33824 sentences\n",
      "Creating a fresh vocabulary\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 625 unique words (63.45% of original 985, drops 360)', 'datetime': '2022-12-04T09:09:55.795914', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 142187 word corpus (99.50% of original 142906, drops 719)', 'datetime': '2022-12-04T09:09:55.796412', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "deleting the raw counts dictionary of 985 items\n",
      "sample=0.001 downsamples 84 most-common words\n",
      "Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 75953.4376911079 word corpus (53.4%% of prior 142187)', 'datetime': '2022-12-04T09:09:55.803912', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "estimated required memory for 625 words and 300 dimensions: 1812500 bytes\n",
      "resetting layer weights\n",
      "Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-12-04T09:09:55.815913', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'training model with 4 workers on 625 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-12-04T09:09:55.816915', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "EPOCH 0: training on 142906 raw words (75861 effective words) took 0.1s, 1055692 effective words/s\n",
      "EPOCH 1: training on 142906 raw words (75916 effective words) took 0.1s, 1263527 effective words/s\n",
      "EPOCH 2: training on 142906 raw words (76025 effective words) took 0.1s, 1129338 effective words/s\n",
      "EPOCH 3: training on 142906 raw words (76233 effective words) took 0.1s, 1013997 effective words/s\n",
      "EPOCH 4: training on 142906 raw words (75919 effective words) took 0.1s, 1247666 effective words/s\n",
      "Word2Vec lifecycle event {'msg': 'training on 714530 raw words (379954 effective words) took 0.4s, 889652 effective words/s', 'datetime': '2022-12-04T09:09:56.244412', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "Word2Vec lifecycle event {'fname_or_handle': './model/model_lvl3_300.w2v', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-12-04T09:09:56.244911', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'}\n",
      "not storing attribute cum_table\n",
      "saved ./model/model_lvl3_300.w2v\n",
      "Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.025>', 'datetime': '2022-12-04T09:10:01.539412', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 48380 words, keeping 1411 word types\n",
      "PROGRESS: at sentence #20000, processed 96712 words, keeping 1691 word types\n",
      "PROGRESS: at sentence #30000, processed 144783 words, keeping 1866 word types\n",
      "collected 1934 word types from a corpus of 163035 raw words and 33824 sentences\n",
      "Creating a fresh vocabulary\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1018 unique words (52.64% of original 1934, drops 916)', 'datetime': '2022-12-04T09:10:01.592914', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 161370 word corpus (98.98% of original 163035, drops 1665)', 'datetime': '2022-12-04T09:10:01.593912', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "deleting the raw counts dictionary of 1934 items\n",
      "sample=0.001 downsamples 79 most-common words\n",
      "Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 98175.60209527746 word corpus (60.8%% of prior 161370)', 'datetime': '2022-12-04T09:10:01.602914', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "estimated required memory for 1018 words and 300 dimensions: 2952200 bytes\n",
      "resetting layer weights\n",
      "Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-12-04T09:10:01.615414', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'training model with 4 workers on 1018 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-12-04T09:10:01.615914', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "EPOCH 0: training on 163035 raw words (98135 effective words) took 0.1s, 1176072 effective words/s\n",
      "EPOCH 1: training on 163035 raw words (98019 effective words) took 0.1s, 1186638 effective words/s\n",
      "EPOCH 2: training on 163035 raw words (98214 effective words) took 0.1s, 1292029 effective words/s\n",
      "EPOCH 3: training on 163035 raw words (98142 effective words) took 0.1s, 1296833 effective words/s\n",
      "EPOCH 4: training on 163035 raw words (98013 effective words) took 0.1s, 1402083 effective words/s\n",
      "Word2Vec lifecycle event {'msg': 'training on 815175 raw words (490523 effective words) took 0.5s, 1018825 effective words/s', 'datetime': '2022-12-04T09:10:02.097914', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "Word2Vec lifecycle event {'fname_or_handle': './model/model_lvl4_300.w2v', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-12-04T09:10:02.098914', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'}\n",
      "not storing attribute cum_table\n",
      "saved ./model/model_lvl4_300.w2v\n"
     ]
    }
   ],
   "source": [
    "# Mean representation for each level of main icd9 code\n",
    "\n",
    "levels = [1, 2, 3, 4]\n",
    "\n",
    "def get_embeddings(level, size=100):\n",
    "\n",
    "    train.dropna(subset=[f'y_l{level}'], inplace = True)\n",
    "    valid.dropna(subset=[f'y_l{level}'], inplace = True)\n",
    "    test.dropna(subset=[f'y_l{level}'], inplace = True)\n",
    "\n",
    "    sentences = train[f'y_l{level}'].map(lambda t: t.split()).values\n",
    "    # df['SENT_TOKENS'] = df[\"HOPI\"].map(lambda t: [p.lower() for p in nltk.RegexpTokenizer(r'\\w+').tokenize(t) if not p.isnumeric()])\n",
    "    # sentences = train[f'levels_desc'].map(lambda t: [m.split() for m in t].flatten ).values \n",
    "    model = w2v.Word2Vec(vector_size = size, min_count = 5, workers = 4, epochs = 5)\n",
    "    model.build_vocab(sentences)\n",
    "    model.train(sentences, total_examples = model.corpus_count, epochs = model.epochs)\n",
    "    model.save(f'./model/model_lvl{level}_{size}.w2v')\n",
    "\n",
    "    wv = model.wv\n",
    "\n",
    "    vocab = model.wv.key_to_index  \n",
    "\n",
    "    ind2w = {i+1:w for i,w in enumerate(sorted(vocab))}\n",
    "    w2ind = {w:i for i,w in ind2w.items()}\n",
    "\n",
    "    PAD_CHAR = \"**PAD**\"\n",
    "\n",
    "    outfile = f'./model/model_lvl{level}_{size}.embed'\n",
    "    W, words = build_matrix(ind2w, wv)\n",
    "\n",
    "    build_matrix(ind2w, wv)\n",
    "    save_embeddings(W, words, outfile)\n",
    "\n",
    "    loaded_embed = torch.Tensor(load_embeddings(outfile))\n",
    "\n",
    "    embed = nn.Embedding(loaded_embed.size()[0], loaded_embed.size()[1], padding_idx = 0)\n",
    "    embed.weight.data = loaded_embed.clone()\n",
    "\n",
    "    train[f'y_l{level}_tokens_{size}'] = train[f'y_l{level}'].map(lambda t: t.split() if (len(t) >= 1) else t).values\n",
    "    valid[f'y_l{level}_tokens_{size}'] = valid[f'y_l{level}'].map(lambda t: t.split() if (len(t) >= 1) else t).values\n",
    "    test[f'y_l{level}_tokens_{size}'] = test[f'y_l{level}'].map(lambda t: t.split() if (len(t) >= 1) else t).values\n",
    "\n",
    "    train[f\"y_l{level}_idx_{size}\"] = train[f'y_l{level}_tokens_{size}'].map(lambda t: [int(w2ind[w]) if w in w2ind else len(w2ind)+1 for w in t])\n",
    "    valid[f\"y_l{level}_idx_{size}\"] = valid[f'y_l{level}_tokens_{size}'].map(lambda t: [int(w2ind[w]) if w in w2ind else len(w2ind)+1 for w in t])\n",
    "    test[f\"y_l{level}_idx_{size}\"] = test[f'y_l{level}_tokens_{size}'].map(lambda t: [int(w2ind[w]) if w in w2ind else len(w2ind)+1 for w in t])\n",
    "\n",
    "    train[f\"y_{level}_emb_{size}\"] = train[f'y_l{level}_idx_{size}'].map(lambda t: [np.array(embed.weight.data[w]) for w in t])\n",
    "    valid[f\"y_{level}_emb_{size}\"] = valid[f'y_l{level}_idx_{size}'].map(lambda t: [np.array(embed.weight.data[w]) for w in t])\n",
    "    test[f\"y_{level}_emb_{size}\"] = test[f'y_l{level}_idx_{size}'].map(lambda t: [np.array(embed.weight.data[w]) for w in t])\n",
    "\n",
    "    return embed.weight.data\n",
    "\n",
    "# DOES NOT RUN YET\n",
    "lvl1_embed = get_embeddings(1)\n",
    "lvl2_embed = get_embeddings(2)\n",
    "lvl3_embed = get_embeddings(3)\n",
    "lvl4_embed = get_embeddings(4)\n",
    "\n",
    "#need 300 var embed for gru\n",
    "lvl1_embed_300 = get_embeddings(1, 300)\n",
    "lvl2_embed_300 = get_embeddings(2, 300)\n",
    "lvl3_embed_300 = get_embeddings(3, 300)\n",
    "lvl4_embed_300 = get_embeddings(4, 300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3524d3",
   "metadata": {},
   "source": [
    "## Baseline Model: TFIDF-Atomic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00750564",
   "metadata": {},
   "source": [
    "#### TFIDF document weights were input into a multinomial logistic regression classifier in order to predict the 4 levels of icd hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0df4d0",
   "metadata": {},
   "source": [
    "### Predict level 1 icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32b285c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:07<00:00,  7.08s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6871122590400897,\n",
       " 'recall': 0.699748743718593,\n",
       " 'f1-score': 0.6768162770150218,\n",
       " 'support': 10348}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multinomial Logistic Regression from sklearn \n",
    "\n",
    "# L2 values to iterate over\n",
    "L2 = np.logspace(.01, 100, num = 50)\n",
    "L2 = [1.023292992280754] # In order to test different L2 values comment out this line\n",
    "\n",
    "# # Enumerate the l1 classes\n",
    "y_l1_dict = dict(zip(np.unique(train['y_l1'].values), np.arange(0, len(np.unique(train['y_l1'].values)), 1)))\n",
    "y_l1_trn = train['y_l1'].map(lambda x: y_l1_dict.get(x))\n",
    "y_l1_val = valid['y_l1'].map(lambda x: y_l1_dict.get(x))\n",
    "y_l1_tst = test['y_l1'].map(lambda x: y_l1_dict.get(x))\n",
    "\n",
    "def log_reg_val(y_trn, y_val, x_trn, x_val, L2):\n",
    "    \n",
    "    # Use the validation data to determine the best model parameter for C\n",
    "    scores = []\n",
    "    i = 1\n",
    "    for C in tqdm(L2): \n",
    "        \n",
    "        tfidf_atomic = LogisticRegression(multi_class = 'multinomial', penalty = 'l2', max_iter = 1500, solver = \"sag\", random_state = 42, C = C)\n",
    "        tfidf_atomic_L1 = tfidf_atomic.fit(x_trn, y_trn)\n",
    "        y_v_pred = tfidf_atomic_L1.predict(x_val)\n",
    "        report = metrics.classification_report(y_val, y_v_pred, digits = 3, output_dict=True)\n",
    "        scores.append((C, report['weighted avg']['recall']))\n",
    "\n",
    "    # C value for best model on valid data\n",
    "    best_C = max(scores,key=lambda x:x[1])[0]\n",
    "    return best_C\n",
    "\n",
    "best_C = log_reg_val(x_trn = tfidf_train, y_trn = y_l1_trn, L2 = L2, y_val = y_l1_val, x_val = tfidf_valid)\n",
    "\n",
    "# Get results on test data for best model\n",
    "tfidf_atomic = LogisticRegression(multi_class = 'multinomial', penalty = 'l2', max_iter = 1500, solver = \"sag\", random_state = 42, C = best_C)\n",
    "tfidf_atomic_l1 = tfidf_atomic.fit(tfidf_train, y_l1_trn)\n",
    "y_pred = tfidf_atomic_l1.predict(tfidf_test)\n",
    "L1_test_report = metrics.classification_report(y_l1_tst, y_pred, digits = 3, output_dict=True)\n",
    "L1_test_report['weighted avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "23aa8ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C for tfidf tier1: 1.023292992280754\n"
     ]
    }
   ],
   "source": [
    "#BEST C Found for tfidf tier1: 1.023292992280754\n",
    "print(f'best C for tfidf tier1: {best_C}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0860306",
   "metadata": {},
   "source": [
    "### Predict level 2 icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f8d7971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [01:39<00:00, 99.61s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.5401008291740531,\n",
       " 'recall': 0.5593580819798917,\n",
       " 'f1-score': 0.5199440875488708,\n",
       " 'support': 10344}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multinomial Logistic Regression from sklearn \n",
    "\n",
    "# L2 values to iterate over; in order to run all iterations uncomment the line below\n",
    "L2 = np.logspace(.01, 100, num = 50)\n",
    "L2 = [1.023292992280754] # In order to test different L2 values comment out this line\n",
    "\n",
    "# Enumerate the l2 classes\n",
    "y_l2_dict = dict(zip(np.unique(train['y_l2'].values), np.arange(0, len(np.unique(train['y_l2'].values)), 1)))\n",
    "y_l2_trn = train['y_l2'].map(lambda x: y_l2_dict.get(x)).fillna('Missing')\n",
    "y_l2_val = valid['y_l2'].map(lambda x: y_l2_dict.get(x)).fillna('Missing')\n",
    "y_l2_tst = test['y_l2'].map(lambda x: y_l2_dict.get(x)).fillna('Missing')\n",
    "\n",
    "# Only include lvl2 labels that are in the training set\n",
    "tst_miss = [y_l2_tst != \"Missing\"]\n",
    "tst_idx_2 = np.where(y_l2_tst != 'Missing')\n",
    "y_l2_tst = np.array(y_l2_tst[y_l2_tst != \"Missing\"]).astype(int)\n",
    "\n",
    "val_miss = [y_l2_val != \"Missing\"]\n",
    "val_idx_2 = np.where(y_l2_val != 'Missing')\n",
    "y_l2_val = np.array(y_l2_val[y_l2_val != \"Missing\"]).astype(int)\n",
    "\n",
    "best_C = log_reg_val(x_trn = tfidf_train, y_trn = y_l2_trn, L2 = L2, y_val = y_l2_val, x_val = tfidf_valid[list(val_idx_2[0]), :])\n",
    "\n",
    "# Get results on test data for best model\n",
    "tfidf_atomic = LogisticRegression(multi_class = 'multinomial', penalty = 'l2', max_iter = 1500, solver = \"sag\", random_state = 42, C = best_C)\n",
    "tfidf_atomic_l2 = tfidf_atomic.fit(tfidf_train, y_l2_trn)\n",
    "y_pred = tfidf_atomic_l2.predict(tfidf_test[list(tst_idx_2[0]), :])\n",
    "L2_test_report = metrics.classification_report(y_l2_tst, y_pred, digits = 3, output_dict=True)\n",
    "L2_test_report['weighted avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7fa5e732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C for tfidf tier2: 1.023292992280754\n"
     ]
    }
   ],
   "source": [
    "# BEST C Found for tfidf tier2: 1.023292992280754\n",
    "print(f'best C for tfidf tier2: {best_C}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d4d31",
   "metadata": {},
   "source": [
    "### Predict level 3 icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "22270f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [05:17<00:00, 317.86s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.40406743146821095,\n",
       " 'recall': 0.45400737434504174,\n",
       " 'f1-score': 0.3950405405347697,\n",
       " 'support': 10306}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multinomial Logistic Regression from sklearn \n",
    "\n",
    "# L2 values to iterate over; in order to run all iterations uncomment the line below\n",
    "L2 = np.logspace(.01, 100, num = 50)\n",
    "L2 = [1.023292992280754] # In order to test different L2 values comment out this line\n",
    "\n",
    "# Enumerate the l3 classes\n",
    "train_miss = train['y_l3'].isna()\n",
    "train_l3 = train[~train_miss]\n",
    "y_l3_dict = dict(zip(set(train_l3['y_l3'].values), np.arange(0, len(set(train_l3['y_l3'].values)), 1)))\n",
    "\n",
    "y_l3_trn = train_l3['y_l3'].map(lambda x: y_l3_dict.get(x)).fillna('Missing')\n",
    "y_l3_val = valid['y_l3'].map(lambda x: y_l3_dict.get(x)).fillna('Missing')\n",
    "y_l3_tst = test['y_l3'].map(lambda x: y_l3_dict.get(x)).fillna('Missing')\n",
    "\n",
    "# Only include lvl2 labels that are in the training set\n",
    "tst_miss = [y_l3_tst != \"Missing\"]\n",
    "tst_idx_3 = np.where(y_l3_tst != 'Missing')\n",
    "y_l3_tst = np.array(y_l3_tst[y_l3_tst != \"Missing\"]).astype(int)\n",
    "\n",
    "val_miss = [y_l3_val != \"Missing\"]\n",
    "val_idx_3 = np.where(y_l3_val != 'Missing')\n",
    "y_l3_val = np.array(y_l3_val[y_l3_val != \"Missing\"]).astype(int)\n",
    "\n",
    "best_C = log_reg_val(x_trn = tfidf_train[~train_miss,:], y_trn = y_l3_trn, L2 = L2, y_val = y_l3_val, x_val = tfidf_valid[list(val_idx_3[0]), :])\n",
    "\n",
    "# Get results on test data for best model\n",
    "tfidf_atomic = LogisticRegression(multi_class = 'multinomial', penalty = 'l2', max_iter = 1500, solver = \"sag\", random_state = 42, C = best_C)\n",
    "tfidf_atomic_l3 = tfidf_atomic.fit(tfidf_train[~train_miss,:], y_l3_trn)\n",
    "y_pred = tfidf_atomic_l3.predict(tfidf_test[list(tst_idx_3[0]), :])\n",
    "L3_test_report = metrics.classification_report(y_l3_tst, y_pred, digits = 3, output_dict=True)\n",
    "L3_test_report['weighted avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c40b11df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C for tfidf tier3: 1.023292992280754\n"
     ]
    }
   ],
   "source": [
    "# BEST C Found for tfidf tier3:\n",
    "print(f'best C for tfidf tier3: {best_C}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0e964f",
   "metadata": {},
   "source": [
    "### Predict terminal icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ec8a3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:02<00:00,  2.98s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.043578096512380295,\n",
       " 'recall': 0.1340547703180212,\n",
       " 'f1-score': 0.05319495151327312,\n",
       " 'support': 4528}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multinomial Logistic Regression from sklearn \n",
    "\n",
    "# L2 values to iterate over; in order to run all iterations uncomment the line below\n",
    "L2 = np.logspace(.01, 100, num = 10)\n",
    "L2 = [1.023292992280754] # In order to test different L2 values comment out this line\n",
    "\n",
    "# Due to long training time; only predict a subset of the level 4 classes\n",
    "train_l4 = train[~train['y_l4'].isna()]\n",
    "\n",
    "# Get top X number of classes in the training data\n",
    "train_top = train_l4['y_l4'].value_counts()[0:32]\n",
    "\n",
    "# 16,869 data points after filtering for the top 32 classes \n",
    "train_l4 = train[train['y_l4'].isin(list(train_top.index))]\n",
    "\n",
    "y_l4_dict = dict(zip(set(train_l4['y_l4'].values), np.arange(0, len(set(train_l4['y_l4'].values)), 1)))\n",
    "\n",
    "train_l4 = train_l4[train['y_l4'].isin(list(train_top.index))]\n",
    "\n",
    "train_idx_4 = np.where(~train_l4['y_l4'].isna())\n",
    "train_l4 = train_l4[~train_l4['y_l4'].isna()]\n",
    "\n",
    "# Remove classes that are not in the dictionary\n",
    "y_l4_trn = train_l4['y_l4'].map(lambda x: y_l4_dict.get(x)).fillna('Missing')\n",
    "y_l4_val = valid['y_l4'].map(lambda x: y_l4_dict.get(x)).fillna('Missing')\n",
    "y_l4_tst = test['y_l4'].map(lambda x: y_l4_dict.get(x)).fillna('Missing')\n",
    "\n",
    "# Only include labels that are in the training set\n",
    "tst_miss = [y_l4_tst != \"Missing\"]\n",
    "tst_idx_4 = np.where(y_l4_tst != 'Missing')\n",
    "y_l4_tst = np.array(y_l4_tst[y_l4_tst != \"Missing\"]).astype(int)\n",
    "\n",
    "# Only include labels that are in the training set\n",
    "val_miss = [y_l4_val != \"Missing\"]\n",
    "val_idx_4 = np.where(y_l4_val != 'Missing')\n",
    "y_l4_val = np.array(y_l4_val[y_l4_val != \"Missing\"]).astype(int)\n",
    "\n",
    "# Test different l2 regularization coefficiants on the validation dataset \n",
    "best_C = log_reg_val(x_trn = tfidf_train[train_idx_4[0],:], y_trn = y_l4_trn, L2 = L2, y_val = y_l4_val, x_val = tfidf_valid[list(val_idx_4[0]), :])\n",
    "\n",
    "# Get results on test data for best model\n",
    "tfidf_atomic = LogisticRegression(multi_class = 'multinomial', penalty = 'l2', max_iter = 1500, solver = \"sag\", random_state = 42, C = best_C)\n",
    "tfidf_atomic_l4 = tfidf_atomic.fit(tfidf_train[~train_idx_4[0],:], y_l4_trn)\n",
    "y_pred = tfidf_atomic_l4.predict(tfidf_test[tst_idx_4[0], :])\n",
    "L4_test_report = metrics.classification_report(y_l4_tst, y_pred, digits = 3, output_dict=True)\n",
    "L4_test_report['weighted avg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe63853",
   "metadata": {},
   "source": [
    "## Mean-atomic\n",
    "\n",
    "### The mean embedding representation for the documents was input into a tensorflow softmax classifier to represent the labels atomically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "959488fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define NN\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "from numpy.random import seed\n",
    "\n",
    "def one_hot(y, c):\n",
    "    \n",
    "    # y--> label/ground truth.\n",
    "    # c--> Number of classes.\n",
    "    \n",
    "    # A zero matrix of size (m, c)\n",
    "    y_hot = np.zeros((len(y), c))\n",
    "    \n",
    "    # Putting 1 for column where the label is,\n",
    "    # Using multidimensional indexing.\n",
    "    y_hot[np.arange(len(y)), y] = 1\n",
    "    \n",
    "    return y_hot\n",
    "\n",
    "def nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate):\n",
    "    # initialize a tensorflow graph\n",
    "    graph = tf.Graph()\n",
    "  \n",
    "    with graph.as_default():\n",
    "        \"\"\"\n",
    "        defining all the nodes\n",
    "        \"\"\"\n",
    "    \n",
    "        # Inputs\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, num_features))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "        # Variables.\n",
    "        weights = tf.Variable(tf.truncated_normal([num_features, num_labels]))\n",
    "        biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "        # Training computation.\n",
    "        logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                            labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "        # Optimizer.\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "        # Predictions for the training, validation, and test data.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "        test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "\n",
    "    # utility function to calculate F1, precision, recall\n",
    "    def sm_metrics(predictions, labels):\n",
    "        y_pred = np.argmax(predictions, axis=1)\n",
    "        y_labs = np.argmax(labels, axis=1)\n",
    "        f1 = f1_score(y_labs, y_pred, average = 'weighted')\n",
    "        prec = precision_score(y_labs, y_pred, average = 'weighted')\n",
    "        rec = recall_score(y_labs, y_pred, average = 'weighted')\n",
    "        return f1, prec, rec\n",
    "    \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        # initialize weights and biases\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"Initialized\")\n",
    "    \n",
    "        for step in range(num_steps):\n",
    "            # pick a randomized offset\n",
    "            offset = np.random.randint(0, train_labels.shape[0] - batch_size - 1)\n",
    "    \n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "            # Prepare the feed dict\n",
    "            feed_dict = {tf_train_dataset : batch_data,\n",
    "                        tf_train_labels : batch_labels}\n",
    "    \n",
    "            # run one step of computation\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction],\n",
    "                                            feed_dict=feed_dict)\n",
    "    \n",
    "        print(\"\\n Test F1 Score: {:.3f}\".format(\n",
    "            sm_metrics(test_prediction.eval(), test_labels)[0]))\n",
    "        print(\"\\n Test Precision: {:.3f}\".format(\n",
    "            sm_metrics(test_prediction.eval(), test_labels)[1]))\n",
    "        print(\"\\n Test Recall: {:.3f}\".format(\n",
    "            sm_metrics(test_prediction.eval(), test_labels)[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138db98d",
   "metadata": {},
   "source": [
    "### Precict level 1 icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5433c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.566\n",
      "\n",
      " Test Precision: 0.558\n",
      "\n",
      " Test Recall: 0.598\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 200\n",
    "\n",
    "# number of target labels\n",
    "num_labels = len(np.unique(y_l1_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 5000\n",
    "\n",
    "\n",
    "  \n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['x_mean_emb'])['x_mean_emb'].apply(pd.Series)))\n",
    "train_labels = one_hot(y_l1_trn, num_labels)\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['x_mean_emb'])['x_mean_emb'].apply(pd.Series)))\n",
    "test_labels = one_hot(y_l1_tst, num_labels)\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['x_mean_emb'])['x_mean_emb'].apply(pd.Series)))\n",
    "valid_labels = one_hot(y_l1_val, num_labels)\n",
    "  \n",
    "\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e521018",
   "metadata": {},
   "source": [
    "### Precict level 2 icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e85b0200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.368\n",
      "\n",
      " Test Precision: 0.369\n",
      "\n",
      " Test Recall: 0.417\n"
     ]
    }
   ],
   "source": [
    "# Set the seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 200\n",
    "\n",
    "# number of target labels\n",
    "num_labels = len(np.unique(y_l2_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 5000\n",
    "  \n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['x_mean_emb'])['x_mean_emb'].apply(pd.Series)))\n",
    "train_labels = one_hot(y_l2_trn, num_labels)\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['x_mean_emb'][tst_idx_2[0]])['x_mean_emb'].apply(pd.Series)))\n",
    "test_labels = one_hot(y_l2_tst, num_labels)\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['x_mean_emb'][val_idx_2[0]])['x_mean_emb'].apply(pd.Series)))\n",
    "valid_labels = one_hot(y_l2_val, num_labels)\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7770d577",
   "metadata": {},
   "source": [
    "### Precict level 3 icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "06652bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.250\n",
      "\n",
      " Test Precision: 0.242\n",
      "\n",
      " Test Recall: 0.308\n"
     ]
    }
   ],
   "source": [
    "# Set the seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 200\n",
    "\n",
    "# number of target labels\n",
    "num_labels = len(np.unique(y_l3_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 5000\n",
    "\n",
    "train_idx = np.where(train_miss != True)\n",
    "  \n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['x_mean_emb'][train_idx[0]])['x_mean_emb'].apply(pd.Series)))\n",
    "train_labels = one_hot(y_l3_trn, num_labels)\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['x_mean_emb'][tst_idx_3[0]])['x_mean_emb'].apply(pd.Series)))\n",
    "test_labels = one_hot(y_l3_tst, num_labels)\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['x_mean_emb'][val_idx_3[0]])['x_mean_emb'].apply(pd.Series)))\n",
    "valid_labels = one_hot(y_l3_val, num_labels)\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b2f6a",
   "metadata": {},
   "source": [
    "### Precict terminal icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ec8b1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.047\n",
      "\n",
      " Test Precision: 0.054\n",
      "\n",
      " Test Recall: 0.155\n"
     ]
    }
   ],
   "source": [
    "# Set the seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 200\n",
    "\n",
    "# number of target labels\n",
    "num_labels = len(np.unique(y_l4_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 5000\n",
    "\n",
    "train_idx_4 = np.where(~train_l4['y_l4'].isna())\n",
    "train_l4 = train_l4[~train_l4['y_l4'].isna()]\n",
    "\n",
    "y_l4_dict = dict(zip(set(train_l4['y_l4'].values), np.arange(0, len(set(train_l4['y_l4'].values)), 1)))\n",
    "\n",
    "# Remove classes that are not in the dictionary\n",
    "y_l4_trn = train_l4['y_l4'].map(lambda x: y_l4_dict.get(x)).fillna('Missing')\n",
    "y_l4_val = valid['y_l4'].map(lambda x: y_l4_dict.get(x)).fillna('Missing')\n",
    "y_l4_tst = test['y_l4'].map(lambda x: y_l4_dict.get(x)).fillna('Missing')\n",
    "\n",
    "# Only include labels that are in the training set\n",
    "tst_idx_4 = np.where(y_l4_tst != 'Missing')\n",
    "y_l4_tst = np.array(y_l4_tst[y_l4_tst != \"Missing\"]).astype(int)\n",
    "\n",
    "# Only include labels that are in the training set\n",
    "val_idx_4 = np.where(y_l4_val != 'Missing')\n",
    "y_l4_val = np.array(y_l4_val[y_l4_val != \"Missing\"]).astype(int)\n",
    "  \n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['x_mean_emb'][train_idx_4[0]])['x_mean_emb'].apply(pd.Series)))\n",
    "train_labels = one_hot(y_l4_trn, num_labels)\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['x_mean_emb'][tst_idx_4[0]])['x_mean_emb'].apply(pd.Series)))\n",
    "test_labels = one_hot(y_l4_tst, num_labels)\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['x_mean_emb'][val_idx_4[0]])['x_mean_emb'].apply(pd.Series)))\n",
    "valid_labels = one_hot(y_l4_val, num_labels)\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c087ff3",
   "metadata": {},
   "source": [
    "# GRU Neural Network Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224952c2",
   "metadata": {},
   "source": [
    "### Prepare the Document data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "64689688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>HOPI</th>\n",
       "      <th>SENT_TOKENS</th>\n",
       "      <th>SENT_TOKENS_COMBO</th>\n",
       "      <th>idx_tokens</th>\n",
       "      <th>trunc_len</th>\n",
       "      <th>trunc_idx</th>\n",
       "      <th>...</th>\n",
       "      <th>y_2_emb_300</th>\n",
       "      <th>y_l3_tokens_300</th>\n",
       "      <th>y_l3_idx_300</th>\n",
       "      <th>y_3_emb_300</th>\n",
       "      <th>y_l4_tokens_300</th>\n",
       "      <th>y_l4_idx_300</th>\n",
       "      <th>y_4_emb_300</th>\n",
       "      <th>pad_idx</th>\n",
       "      <th>emb_tokens</th>\n",
       "      <th>gru_emb_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>121167</td>\n",
       "      <td>[41071, 41401, 4280, 4289, 4168, 42731, 25040,...</td>\n",
       "      <td>11161</td>\n",
       "      <td>Admission Date:  [**2119-3-8**]     Discharge ...</td>\n",
       "      <td>M D Dictated</td>\n",
       "      <td>[d, dictated, m]</td>\n",
       "      <td>d dictated m</td>\n",
       "      <td>[5954, 6531, 12240]</td>\n",
       "      <td>3</td>\n",
       "      <td>[487, 7132, 525]</td>\n",
       "      <td>...</td>\n",
       "      <td>[[-0.06420536, -0.073057145, 0.039527323, -0.0...</td>\n",
       "      <td>[Acute, myocardial, infarction]</td>\n",
       "      <td>[6, 449, 367]</td>\n",
       "      <td>[[0.027833814, 0.0047603794, 0.016183794, -0.0...</td>\n",
       "      <td>[Acute, myocardial, infarction,, subendocardia...</td>\n",
       "      <td>[17, 731, 609, 917, 608]</td>\n",
       "      <td>[[-0.031063288, 0.018695444, 0.012389156, 0.05...</td>\n",
       "      <td>[5954, 6531, 12240, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[-0.10389793, -0.05602143, 0.020491745, 0.111...</td>\n",
       "      <td>[[0.027763844, -0.09326028, -0.049879476, 0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>172162</td>\n",
       "      <td>[99811, 2851, 5990, 56982, 42832, 5781, 42731,...</td>\n",
       "      <td>43529</td>\n",
       "      <td>Admission Date:  [**2115-9-20**]              ...</td>\n",
       "      <td>simvastatin coumadin B</td>\n",
       "      <td>[b, coumadin, simvastatin]</td>\n",
       "      <td>b coumadin simvastatin</td>\n",
       "      <td>[3034, 5613, 18335]</td>\n",
       "      <td>3</td>\n",
       "      <td>[599, 1684, 5846]</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0.05226636, 0.101625346, -0.045206733, -0.03...</td>\n",
       "      <td>[Other, complications, of, procedures,, NEC]</td>\n",
       "      <td>[118, 260, 468, 510, 99]</td>\n",
       "      <td>[[-0.067778505, 0.12587744, -0.09941409, 0.086...</td>\n",
       "      <td>[Hemorrhage, or, hematoma, complicating, a, pr...</td>\n",
       "      <td>[135, 771, 567, 420, 276, 836]</td>\n",
       "      <td>[[-0.04979415, 0.030295637, 0.015073402, 0.077...</td>\n",
       "      <td>[3034, 5613, 18335, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0.063811064, -0.017807638, 0.07270181, -0.06...</td>\n",
       "      <td>[[-0.0258666, 0.013970568, -0.07689314, -0.001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108316</td>\n",
       "      <td>[1749, 4019, 2449, 53081]</td>\n",
       "      <td>20688</td>\n",
       "      <td>Admission Date: [**2159-4-9**]        Discharg...</td>\n",
       "      <td>Right breast cancer PHYSICAL</td>\n",
       "      <td>[breast, cancer, physical, right]</td>\n",
       "      <td>breast cancer physical right</td>\n",
       "      <td>[3764, 4100, 15230, 17458]</td>\n",
       "      <td>4</td>\n",
       "      <td>[3441, 2171, 190, 212]</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0.07328868, 0.024494926, 0.038389012, 0.1365...</td>\n",
       "      <td>[Malignant, neoplasm, of, female, breast]</td>\n",
       "      <td>[93, 453, 468, 322, 222]</td>\n",
       "      <td>[[-0.0068983953, 0.13078071, -0.122653775, -0....</td>\n",
       "      <td>[Malignant, neoplasm, of, breast, (female),, u...</td>\n",
       "      <td>[176, 739, 764, 368, 1019, 980]</td>\n",
       "      <td>[[-0.037677478, 0.020964323, 0.016462605, -0.0...</td>\n",
       "      <td>[3764, 4100, 15230, 17458, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[[-0.024922723, 0.03270357, 0.045428474, -0.02...</td>\n",
       "      <td>[[-0.0324082, 0.05123591, -0.06310296, -0.1345...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>185502</td>\n",
       "      <td>[1572, 1962]</td>\n",
       "      <td>9403</td>\n",
       "      <td>Admission Date:  [**2127-9-4**]       Discharg...</td>\n",
       "      <td>Distal pancreatic mass PHYSICAL</td>\n",
       "      <td>[distal, mass, pancreatic, physical]</td>\n",
       "      <td>distal mass pancreatic physical</td>\n",
       "      <td>[6799, 12436, 14640, 15230]</td>\n",
       "      <td>4</td>\n",
       "      <td>[1092, 156, 1911, 190]</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0.07328868, 0.024494926, 0.038389012, 0.1365...</td>\n",
       "      <td>[Malignant, neoplasm, of, pancreas]</td>\n",
       "      <td>[93, 453, 468, 480]</td>\n",
       "      <td>[[-0.0068983953, 0.13078071, -0.122653775, -0....</td>\n",
       "      <td>[Malignant, neoplasm, of, tail, of, pancreas]</td>\n",
       "      <td>[176, 739, 764, 932, 764, 785]</td>\n",
       "      <td>[[-0.037677478, 0.020964323, 0.016462605, -0.0...</td>\n",
       "      <td>[6799, 12436, 14640, 15230, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[-0.08239181, 0.016800813, 0.15329131, 0.0855...</td>\n",
       "      <td>[[-0.071386434, -0.09940714, -0.03916442, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110176</td>\n",
       "      <td>[53140, 2851, 25000, 2449, 4240, 2720, V113]</td>\n",
       "      <td>17540</td>\n",
       "      <td>Admission Date:  [**2157-7-21**]     Discharge...</td>\n",
       "      <td>Upper gastrointestinal bleed PHYSICAL</td>\n",
       "      <td>[bleed, gastrointestinal, physical, upper]</td>\n",
       "      <td>bleed gastrointestinal physical upper</td>\n",
       "      <td>[3513, 9025, 15230, 20854]</td>\n",
       "      <td>4</td>\n",
       "      <td>[1007, 828, 190, 792]</td>\n",
       "      <td>...</td>\n",
       "      <td>[[-0.07873408, -0.05463389, 0.029613571, 0.052...</td>\n",
       "      <td>[Gastric, ulcer]</td>\n",
       "      <td>[60, 601]</td>\n",
       "      <td>[[0.012612816, 0.06259193, 0.0023962534, -0.03...</td>\n",
       "      <td>[Chronic, or, unspecified, gastric, ulcer, wit...</td>\n",
       "      <td>[64, 771, 980, 547, 974, 1014, 569]</td>\n",
       "      <td>[[-0.050165668, 0.005476545, 0.14402065, 0.079...</td>\n",
       "      <td>[3513, 9025, 15230, 20854, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[[-0.029273838, 0.008564865, -0.033620443, -0....</td>\n",
       "      <td>[[-0.06115455, 0.046467472, -0.057267115, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                          ICD9_CODE  SUBJECT_ID  \\\n",
       "0   121167  [41071, 41401, 4280, 4289, 4168, 42731, 25040,...       11161   \n",
       "1   172162  [99811, 2851, 5990, 56982, 42832, 5781, 42731,...       43529   \n",
       "2   108316                          [1749, 4019, 2449, 53081]       20688   \n",
       "3   185502                                       [1572, 1962]        9403   \n",
       "4   110176       [53140, 2851, 25000, 2449, 4240, 2720, V113]       17540   \n",
       "\n",
       "                                                TEXT  \\\n",
       "0  Admission Date:  [**2119-3-8**]     Discharge ...   \n",
       "1  Admission Date:  [**2115-9-20**]              ...   \n",
       "2  Admission Date: [**2159-4-9**]        Discharg...   \n",
       "3  Admission Date:  [**2127-9-4**]       Discharg...   \n",
       "4  Admission Date:  [**2157-7-21**]     Discharge...   \n",
       "\n",
       "                                     HOPI  \\\n",
       "0                            M D Dictated   \n",
       "1                  simvastatin coumadin B   \n",
       "2            Right breast cancer PHYSICAL   \n",
       "3         Distal pancreatic mass PHYSICAL   \n",
       "4   Upper gastrointestinal bleed PHYSICAL   \n",
       "\n",
       "                                  SENT_TOKENS  \\\n",
       "0                            [d, dictated, m]   \n",
       "1                  [b, coumadin, simvastatin]   \n",
       "2           [breast, cancer, physical, right]   \n",
       "3        [distal, mass, pancreatic, physical]   \n",
       "4  [bleed, gastrointestinal, physical, upper]   \n",
       "\n",
       "                       SENT_TOKENS_COMBO                   idx_tokens  \\\n",
       "0                           d dictated m          [5954, 6531, 12240]   \n",
       "1                 b coumadin simvastatin          [3034, 5613, 18335]   \n",
       "2           breast cancer physical right   [3764, 4100, 15230, 17458]   \n",
       "3        distal mass pancreatic physical  [6799, 12436, 14640, 15230]   \n",
       "4  bleed gastrointestinal physical upper   [3513, 9025, 15230, 20854]   \n",
       "\n",
       "   trunc_len               trunc_idx  ...  \\\n",
       "0          3        [487, 7132, 525]  ...   \n",
       "1          3       [599, 1684, 5846]  ...   \n",
       "2          4  [3441, 2171, 190, 212]  ...   \n",
       "3          4  [1092, 156, 1911, 190]  ...   \n",
       "4          4   [1007, 828, 190, 792]  ...   \n",
       "\n",
       "                                         y_2_emb_300  \\\n",
       "0  [[-0.06420536, -0.073057145, 0.039527323, -0.0...   \n",
       "1  [[0.05226636, 0.101625346, -0.045206733, -0.03...   \n",
       "2  [[0.07328868, 0.024494926, 0.038389012, 0.1365...   \n",
       "3  [[0.07328868, 0.024494926, 0.038389012, 0.1365...   \n",
       "4  [[-0.07873408, -0.05463389, 0.029613571, 0.052...   \n",
       "\n",
       "                                y_l3_tokens_300              y_l3_idx_300  \\\n",
       "0               [Acute, myocardial, infarction]             [6, 449, 367]   \n",
       "1  [Other, complications, of, procedures,, NEC]  [118, 260, 468, 510, 99]   \n",
       "2     [Malignant, neoplasm, of, female, breast]  [93, 453, 468, 322, 222]   \n",
       "3           [Malignant, neoplasm, of, pancreas]       [93, 453, 468, 480]   \n",
       "4                              [Gastric, ulcer]                 [60, 601]   \n",
       "\n",
       "                                         y_3_emb_300  \\\n",
       "0  [[0.027833814, 0.0047603794, 0.016183794, -0.0...   \n",
       "1  [[-0.067778505, 0.12587744, -0.09941409, 0.086...   \n",
       "2  [[-0.0068983953, 0.13078071, -0.122653775, -0....   \n",
       "3  [[-0.0068983953, 0.13078071, -0.122653775, -0....   \n",
       "4  [[0.012612816, 0.06259193, 0.0023962534, -0.03...   \n",
       "\n",
       "                                     y_l4_tokens_300  \\\n",
       "0  [Acute, myocardial, infarction,, subendocardia...   \n",
       "1  [Hemorrhage, or, hematoma, complicating, a, pr...   \n",
       "2  [Malignant, neoplasm, of, breast, (female),, u...   \n",
       "3      [Malignant, neoplasm, of, tail, of, pancreas]   \n",
       "4  [Chronic, or, unspecified, gastric, ulcer, wit...   \n",
       "\n",
       "                          y_l4_idx_300  \\\n",
       "0             [17, 731, 609, 917, 608]   \n",
       "1       [135, 771, 567, 420, 276, 836]   \n",
       "2      [176, 739, 764, 368, 1019, 980]   \n",
       "3       [176, 739, 764, 932, 764, 785]   \n",
       "4  [64, 771, 980, 547, 974, 1014, 569]   \n",
       "\n",
       "                                         y_4_emb_300  \\\n",
       "0  [[-0.031063288, 0.018695444, 0.012389156, 0.05...   \n",
       "1  [[-0.04979415, 0.030295637, 0.015073402, 0.077...   \n",
       "2  [[-0.037677478, 0.020964323, 0.016462605, -0.0...   \n",
       "3  [[-0.037677478, 0.020964323, 0.016462605, -0.0...   \n",
       "4  [[-0.050165668, 0.005476545, 0.14402065, 0.079...   \n",
       "\n",
       "                                             pad_idx  \\\n",
       "0  [5954, 6531, 12240, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1  [3034, 5613, 18335, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2  [3764, 4100, 15230, 17458, 0, 0, 0, 0, 0, 0, 0...   \n",
       "3  [6799, 12436, 14640, 15230, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [3513, 9025, 15230, 20854, 0, 0, 0, 0, 0, 0, 0...   \n",
       "\n",
       "                                          emb_tokens  \\\n",
       "0  [[-0.10389793, -0.05602143, 0.020491745, 0.111...   \n",
       "1  [[0.063811064, -0.017807638, 0.07270181, -0.06...   \n",
       "2  [[-0.024922723, 0.03270357, 0.045428474, -0.02...   \n",
       "3  [[-0.08239181, 0.016800813, 0.15329131, 0.0855...   \n",
       "4  [[-0.029273838, 0.008564865, -0.033620443, -0....   \n",
       "\n",
       "                                      gru_emb_tokens  \n",
       "0  [[0.027763844, -0.09326028, -0.049879476, 0.05...  \n",
       "1  [[-0.0258666, 0.013970568, -0.07689314, -0.001...  \n",
       "2  [[-0.0324082, 0.05123591, -0.06310296, -0.1345...  \n",
       "3  [[-0.071386434, -0.09940714, -0.03916442, -0.0...  \n",
       "4  [[-0.06115455, 0.046467472, -0.057267115, -0.0...  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the training data and split it buckets of ~4000 tokens per bucket \n",
    "train['trunc_len'] = train['idx_tokens'].map(lambda c: len(c))\n",
    "sort = train.sort_values(by = ['trunc_len'])\n",
    "\n",
    "trn_bucket1 = sort[0:4000].reset_index(drop = True)\n",
    "trn_bucket2 = sort[4001:8001].reset_index(drop = True)\n",
    "trn_bucket3 = sort[8002:12002].reset_index(drop = True)\n",
    "trn_bucket4 = sort[12003:16003].reset_index(drop = True)\n",
    "trn_bucket5 = sort[16004:20004].reset_index(drop = True)\n",
    "trn_bucket6 = sort[20005:24005].reset_index(drop = True)\n",
    "trn_bucket7 = sort[24006:28006].reset_index(drop = True)\n",
    "trn_bucket8 = sort[28007:32007].reset_index(drop = True)\n",
    "trn_bucket9 = sort[32008:33008].reset_index(drop = True)\n",
    "trn_bucket10 = sort[33009:34009].reset_index(drop = True)\n",
    "# trn_bucket11 = sort[34010:35010].reset_index(drop = True)\n",
    "# trn_bucket12 = sort[35011:36008].reset_index(drop = True)\n",
    "\n",
    "valid['trunc_len'] = valid['idx_tokens'].map(lambda c: len(c))\n",
    "val_sort = valid.sort_values(by = ['trunc_len'])\n",
    "val_bucket1 = val_sort[0:4000].reset_index(drop = True)\n",
    "val_bucket2 = val_sort[4001:].reset_index(drop = True)\n",
    "\n",
    "\n",
    "test['trunc_len'] = test['idx_tokens'].map(lambda c: len(c))\n",
    "test_sort = test.sort_values(by = ['trunc_len'])\n",
    "test_bucket1 = test_sort[0:4000].reset_index(drop = True)\n",
    "test_bucket2 = test_sort[4001:8001].reset_index(drop = True)\n",
    "test_bucket3 = test_sort[8002:].reset_index(drop = True)\n",
    "\n",
    "# train_buckets = [trn_bucket1, trn_bucket2, trn_bucket3, trn_bucket4, trn_bucket5, trn_bucket6, trn_bucket7, trn_bucket8, trn_bucket9, trn_bucket10, trn_bucket11, trn_bucket12]\n",
    "train_buckets = [trn_bucket1, trn_bucket2, trn_bucket3, trn_bucket4, trn_bucket5, trn_bucket6, trn_bucket7, trn_bucket8, trn_bucket9, trn_bucket10]\n",
    "\n",
    "valid_buckets = [val_bucket1, val_bucket2]\n",
    "\n",
    "test_buckets = [test_bucket1, test_bucket2, test_bucket3]\n",
    "\n",
    "\n",
    "buckets = train_buckets + valid_buckets + test_buckets\n",
    "\n",
    "# Add paddings to docs in each bucket\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras import Sequential\n",
    "\n",
    "bi_grus = torch.nn.GRU(input_size=200, hidden_size=50, num_layers=1, dropout=.3, batch_first=False, bidirectional=True)\n",
    "\n",
    "# Pad the word embeddings in each bucket so that they are all the same length\n",
    "for b in train_buckets: \n",
    "    t = pad_sequences([torch.tensor(x) for x in b['idx_tokens']], padding='post')\n",
    "    b['pad_idx'] = t.tolist()\n",
    "    b['emb_tokens'] = b['pad_idx'].map(lambda t: [np.array(embed.weight.data[w]) for w in t])\n",
    "    b['gru_emb_tokens'] = b['emb_tokens'].map(lambda t: bi_grus(torch.tensor(t))[0].detach().numpy())\n",
    "\n",
    "\n",
    "trn_bucket1.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "825a6472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>HOPI</th>\n",
       "      <th>SENT_TOKENS</th>\n",
       "      <th>SENT_TOKENS_COMBO</th>\n",
       "      <th>idx_tokens</th>\n",
       "      <th>trunc_len</th>\n",
       "      <th>trunc_idx</th>\n",
       "      <th>...</th>\n",
       "      <th>y_2_emb_300</th>\n",
       "      <th>y_l3_tokens_300</th>\n",
       "      <th>y_l3_idx_300</th>\n",
       "      <th>y_3_emb_300</th>\n",
       "      <th>y_l4_tokens_300</th>\n",
       "      <th>y_l4_idx_300</th>\n",
       "      <th>y_4_emb_300</th>\n",
       "      <th>pad_idx</th>\n",
       "      <th>emb_tokens</th>\n",
       "      <th>gru_emb_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>155817</td>\n",
       "      <td>[41401, 42731, 4139, 9975, 78820, 2766, 3963, ...</td>\n",
       "      <td>19218</td>\n",
       "      <td>Admission Date:  [**2150-12-21**]       Discha...</td>\n",
       "      <td>M D Dictated</td>\n",
       "      <td>[d, dictated, m]</td>\n",
       "      <td>d dictated m</td>\n",
       "      <td>[5954, 6531, 12240]</td>\n",
       "      <td>3</td>\n",
       "      <td>[487, 7132, 525]</td>\n",
       "      <td>...</td>\n",
       "      <td>[[-0.06420536, -0.073057145, 0.039527323, -0.0...</td>\n",
       "      <td>[Other, forms, of, chronic, ischemic, heart, d...</td>\n",
       "      <td>[118, 328, 468, 251, 387, 346, 280]</td>\n",
       "      <td>[[-0.067778505, 0.12587744, -0.09941409, 0.086...</td>\n",
       "      <td>[Coronary, atherosclerosis]</td>\n",
       "      <td>[76, 345]</td>\n",
       "      <td>[[-0.07078964, 0.10904761, 0.009895855, 0.0866...</td>\n",
       "      <td>[5954, 6531, 12240, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[-0.10389793, -0.05602143, 0.020491745, 0.111...</td>\n",
       "      <td>[[0.027763844, -0.09326028, -0.049879476, 0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>158782</td>\n",
       "      <td>[27401, 42823, 2762, 41400, 78820, 4280, 25082...</td>\n",
       "      <td>40911</td>\n",
       "      <td>Admission Date:  [**2122-2-20**]              ...</td>\n",
       "      <td>EAST HOSPITAL MEDICINE ATTENDING ADMISSION NOTE</td>\n",
       "      <td>[admission, attending, east, hospital, medicin...</td>\n",
       "      <td>admission attending east hospital medicine note</td>\n",
       "      <td>[1785, 2875, 7249, 10002, 12566, 13831]</td>\n",
       "      <td>6</td>\n",
       "      <td>[679, 3437, 9760, 20, 729, 535]</td>\n",
       "      <td>...</td>\n",
       "      <td>[[-0.067243345, 0.0037505117, -0.019873692, 0....</td>\n",
       "      <td>[Gout]</td>\n",
       "      <td>[626]</td>\n",
       "      <td>[[-0.15437363, -0.0025372824, 0.054457933, 0.1...</td>\n",
       "      <td>[Gouty, arthropathy]</td>\n",
       "      <td>[1019, 338]</td>\n",
       "      <td>[[-0.06621397, -0.038178395, 0.032522444, -0.0...</td>\n",
       "      <td>[1785, 2875, 7249, 10002, 12566, 13831, 0, 0, ...</td>\n",
       "      <td>[[0.11660509, 0.023656836, 0.18028942, 0.03354...</td>\n",
       "      <td>[[-0.056399833, 0.020561969, -0.022728601, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>147821</td>\n",
       "      <td>[1888, 1981, 2639, 19882, 4019]</td>\n",
       "      <td>11193</td>\n",
       "      <td>Admission Date:  [**2133-1-20**]       Dischar...</td>\n",
       "      <td>Transitional cell carcioma of the bladder PHY...</td>\n",
       "      <td>[bladder, carcioma, cell, of, physical, the, t...</td>\n",
       "      <td>bladder carcioma cell of physical the transiti...</td>\n",
       "      <td>[3489, 21878, 4424, 14079, 15230, 19852, 20329]</td>\n",
       "      <td>7</td>\n",
       "      <td>[2240, 85, 29, 190, 46, 5622]</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0.07328868, 0.024494926, 0.038389012, 0.1365...</td>\n",
       "      <td>[Malignant, neoplasm, of, bladder]</td>\n",
       "      <td>[93, 453, 468, 217]</td>\n",
       "      <td>[[-0.0068983953, 0.13078071, -0.122653775, -0....</td>\n",
       "      <td>[Malignant, neoplasm, of, other, specified, si...</td>\n",
       "      <td>[176, 739, 764, 780, 898, 887, 764, 357]</td>\n",
       "      <td>[[-0.037677478, 0.020964323, 0.016462605, -0.0...</td>\n",
       "      <td>[3489, 21878, 4424, 14079, 15230, 19852, 20329...</td>\n",
       "      <td>[[0.008587209, 0.0070398315, -0.010075485, -0....</td>\n",
       "      <td>[[-0.057249315, 0.04795399, -0.081635386, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>172825</td>\n",
       "      <td>[73600, 4019]</td>\n",
       "      <td>1629</td>\n",
       "      <td>Admission Date: [**2158-4-6**]        Discharg...</td>\n",
       "      <td>Gunshot wound to the right hand PHYSICAL</td>\n",
       "      <td>[gunshot, hand, physical, right, the, to, wound]</td>\n",
       "      <td>gunshot hand physical right the to wound</td>\n",
       "      <td>[9412, 9481, 15230, 17458, 19852, 20108, 21700]</td>\n",
       "      <td>7</td>\n",
       "      <td>[2752, 1345, 190, 212, 46, 47, 286]</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0.01820074, 0.028762022, -0.06730661, 0.0485...</td>\n",
       "      <td>[Other, acquired, deformities, of, limbs]</td>\n",
       "      <td>[118, 170, 269, 468, 404]</td>\n",
       "      <td>[[-0.067778505, 0.12587744, -0.09941409, 0.086...</td>\n",
       "      <td>[Acquired, deformities, of, forearm,, excludin...</td>\n",
       "      <td>[16, 447, 764, 539, 1019, 1019]</td>\n",
       "      <td>[[-0.00055701315, 0.10010993, 0.019460596, 0.0...</td>\n",
       "      <td>[9412, 9481, 15230, 17458, 19852, 20108, 21700...</td>\n",
       "      <td>[[-0.026091304, 7.5319134e-05, -0.06066603, -0...</td>\n",
       "      <td>[[-0.01597621, -0.06653981, -0.044673093, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>121183</td>\n",
       "      <td>[V3000, 76515, 769, 7707, 7742, 7793, 1123, 69...</td>\n",
       "      <td>7657</td>\n",
       "      <td>Admission Date: [**2172-3-14**]        Dischar...</td>\n",
       "      <td>A 29 week gestation infant admitted with resp...</td>\n",
       "      <td>[a, admitted, distress, gestation, infant, mat...</td>\n",
       "      <td>a admitted distress gestation infant maternal ...</td>\n",
       "      <td>[1424, 1793, 6821, 9116, 10751, 12459, 17222, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>[0, 299, 2292, 617, 626, 6029, 207, 445, 248]</td>\n",
       "      <td>...</td>\n",
       "      <td>[[-0.050854385, 0.104824334, -0.08132969, -0.0...</td>\n",
       "      <td>[Single, liveborn]</td>\n",
       "      <td>[141, 407]</td>\n",
       "      <td>[[0.05149791, 0.056716908, -0.02219789, -0.023...</td>\n",
       "      <td>[Single, liveborn,, born, in, hospital]</td>\n",
       "      <td>[235, 671, 365, 604, 578]</td>\n",
       "      <td>[[-0.051121183, 0.06648078, 0.017757863, -0.06...</td>\n",
       "      <td>[1424, 1793, 6821, 9116, 10751, 12459, 17222, ...</td>\n",
       "      <td>[[-0.008632651, 0.07423111, 0.21304595, 0.0201...</td>\n",
       "      <td>[[-0.046334267, 0.06169137, 0.001709951, -0.00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                          ICD9_CODE  SUBJECT_ID  \\\n",
       "0   155817  [41401, 42731, 4139, 9975, 78820, 2766, 3963, ...       19218   \n",
       "1   158782  [27401, 42823, 2762, 41400, 78820, 4280, 25082...       40911   \n",
       "2   147821                    [1888, 1981, 2639, 19882, 4019]       11193   \n",
       "3   172825                                      [73600, 4019]        1629   \n",
       "4   121183  [V3000, 76515, 769, 7707, 7742, 7793, 1123, 69...        7657   \n",
       "\n",
       "                                                TEXT  \\\n",
       "0  Admission Date:  [**2150-12-21**]       Discha...   \n",
       "1  Admission Date:  [**2122-2-20**]              ...   \n",
       "2  Admission Date:  [**2133-1-20**]       Dischar...   \n",
       "3  Admission Date: [**2158-4-6**]        Discharg...   \n",
       "4  Admission Date: [**2172-3-14**]        Dischar...   \n",
       "\n",
       "                                                HOPI  \\\n",
       "0                                       M D Dictated   \n",
       "1    EAST HOSPITAL MEDICINE ATTENDING ADMISSION NOTE   \n",
       "2   Transitional cell carcioma of the bladder PHY...   \n",
       "3           Gunshot wound to the right hand PHYSICAL   \n",
       "4   A 29 week gestation infant admitted with resp...   \n",
       "\n",
       "                                         SENT_TOKENS  \\\n",
       "0                                   [d, dictated, m]   \n",
       "1  [admission, attending, east, hospital, medicin...   \n",
       "2  [bladder, carcioma, cell, of, physical, the, t...   \n",
       "3   [gunshot, hand, physical, right, the, to, wound]   \n",
       "4  [a, admitted, distress, gestation, infant, mat...   \n",
       "\n",
       "                                   SENT_TOKENS_COMBO  \\\n",
       "0                                       d dictated m   \n",
       "1    admission attending east hospital medicine note   \n",
       "2  bladder carcioma cell of physical the transiti...   \n",
       "3           gunshot hand physical right the to wound   \n",
       "4  a admitted distress gestation infant maternal ...   \n",
       "\n",
       "                                          idx_tokens  trunc_len  \\\n",
       "0                                [5954, 6531, 12240]          3   \n",
       "1            [1785, 2875, 7249, 10002, 12566, 13831]          6   \n",
       "2    [3489, 21878, 4424, 14079, 15230, 19852, 20329]          7   \n",
       "3    [9412, 9481, 15230, 17458, 19852, 20108, 21700]          7   \n",
       "4  [1424, 1793, 6821, 9116, 10751, 12459, 17222, ...          9   \n",
       "\n",
       "                                       trunc_idx  ...  \\\n",
       "0                               [487, 7132, 525]  ...   \n",
       "1                [679, 3437, 9760, 20, 729, 535]  ...   \n",
       "2                  [2240, 85, 29, 190, 46, 5622]  ...   \n",
       "3            [2752, 1345, 190, 212, 46, 47, 286]  ...   \n",
       "4  [0, 299, 2292, 617, 626, 6029, 207, 445, 248]  ...   \n",
       "\n",
       "                                         y_2_emb_300  \\\n",
       "0  [[-0.06420536, -0.073057145, 0.039527323, -0.0...   \n",
       "1  [[-0.067243345, 0.0037505117, -0.019873692, 0....   \n",
       "2  [[0.07328868, 0.024494926, 0.038389012, 0.1365...   \n",
       "3  [[0.01820074, 0.028762022, -0.06730661, 0.0485...   \n",
       "4  [[-0.050854385, 0.104824334, -0.08132969, -0.0...   \n",
       "\n",
       "                                     y_l3_tokens_300  \\\n",
       "0  [Other, forms, of, chronic, ischemic, heart, d...   \n",
       "1                                             [Gout]   \n",
       "2                 [Malignant, neoplasm, of, bladder]   \n",
       "3          [Other, acquired, deformities, of, limbs]   \n",
       "4                                 [Single, liveborn]   \n",
       "\n",
       "                          y_l3_idx_300  \\\n",
       "0  [118, 328, 468, 251, 387, 346, 280]   \n",
       "1                                [626]   \n",
       "2                  [93, 453, 468, 217]   \n",
       "3            [118, 170, 269, 468, 404]   \n",
       "4                           [141, 407]   \n",
       "\n",
       "                                         y_3_emb_300  \\\n",
       "0  [[-0.067778505, 0.12587744, -0.09941409, 0.086...   \n",
       "1  [[-0.15437363, -0.0025372824, 0.054457933, 0.1...   \n",
       "2  [[-0.0068983953, 0.13078071, -0.122653775, -0....   \n",
       "3  [[-0.067778505, 0.12587744, -0.09941409, 0.086...   \n",
       "4  [[0.05149791, 0.056716908, -0.02219789, -0.023...   \n",
       "\n",
       "                                     y_l4_tokens_300  \\\n",
       "0                        [Coronary, atherosclerosis]   \n",
       "1                               [Gouty, arthropathy]   \n",
       "2  [Malignant, neoplasm, of, other, specified, si...   \n",
       "3  [Acquired, deformities, of, forearm,, excludin...   \n",
       "4            [Single, liveborn,, born, in, hospital]   \n",
       "\n",
       "                               y_l4_idx_300  \\\n",
       "0                                 [76, 345]   \n",
       "1                               [1019, 338]   \n",
       "2  [176, 739, 764, 780, 898, 887, 764, 357]   \n",
       "3           [16, 447, 764, 539, 1019, 1019]   \n",
       "4                 [235, 671, 365, 604, 578]   \n",
       "\n",
       "                                         y_4_emb_300  \\\n",
       "0  [[-0.07078964, 0.10904761, 0.009895855, 0.0866...   \n",
       "1  [[-0.06621397, -0.038178395, 0.032522444, -0.0...   \n",
       "2  [[-0.037677478, 0.020964323, 0.016462605, -0.0...   \n",
       "3  [[-0.00055701315, 0.10010993, 0.019460596, 0.0...   \n",
       "4  [[-0.051121183, 0.06648078, 0.017757863, -0.06...   \n",
       "\n",
       "                                             pad_idx  \\\n",
       "0  [5954, 6531, 12240, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1  [1785, 2875, 7249, 10002, 12566, 13831, 0, 0, ...   \n",
       "2  [3489, 21878, 4424, 14079, 15230, 19852, 20329...   \n",
       "3  [9412, 9481, 15230, 17458, 19852, 20108, 21700...   \n",
       "4  [1424, 1793, 6821, 9116, 10751, 12459, 17222, ...   \n",
       "\n",
       "                                          emb_tokens  \\\n",
       "0  [[-0.10389793, -0.05602143, 0.020491745, 0.111...   \n",
       "1  [[0.11660509, 0.023656836, 0.18028942, 0.03354...   \n",
       "2  [[0.008587209, 0.0070398315, -0.010075485, -0....   \n",
       "3  [[-0.026091304, 7.5319134e-05, -0.06066603, -0...   \n",
       "4  [[-0.008632651, 0.07423111, 0.21304595, 0.0201...   \n",
       "\n",
       "                                      gru_emb_tokens  \n",
       "0  [[0.027763844, -0.09326028, -0.049879476, 0.05...  \n",
       "1  [[-0.056399833, 0.020561969, -0.022728601, -0....  \n",
       "2  [[-0.057249315, 0.04795399, -0.081635386, -0.0...  \n",
       "3  [[-0.01597621, -0.06653981, -0.044673093, -0.0...  \n",
       "4  [[-0.046334267, 0.06169137, 0.001709951, -0.00...  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up valid and test\n",
    "valid_and_test_buckets = valid_buckets + test_buckets\n",
    "\n",
    "for b in valid_and_test_buckets: \n",
    "    t = pad_sequences([torch.tensor(x) for x in b['idx_tokens']], padding='post')\n",
    "    b['pad_idx'] = t.tolist()\n",
    "    b['emb_tokens'] = b['pad_idx'].map(lambda t: [np.array(embed.weight.data[w]) for w in t])\n",
    "    b['gru_emb_tokens'] = b['emb_tokens'].map(lambda t: bi_grus(torch.tensor(t))[0].detach().numpy())\n",
    "\n",
    "\n",
    "test_bucket1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2f0b16",
   "metadata": {},
   "source": [
    "## prep data 2 get mean of gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f8c32a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>HOPI</th>\n",
       "      <th>SENT_TOKENS</th>\n",
       "      <th>SENT_TOKENS_COMBO</th>\n",
       "      <th>idx_tokens</th>\n",
       "      <th>trunc_len</th>\n",
       "      <th>trunc_idx</th>\n",
       "      <th>...</th>\n",
       "      <th>y_l3_tokens_300</th>\n",
       "      <th>y_l3_idx_300</th>\n",
       "      <th>y_3_emb_300</th>\n",
       "      <th>y_l4_tokens_300</th>\n",
       "      <th>y_l4_idx_300</th>\n",
       "      <th>y_4_emb_300</th>\n",
       "      <th>pad_idx</th>\n",
       "      <th>emb_tokens</th>\n",
       "      <th>gru_emb_tokens</th>\n",
       "      <th>mean_gru_emb_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>121167</td>\n",
       "      <td>[41071, 41401, 4280, 4289, 4168, 42731, 25040,...</td>\n",
       "      <td>11161</td>\n",
       "      <td>Admission Date:  [**2119-3-8**]     Discharge ...</td>\n",
       "      <td>M D Dictated</td>\n",
       "      <td>[d, dictated, m]</td>\n",
       "      <td>d dictated m</td>\n",
       "      <td>[5954, 6531, 12240]</td>\n",
       "      <td>3</td>\n",
       "      <td>[487, 7132, 525]</td>\n",
       "      <td>...</td>\n",
       "      <td>[Acute, myocardial, infarction]</td>\n",
       "      <td>[6, 449, 367]</td>\n",
       "      <td>[[0.027833814, 0.0047603794, 0.016183794, -0.0...</td>\n",
       "      <td>[Acute, myocardial, infarction,, subendocardia...</td>\n",
       "      <td>[17, 731, 609, 917, 608]</td>\n",
       "      <td>[[-0.031063288, 0.018695444, 0.012389156, 0.05...</td>\n",
       "      <td>[5954, 6531, 12240, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[-0.10389793, -0.05602143, 0.020491745, 0.111...</td>\n",
       "      <td>[[0.027763844, -0.09326028, -0.049879476, 0.05...</td>\n",
       "      <td>[0.017632322, -0.10008569, -0.12182482, -0.075...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>172162</td>\n",
       "      <td>[99811, 2851, 5990, 56982, 42832, 5781, 42731,...</td>\n",
       "      <td>43529</td>\n",
       "      <td>Admission Date:  [**2115-9-20**]              ...</td>\n",
       "      <td>simvastatin coumadin B</td>\n",
       "      <td>[b, coumadin, simvastatin]</td>\n",
       "      <td>b coumadin simvastatin</td>\n",
       "      <td>[3034, 5613, 18335]</td>\n",
       "      <td>3</td>\n",
       "      <td>[599, 1684, 5846]</td>\n",
       "      <td>...</td>\n",
       "      <td>[Other, complications, of, procedures,, NEC]</td>\n",
       "      <td>[118, 260, 468, 510, 99]</td>\n",
       "      <td>[[-0.067778505, 0.12587744, -0.09941409, 0.086...</td>\n",
       "      <td>[Hemorrhage, or, hematoma, complicating, a, pr...</td>\n",
       "      <td>[135, 771, 567, 420, 276, 836]</td>\n",
       "      <td>[[-0.04979415, 0.030295637, 0.015073402, 0.077...</td>\n",
       "      <td>[3034, 5613, 18335, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0.063811064, -0.017807638, 0.07270181, -0.06...</td>\n",
       "      <td>[[-0.0258666, 0.013970568, -0.07689314, -0.001...</td>\n",
       "      <td>[0.016230721, -0.094705455, -0.12330444, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108316</td>\n",
       "      <td>[1749, 4019, 2449, 53081]</td>\n",
       "      <td>20688</td>\n",
       "      <td>Admission Date: [**2159-4-9**]        Discharg...</td>\n",
       "      <td>Right breast cancer PHYSICAL</td>\n",
       "      <td>[breast, cancer, physical, right]</td>\n",
       "      <td>breast cancer physical right</td>\n",
       "      <td>[3764, 4100, 15230, 17458]</td>\n",
       "      <td>4</td>\n",
       "      <td>[3441, 2171, 190, 212]</td>\n",
       "      <td>...</td>\n",
       "      <td>[Malignant, neoplasm, of, female, breast]</td>\n",
       "      <td>[93, 453, 468, 322, 222]</td>\n",
       "      <td>[[-0.0068983953, 0.13078071, -0.122653775, -0....</td>\n",
       "      <td>[Malignant, neoplasm, of, breast, (female),, u...</td>\n",
       "      <td>[176, 739, 764, 368, 1019, 980]</td>\n",
       "      <td>[[-0.037677478, 0.020964323, 0.016462605, -0.0...</td>\n",
       "      <td>[3764, 4100, 15230, 17458, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[[-0.024922723, 0.03270357, 0.045428474, -0.02...</td>\n",
       "      <td>[[-0.0324082, 0.05123591, -0.06310296, -0.1345...</td>\n",
       "      <td>[0.021504043, -0.09192504, -0.1198625, -0.0809...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>185502</td>\n",
       "      <td>[1572, 1962]</td>\n",
       "      <td>9403</td>\n",
       "      <td>Admission Date:  [**2127-9-4**]       Discharg...</td>\n",
       "      <td>Distal pancreatic mass PHYSICAL</td>\n",
       "      <td>[distal, mass, pancreatic, physical]</td>\n",
       "      <td>distal mass pancreatic physical</td>\n",
       "      <td>[6799, 12436, 14640, 15230]</td>\n",
       "      <td>4</td>\n",
       "      <td>[1092, 156, 1911, 190]</td>\n",
       "      <td>...</td>\n",
       "      <td>[Malignant, neoplasm, of, pancreas]</td>\n",
       "      <td>[93, 453, 468, 480]</td>\n",
       "      <td>[[-0.0068983953, 0.13078071, -0.122653775, -0....</td>\n",
       "      <td>[Malignant, neoplasm, of, tail, of, pancreas]</td>\n",
       "      <td>[176, 739, 764, 932, 764, 785]</td>\n",
       "      <td>[[-0.037677478, 0.020964323, 0.016462605, -0.0...</td>\n",
       "      <td>[6799, 12436, 14640, 15230, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[-0.08239181, 0.016800813, 0.15329131, 0.0855...</td>\n",
       "      <td>[[-0.071386434, -0.09940714, -0.03916442, -0.0...</td>\n",
       "      <td>[0.014165727, -0.09899124, -0.12228402, -0.077...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110176</td>\n",
       "      <td>[53140, 2851, 25000, 2449, 4240, 2720, V113]</td>\n",
       "      <td>17540</td>\n",
       "      <td>Admission Date:  [**2157-7-21**]     Discharge...</td>\n",
       "      <td>Upper gastrointestinal bleed PHYSICAL</td>\n",
       "      <td>[bleed, gastrointestinal, physical, upper]</td>\n",
       "      <td>bleed gastrointestinal physical upper</td>\n",
       "      <td>[3513, 9025, 15230, 20854]</td>\n",
       "      <td>4</td>\n",
       "      <td>[1007, 828, 190, 792]</td>\n",
       "      <td>...</td>\n",
       "      <td>[Gastric, ulcer]</td>\n",
       "      <td>[60, 601]</td>\n",
       "      <td>[[0.012612816, 0.06259193, 0.0023962534, -0.03...</td>\n",
       "      <td>[Chronic, or, unspecified, gastric, ulcer, wit...</td>\n",
       "      <td>[64, 771, 980, 547, 974, 1014, 569]</td>\n",
       "      <td>[[-0.050165668, 0.005476545, 0.14402065, 0.079...</td>\n",
       "      <td>[3513, 9025, 15230, 20854, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[[-0.029273838, 0.008564865, -0.033620443, -0....</td>\n",
       "      <td>[[-0.06115455, 0.046467472, -0.057267115, -0.0...</td>\n",
       "      <td>[0.019398564, -0.09190198, -0.12314307, -0.079...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                          ICD9_CODE  SUBJECT_ID  \\\n",
       "0   121167  [41071, 41401, 4280, 4289, 4168, 42731, 25040,...       11161   \n",
       "1   172162  [99811, 2851, 5990, 56982, 42832, 5781, 42731,...       43529   \n",
       "2   108316                          [1749, 4019, 2449, 53081]       20688   \n",
       "3   185502                                       [1572, 1962]        9403   \n",
       "4   110176       [53140, 2851, 25000, 2449, 4240, 2720, V113]       17540   \n",
       "\n",
       "                                                TEXT  \\\n",
       "0  Admission Date:  [**2119-3-8**]     Discharge ...   \n",
       "1  Admission Date:  [**2115-9-20**]              ...   \n",
       "2  Admission Date: [**2159-4-9**]        Discharg...   \n",
       "3  Admission Date:  [**2127-9-4**]       Discharg...   \n",
       "4  Admission Date:  [**2157-7-21**]     Discharge...   \n",
       "\n",
       "                                     HOPI  \\\n",
       "0                            M D Dictated   \n",
       "1                  simvastatin coumadin B   \n",
       "2            Right breast cancer PHYSICAL   \n",
       "3         Distal pancreatic mass PHYSICAL   \n",
       "4   Upper gastrointestinal bleed PHYSICAL   \n",
       "\n",
       "                                  SENT_TOKENS  \\\n",
       "0                            [d, dictated, m]   \n",
       "1                  [b, coumadin, simvastatin]   \n",
       "2           [breast, cancer, physical, right]   \n",
       "3        [distal, mass, pancreatic, physical]   \n",
       "4  [bleed, gastrointestinal, physical, upper]   \n",
       "\n",
       "                       SENT_TOKENS_COMBO                   idx_tokens  \\\n",
       "0                           d dictated m          [5954, 6531, 12240]   \n",
       "1                 b coumadin simvastatin          [3034, 5613, 18335]   \n",
       "2           breast cancer physical right   [3764, 4100, 15230, 17458]   \n",
       "3        distal mass pancreatic physical  [6799, 12436, 14640, 15230]   \n",
       "4  bleed gastrointestinal physical upper   [3513, 9025, 15230, 20854]   \n",
       "\n",
       "   trunc_len               trunc_idx  ...  \\\n",
       "0          3        [487, 7132, 525]  ...   \n",
       "1          3       [599, 1684, 5846]  ...   \n",
       "2          4  [3441, 2171, 190, 212]  ...   \n",
       "3          4  [1092, 156, 1911, 190]  ...   \n",
       "4          4   [1007, 828, 190, 792]  ...   \n",
       "\n",
       "                                y_l3_tokens_300              y_l3_idx_300  \\\n",
       "0               [Acute, myocardial, infarction]             [6, 449, 367]   \n",
       "1  [Other, complications, of, procedures,, NEC]  [118, 260, 468, 510, 99]   \n",
       "2     [Malignant, neoplasm, of, female, breast]  [93, 453, 468, 322, 222]   \n",
       "3           [Malignant, neoplasm, of, pancreas]       [93, 453, 468, 480]   \n",
       "4                              [Gastric, ulcer]                 [60, 601]   \n",
       "\n",
       "                                         y_3_emb_300  \\\n",
       "0  [[0.027833814, 0.0047603794, 0.016183794, -0.0...   \n",
       "1  [[-0.067778505, 0.12587744, -0.09941409, 0.086...   \n",
       "2  [[-0.0068983953, 0.13078071, -0.122653775, -0....   \n",
       "3  [[-0.0068983953, 0.13078071, -0.122653775, -0....   \n",
       "4  [[0.012612816, 0.06259193, 0.0023962534, -0.03...   \n",
       "\n",
       "                                     y_l4_tokens_300  \\\n",
       "0  [Acute, myocardial, infarction,, subendocardia...   \n",
       "1  [Hemorrhage, or, hematoma, complicating, a, pr...   \n",
       "2  [Malignant, neoplasm, of, breast, (female),, u...   \n",
       "3      [Malignant, neoplasm, of, tail, of, pancreas]   \n",
       "4  [Chronic, or, unspecified, gastric, ulcer, wit...   \n",
       "\n",
       "                          y_l4_idx_300  \\\n",
       "0             [17, 731, 609, 917, 608]   \n",
       "1       [135, 771, 567, 420, 276, 836]   \n",
       "2      [176, 739, 764, 368, 1019, 980]   \n",
       "3       [176, 739, 764, 932, 764, 785]   \n",
       "4  [64, 771, 980, 547, 974, 1014, 569]   \n",
       "\n",
       "                                         y_4_emb_300  \\\n",
       "0  [[-0.031063288, 0.018695444, 0.012389156, 0.05...   \n",
       "1  [[-0.04979415, 0.030295637, 0.015073402, 0.077...   \n",
       "2  [[-0.037677478, 0.020964323, 0.016462605, -0.0...   \n",
       "3  [[-0.037677478, 0.020964323, 0.016462605, -0.0...   \n",
       "4  [[-0.050165668, 0.005476545, 0.14402065, 0.079...   \n",
       "\n",
       "                                             pad_idx  \\\n",
       "0  [5954, 6531, 12240, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1  [3034, 5613, 18335, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2  [3764, 4100, 15230, 17458, 0, 0, 0, 0, 0, 0, 0...   \n",
       "3  [6799, 12436, 14640, 15230, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [3513, 9025, 15230, 20854, 0, 0, 0, 0, 0, 0, 0...   \n",
       "\n",
       "                                          emb_tokens  \\\n",
       "0  [[-0.10389793, -0.05602143, 0.020491745, 0.111...   \n",
       "1  [[0.063811064, -0.017807638, 0.07270181, -0.06...   \n",
       "2  [[-0.024922723, 0.03270357, 0.045428474, -0.02...   \n",
       "3  [[-0.08239181, 0.016800813, 0.15329131, 0.0855...   \n",
       "4  [[-0.029273838, 0.008564865, -0.033620443, -0....   \n",
       "\n",
       "                                      gru_emb_tokens  \\\n",
       "0  [[0.027763844, -0.09326028, -0.049879476, 0.05...   \n",
       "1  [[-0.0258666, 0.013970568, -0.07689314, -0.001...   \n",
       "2  [[-0.0324082, 0.05123591, -0.06310296, -0.1345...   \n",
       "3  [[-0.071386434, -0.09940714, -0.03916442, -0.0...   \n",
       "4  [[-0.06115455, 0.046467472, -0.057267115, -0.0...   \n",
       "\n",
       "                                 mean_gru_emb_tokens  \n",
       "0  [0.017632322, -0.10008569, -0.12182482, -0.075...  \n",
       "1  [0.016230721, -0.094705455, -0.12330444, -0.07...  \n",
       "2  [0.021504043, -0.09192504, -0.1198625, -0.0809...  \n",
       "3  [0.014165727, -0.09899124, -0.12228402, -0.077...  \n",
       "4  [0.019398564, -0.09190198, -0.12314307, -0.079...  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preping adding mean gru of doc \n",
    "for b in buckets: \n",
    "    b['mean_gru_emb_tokens'] = b['gru_emb_tokens'].map(lambda t: np.average(t, axis = 0))\n",
    "    # print(b['mean_gru_emb_tokens'][0])\n",
    "\n",
    "\n",
    "trn_bucket1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d0b1c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset train test and valid with the combined\n",
    "train = pd.concat(train_buckets, ignore_index=True)\n",
    "valid = pd.concat(valid_buckets, ignore_index=True)\n",
    "test = pd.concat(test_buckets, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373daa93",
   "metadata": {},
   "source": [
    "# Prep the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0d5825c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>HOPI</th>\n",
       "      <th>SENT_TOKENS</th>\n",
       "      <th>SENT_TOKENS_COMBO</th>\n",
       "      <th>idx_tokens</th>\n",
       "      <th>trunc_len</th>\n",
       "      <th>trunc_idx</th>\n",
       "      <th>...</th>\n",
       "      <th>gru_emb_tokens</th>\n",
       "      <th>mean_gru_emb_tokens</th>\n",
       "      <th>pad_y_1_emb_100</th>\n",
       "      <th>mean_y_1_emb</th>\n",
       "      <th>pad_y_2_emb_100</th>\n",
       "      <th>mean_y_2_emb</th>\n",
       "      <th>pad_y_3_emb_100</th>\n",
       "      <th>mean_y_3_emb</th>\n",
       "      <th>pad_y_4_emb_100</th>\n",
       "      <th>mean_y_4_emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>121167</td>\n",
       "      <td>[41071, 41401, 4280, 4289, 4168, 42731, 25040,...</td>\n",
       "      <td>11161</td>\n",
       "      <td>Admission Date:  [**2119-3-8**]     Discharge ...</td>\n",
       "      <td>M D Dictated</td>\n",
       "      <td>[d, dictated, m]</td>\n",
       "      <td>d dictated m</td>\n",
       "      <td>[5954, 6531, 12240]</td>\n",
       "      <td>3</td>\n",
       "      <td>[487, 7132, 525]</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0.027763844, -0.09326028, -0.049879476, 0.05...</td>\n",
       "      <td>[0.017632322, -0.10008569, -0.12182482, -0.075...</td>\n",
       "      <td>[17, 38, 57, 9, 56, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[-0.026314517, 0.006925957, 0.022143701, 0.028...</td>\n",
       "      <td>[146, 115, 75, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[-0.018956209, 0.019395605, 0.014926362, -0.00...</td>\n",
       "      <td>[7, 494, 411, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[0.0046419976, 0.011544917, -0.0017760558, -0....</td>\n",
       "      <td>[17, 731, 609, 917, 608, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-0.042732865, -0.013204128, 0.0010981254, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>172162</td>\n",
       "      <td>[99811, 2851, 5990, 56982, 42832, 5781, 42731,...</td>\n",
       "      <td>43529</td>\n",
       "      <td>Admission Date:  [**2115-9-20**]              ...</td>\n",
       "      <td>simvastatin coumadin B</td>\n",
       "      <td>[b, coumadin, simvastatin]</td>\n",
       "      <td>b coumadin simvastatin</td>\n",
       "      <td>[3034, 5613, 18335]</td>\n",
       "      <td>3</td>\n",
       "      <td>[599, 1684, 5846]</td>\n",
       "      <td>...</td>\n",
       "      <td>[[-0.0258666, 0.013970568, -0.07689314, -0.001...</td>\n",
       "      <td>[0.016230721, -0.094705455, -0.12330444, -0.07...</td>\n",
       "      <td>[30, 1, 44, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0.019214593, -0.01637872, 0.012677816, 0.0027...</td>\n",
       "      <td>[59, 190, 254, 16, 160, 41, 186, 87, 56, 0, 0,...</td>\n",
       "      <td>[0.05546665, 0.025918746, 0.020460205, 0.06047...</td>\n",
       "      <td>[130, 293, 514, 560, 109, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[-0.009753919, 0.0026840866, 0.0001926033, 0.0...</td>\n",
       "      <td>[135, 771, 567, 420, 276, 836, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0015863212, 0.03231821, 0.012854504, 0.0152...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108316</td>\n",
       "      <td>[1749, 4019, 2449, 53081]</td>\n",
       "      <td>20688</td>\n",
       "      <td>Admission Date: [**2159-4-9**]        Discharg...</td>\n",
       "      <td>Right breast cancer PHYSICAL</td>\n",
       "      <td>[breast, cancer, physical, right]</td>\n",
       "      <td>breast cancer physical right</td>\n",
       "      <td>[3764, 4100, 15230, 17458]</td>\n",
       "      <td>4</td>\n",
       "      <td>[3441, 2171, 190, 212]</td>\n",
       "      <td>...</td>\n",
       "      <td>[[-0.0324082, 0.05123591, -0.06310296, -0.1345...</td>\n",
       "      <td>[0.021504043, -0.09192504, -0.1198625, -0.0809...</td>\n",
       "      <td>[34, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[-0.013332017, 0.01341917, -0.010905109, -0.00...</td>\n",
       "      <td>[159, 172, 190, 34, 62, 268, 241, 16, 35, 0, 0...</td>\n",
       "      <td>[0.0049688, -0.008121339, 0.0063727186, 0.0042...</td>\n",
       "      <td>[103, 499, 514, 362, 255, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[-0.015918791, 0.00014563788, -0.012790434, -0...</td>\n",
       "      <td>[176, 739, 764, 368, 1019, 980, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[-0.012351215, -0.009038208, 0.028531745, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>185502</td>\n",
       "      <td>[1572, 1962]</td>\n",
       "      <td>9403</td>\n",
       "      <td>Admission Date:  [**2127-9-4**]       Discharg...</td>\n",
       "      <td>Distal pancreatic mass PHYSICAL</td>\n",
       "      <td>[distal, mass, pancreatic, physical]</td>\n",
       "      <td>distal mass pancreatic physical</td>\n",
       "      <td>[6799, 12436, 14640, 15230]</td>\n",
       "      <td>4</td>\n",
       "      <td>[1092, 156, 1911, 190]</td>\n",
       "      <td>...</td>\n",
       "      <td>[[-0.071386434, -0.09940714, -0.03916442, -0.0...</td>\n",
       "      <td>[0.014165727, -0.09899124, -0.12228402, -0.077...</td>\n",
       "      <td>[34, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[-0.013332017, 0.01341917, -0.010905109, -0.00...</td>\n",
       "      <td>[159, 172, 190, 74, 195, 16, 210, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.014117454, 0.0040834034, 0.027410908, 0.017...</td>\n",
       "      <td>[103, 499, 514, 527, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[-0.0103078075, -0.0015769633, -0.010901566, -...</td>\n",
       "      <td>[176, 739, 764, 932, 764, 785, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-0.01023323, -0.0017431829, 0.033358965, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110176</td>\n",
       "      <td>[53140, 2851, 25000, 2449, 4240, 2720, V113]</td>\n",
       "      <td>17540</td>\n",
       "      <td>Admission Date:  [**2157-7-21**]     Discharge...</td>\n",
       "      <td>Upper gastrointestinal bleed PHYSICAL</td>\n",
       "      <td>[bleed, gastrointestinal, physical, upper]</td>\n",
       "      <td>bleed gastrointestinal physical upper</td>\n",
       "      <td>[3513, 9025, 15230, 20854]</td>\n",
       "      <td>4</td>\n",
       "      <td>[1007, 828, 190, 792]</td>\n",
       "      <td>...</td>\n",
       "      <td>[[-0.06115455, 0.046467472, -0.057267115, -0.0...</td>\n",
       "      <td>[0.019398564, -0.09190198, -0.12314307, -0.079...</td>\n",
       "      <td>[17, 38, 57, 16, 56, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[-0.026246002, 0.0066486686, 0.02111801, 0.028...</td>\n",
       "      <td>[76, 190, 93, 249, 16, 83, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[0.02366254, 0.015644196, 0.0011059324, 0.0164...</td>\n",
       "      <td>[66, 662, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-0.0018712672, 0.005444214, 0.00023833559, -0...</td>\n",
       "      <td>[64, 771, 980, 547, 974, 1014, 569, 0, 0, 0, 0...</td>\n",
       "      <td>[-0.014470133, -0.0051942724, 0.00265404, 0.01...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                          ICD9_CODE  SUBJECT_ID  \\\n",
       "0   121167  [41071, 41401, 4280, 4289, 4168, 42731, 25040,...       11161   \n",
       "1   172162  [99811, 2851, 5990, 56982, 42832, 5781, 42731,...       43529   \n",
       "2   108316                          [1749, 4019, 2449, 53081]       20688   \n",
       "3   185502                                       [1572, 1962]        9403   \n",
       "4   110176       [53140, 2851, 25000, 2449, 4240, 2720, V113]       17540   \n",
       "\n",
       "                                                TEXT  \\\n",
       "0  Admission Date:  [**2119-3-8**]     Discharge ...   \n",
       "1  Admission Date:  [**2115-9-20**]              ...   \n",
       "2  Admission Date: [**2159-4-9**]        Discharg...   \n",
       "3  Admission Date:  [**2127-9-4**]       Discharg...   \n",
       "4  Admission Date:  [**2157-7-21**]     Discharge...   \n",
       "\n",
       "                                     HOPI  \\\n",
       "0                            M D Dictated   \n",
       "1                  simvastatin coumadin B   \n",
       "2            Right breast cancer PHYSICAL   \n",
       "3         Distal pancreatic mass PHYSICAL   \n",
       "4   Upper gastrointestinal bleed PHYSICAL   \n",
       "\n",
       "                                  SENT_TOKENS  \\\n",
       "0                            [d, dictated, m]   \n",
       "1                  [b, coumadin, simvastatin]   \n",
       "2           [breast, cancer, physical, right]   \n",
       "3        [distal, mass, pancreatic, physical]   \n",
       "4  [bleed, gastrointestinal, physical, upper]   \n",
       "\n",
       "                       SENT_TOKENS_COMBO                   idx_tokens  \\\n",
       "0                           d dictated m          [5954, 6531, 12240]   \n",
       "1                 b coumadin simvastatin          [3034, 5613, 18335]   \n",
       "2           breast cancer physical right   [3764, 4100, 15230, 17458]   \n",
       "3        distal mass pancreatic physical  [6799, 12436, 14640, 15230]   \n",
       "4  bleed gastrointestinal physical upper   [3513, 9025, 15230, 20854]   \n",
       "\n",
       "   trunc_len               trunc_idx  ...  \\\n",
       "0          3        [487, 7132, 525]  ...   \n",
       "1          3       [599, 1684, 5846]  ...   \n",
       "2          4  [3441, 2171, 190, 212]  ...   \n",
       "3          4  [1092, 156, 1911, 190]  ...   \n",
       "4          4   [1007, 828, 190, 792]  ...   \n",
       "\n",
       "                                      gru_emb_tokens  \\\n",
       "0  [[0.027763844, -0.09326028, -0.049879476, 0.05...   \n",
       "1  [[-0.0258666, 0.013970568, -0.07689314, -0.001...   \n",
       "2  [[-0.0324082, 0.05123591, -0.06310296, -0.1345...   \n",
       "3  [[-0.071386434, -0.09940714, -0.03916442, -0.0...   \n",
       "4  [[-0.06115455, 0.046467472, -0.057267115, -0.0...   \n",
       "\n",
       "                                 mean_gru_emb_tokens  \\\n",
       "0  [0.017632322, -0.10008569, -0.12182482, -0.075...   \n",
       "1  [0.016230721, -0.094705455, -0.12330444, -0.07...   \n",
       "2  [0.021504043, -0.09192504, -0.1198625, -0.0809...   \n",
       "3  [0.014165727, -0.09899124, -0.12228402, -0.077...   \n",
       "4  [0.019398564, -0.09190198, -0.12314307, -0.079...   \n",
       "\n",
       "                             pad_y_1_emb_100  \\\n",
       "0   [17, 38, 57, 9, 56, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1     [30, 1, 44, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2      [34, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "3      [34, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4  [17, 38, 57, 16, 56, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                        mean_y_1_emb  \\\n",
       "0  [-0.026314517, 0.006925957, 0.022143701, 0.028...   \n",
       "1  [0.019214593, -0.01637872, 0.012677816, 0.0027...   \n",
       "2  [-0.013332017, 0.01341917, -0.010905109, -0.00...   \n",
       "3  [-0.013332017, 0.01341917, -0.010905109, -0.00...   \n",
       "4  [-0.026246002, 0.0066486686, 0.02111801, 0.028...   \n",
       "\n",
       "                                     pad_y_2_emb_100  \\\n",
       "0  [146, 115, 75, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "1  [59, 190, 254, 16, 160, 41, 186, 87, 56, 0, 0,...   \n",
       "2  [159, 172, 190, 34, 62, 268, 241, 16, 35, 0, 0...   \n",
       "3  [159, 172, 190, 74, 195, 16, 210, 0, 0, 0, 0, ...   \n",
       "4  [76, 190, 93, 249, 16, 83, 0, 0, 0, 0, 0, 0, 0...   \n",
       "\n",
       "                                        mean_y_2_emb  \\\n",
       "0  [-0.018956209, 0.019395605, 0.014926362, -0.00...   \n",
       "1  [0.05546665, 0.025918746, 0.020460205, 0.06047...   \n",
       "2  [0.0049688, -0.008121339, 0.0063727186, 0.0042...   \n",
       "3  [0.014117454, 0.0040834034, 0.027410908, 0.017...   \n",
       "4  [0.02366254, 0.015644196, 0.0011059324, 0.0164...   \n",
       "\n",
       "                                     pad_y_3_emb_100  \\\n",
       "0  [7, 494, 411, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1  [130, 293, 514, 560, 109, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2  [103, 499, 514, 362, 255, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "3  [103, 499, 514, 527, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "4  [66, 662, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                        mean_y_3_emb  \\\n",
       "0  [0.0046419976, 0.011544917, -0.0017760558, -0....   \n",
       "1  [-0.009753919, 0.0026840866, 0.0001926033, 0.0...   \n",
       "2  [-0.015918791, 0.00014563788, -0.012790434, -0...   \n",
       "3  [-0.0103078075, -0.0015769633, -0.010901566, -...   \n",
       "4  [-0.0018712672, 0.005444214, 0.00023833559, -0...   \n",
       "\n",
       "                                     pad_y_4_emb_100  \\\n",
       "0  [17, 731, 609, 917, 608, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [135, 771, 567, 420, 276, 836, 0, 0, 0, 0, 0, ...   \n",
       "2  [176, 739, 764, 368, 1019, 980, 0, 0, 0, 0, 0,...   \n",
       "3  [176, 739, 764, 932, 764, 785, 0, 0, 0, 0, 0, ...   \n",
       "4  [64, 771, 980, 547, 974, 1014, 569, 0, 0, 0, 0...   \n",
       "\n",
       "                                        mean_y_4_emb  \n",
       "0  [-0.042732865, -0.013204128, 0.0010981254, 0.0...  \n",
       "1  [0.0015863212, 0.03231821, 0.012854504, 0.0152...  \n",
       "2  [-0.012351215, -0.009038208, 0.028531745, -0.0...  \n",
       "3  [-0.01023323, -0.0017431829, 0.033358965, -0.0...  \n",
       "4  [-0.014470133, -0.0051942724, 0.00265404, 0.01...  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean embed for labels\n",
    "\n",
    "levels = [1, 2, 3, 4]\n",
    "embed_sizes = [100]\n",
    "\n",
    "dict_emb = {\n",
    "    \"y_l1\": lvl1_embed,\n",
    "    \"y_l2\": lvl2_embed,\n",
    "    \"y_l3\": lvl3_embed,\n",
    "    \"y_l4\": lvl4_embed\n",
    "}\n",
    "\n",
    "# for level in levels:\n",
    "#     for embed_size in embed_sizes:\n",
    "#         embeddings = dict_emb.get(f'y_l{level}_{embed_size}')\n",
    "#         for b in [train, valid, test]:\n",
    "#             t = pad_sequences([torch.tensor(x) for x in b[f'y_l{level}_idx_{embed_size}']], padding='post')\n",
    "#             b[f'pad_y_{level}_emb_{embed_size}'] = t.tolist()\n",
    "#             b[f'y_{level}_pre_gru_emb_tokens'] = b[f'pad_y_{level}_emb_{embed_size}'].map(lambda t: [np.array(embeddings[w]) for w in t])\n",
    "\n",
    "for level in levels:\n",
    "    for embed_size in embed_sizes:\n",
    "        embeddings = dict_emb.get(f'y_l{level}')\n",
    "        for b in [train, valid, test]:\n",
    "            t = pad_sequences([torch.tensor(x) for x in b[f'y_l{level}_idx_{embed_size}']], padding='post')\n",
    "            b[f'pad_y_{level}_emb_{embed_size}'] = t.tolist()\n",
    "            b[f'mean_y_{level}_emb'] = b[f'pad_y_{level}_emb_{embed_size}'].map(lambda t: np.average([np.array(embeddings[w]) for w in t], axis = 0))\n",
    "        # train[f\"mean_y_{level}_emb\"] = train[f\"y_{level}_emb_{embed_size}\"].map(lambda t: np.average(t, axis = 0))\n",
    "        # valid[f\"mean_y_{level}_emb\"] = valid[f\"y_{level}_emb_{embed_size}\"].map(lambda t: np.average(t, axis = 0))\n",
    "        # test[f\"mean_y_{level}_emb\"] = test[f\"y_{level}_emb_{embed_size}\"].map(lambda t: np.average(t, axis = 0))\n",
    "\n",
    "train.head(5)       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3c8b0f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>HOPI</th>\n",
       "      <th>SENT_TOKENS</th>\n",
       "      <th>SENT_TOKENS_COMBO</th>\n",
       "      <th>idx_tokens</th>\n",
       "      <th>trunc_len</th>\n",
       "      <th>trunc_idx</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_gru_emb_tokens_y_1</th>\n",
       "      <th>pad_y_2_emb_300</th>\n",
       "      <th>y_2_pre_gru_emb_tokens</th>\n",
       "      <th>mean_gru_emb_tokens_y_2</th>\n",
       "      <th>pad_y_3_emb_300</th>\n",
       "      <th>y_3_pre_gru_emb_tokens</th>\n",
       "      <th>mean_gru_emb_tokens_y_3</th>\n",
       "      <th>pad_y_4_emb_300</th>\n",
       "      <th>y_4_pre_gru_emb_tokens</th>\n",
       "      <th>mean_gru_emb_tokens_y_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>121167</td>\n",
       "      <td>[41071, 41401, 4280, 4289, 4168, 42731, 25040,...</td>\n",
       "      <td>11161</td>\n",
       "      <td>Admission Date:  [**2119-3-8**]     Discharge ...</td>\n",
       "      <td>M D Dictated</td>\n",
       "      <td>[d, dictated, m]</td>\n",
       "      <td>d dictated m</td>\n",
       "      <td>[5954, 6531, 12240]</td>\n",
       "      <td>3</td>\n",
       "      <td>[487, 7132, 525]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.059478458, -0.045454238, -0.0017273542, 0.0...</td>\n",
       "      <td>[139, 111, 71, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[[-0.06420536, -0.073057145, 0.039527323, -0.0...</td>\n",
       "      <td>[0.06914873, -0.039207477, -0.022554472, 0.102...</td>\n",
       "      <td>[6, 449, 367, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0.027833814, 0.0047603794, 0.016183794, -0.0...</td>\n",
       "      <td>[0.07076735, -0.03935192, -0.02299309, 0.09065...</td>\n",
       "      <td>[17, 731, 609, 917, 608, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[-0.031063288, 0.018695444, 0.012389156, 0.05...</td>\n",
       "      <td>[0.060728285, -0.037503116, -0.014770587, 0.11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>172162</td>\n",
       "      <td>[99811, 2851, 5990, 56982, 42832, 5781, 42731,...</td>\n",
       "      <td>43529</td>\n",
       "      <td>Admission Date:  [**2115-9-20**]              ...</td>\n",
       "      <td>simvastatin coumadin B</td>\n",
       "      <td>[b, coumadin, simvastatin]</td>\n",
       "      <td>b coumadin simvastatin</td>\n",
       "      <td>[3034, 5613, 18335]</td>\n",
       "      <td>3</td>\n",
       "      <td>[599, 1684, 5846]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.060690463, -0.04955192, -0.018655159, 0.079...</td>\n",
       "      <td>[57, 181, 245, 16, 152, 39, 178, 83, 54, 0, 0,...</td>\n",
       "      <td>[[0.05226636, 0.101625346, -0.045206733, -0.03...</td>\n",
       "      <td>[0.073901184, -0.04959332, -0.007015997, 0.094...</td>\n",
       "      <td>[118, 260, 468, 510, 99, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[-0.067778505, 0.12587744, -0.09941409, 0.086...</td>\n",
       "      <td>[0.0522563, -0.04090999, -0.0073791984, 0.0927...</td>\n",
       "      <td>[135, 771, 567, 420, 276, 836, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[-0.04979415, 0.030295637, 0.015073402, 0.077...</td>\n",
       "      <td>[0.07409605, -0.048002392, -0.026867218, 0.093...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108316</td>\n",
       "      <td>[1749, 4019, 2449, 53081]</td>\n",
       "      <td>20688</td>\n",
       "      <td>Admission Date: [**2159-4-9**]        Discharg...</td>\n",
       "      <td>Right breast cancer PHYSICAL</td>\n",
       "      <td>[breast, cancer, physical, right]</td>\n",
       "      <td>breast cancer physical right</td>\n",
       "      <td>[3764, 4100, 15230, 17458]</td>\n",
       "      <td>4</td>\n",
       "      <td>[3441, 2171, 190, 212]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.06731365, -0.04114383, -0.018741077, 0.0937...</td>\n",
       "      <td>[151, 164, 181, 33, 60, 259, 232, 16, 34, 0, 0...</td>\n",
       "      <td>[[0.07328868, 0.024494926, 0.038389012, 0.1365...</td>\n",
       "      <td>[0.055725317, -0.050798643, -0.020149566, 0.10...</td>\n",
       "      <td>[93, 453, 468, 322, 222, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[-0.0068983953, 0.13078071, -0.122653775, -0....</td>\n",
       "      <td>[0.063069336, -0.05080911, -0.016955597, 0.086...</td>\n",
       "      <td>[176, 739, 764, 368, 1019, 980, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[-0.037677478, 0.020964323, 0.016462605, -0.0...</td>\n",
       "      <td>[0.06460077, -0.048875496, -0.019074915, 0.098...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>185502</td>\n",
       "      <td>[1572, 1962]</td>\n",
       "      <td>9403</td>\n",
       "      <td>Admission Date:  [**2127-9-4**]       Discharg...</td>\n",
       "      <td>Distal pancreatic mass PHYSICAL</td>\n",
       "      <td>[distal, mass, pancreatic, physical]</td>\n",
       "      <td>distal mass pancreatic physical</td>\n",
       "      <td>[6799, 12436, 14640, 15230]</td>\n",
       "      <td>4</td>\n",
       "      <td>[1092, 156, 1911, 190]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.06731365, -0.04114383, -0.018741077, 0.0937...</td>\n",
       "      <td>[151, 164, 181, 70, 186, 16, 201, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0.07328868, 0.024494926, 0.038389012, 0.1365...</td>\n",
       "      <td>[0.05280675, -0.03909461, -0.010505624, 0.0953...</td>\n",
       "      <td>[93, 453, 468, 480, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[-0.0068983953, 0.13078071, -0.122653775, -0....</td>\n",
       "      <td>[0.06486169, -0.047038384, -0.016133888, 0.087...</td>\n",
       "      <td>[176, 739, 764, 932, 764, 785, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[-0.037677478, 0.020964323, 0.016462605, -0.0...</td>\n",
       "      <td>[0.058684308, -0.050492644, -0.02025734, 0.092...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110176</td>\n",
       "      <td>[53140, 2851, 25000, 2449, 4240, 2720, V113]</td>\n",
       "      <td>17540</td>\n",
       "      <td>Admission Date:  [**2157-7-21**]     Discharge...</td>\n",
       "      <td>Upper gastrointestinal bleed PHYSICAL</td>\n",
       "      <td>[bleed, gastrointestinal, physical, upper]</td>\n",
       "      <td>bleed gastrointestinal physical upper</td>\n",
       "      <td>[3513, 9025, 15230, 20854]</td>\n",
       "      <td>4</td>\n",
       "      <td>[1007, 828, 190, 792]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.059089392, -0.045568395, -0.0015976013, 0.0...</td>\n",
       "      <td>[72, 181, 89, 240, 16, 79, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[[-0.07873408, -0.05463389, 0.029613571, 0.052...</td>\n",
       "      <td>[0.06548796, -0.040969513, -0.019570673, 0.093...</td>\n",
       "      <td>[60, 601, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0.012612816, 0.06259193, 0.0023962534, -0.03...</td>\n",
       "      <td>[0.061512876, -0.041455172, -0.020982469, 0.09...</td>\n",
       "      <td>[64, 771, 980, 547, 974, 1014, 569, 0, 0, 0, 0...</td>\n",
       "      <td>[[-0.050165668, 0.005476545, 0.14402065, 0.079...</td>\n",
       "      <td>[0.0946536, -0.058965012, -0.023040982, 0.0944...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                          ICD9_CODE  SUBJECT_ID  \\\n",
       "0   121167  [41071, 41401, 4280, 4289, 4168, 42731, 25040,...       11161   \n",
       "1   172162  [99811, 2851, 5990, 56982, 42832, 5781, 42731,...       43529   \n",
       "2   108316                          [1749, 4019, 2449, 53081]       20688   \n",
       "3   185502                                       [1572, 1962]        9403   \n",
       "4   110176       [53140, 2851, 25000, 2449, 4240, 2720, V113]       17540   \n",
       "\n",
       "                                                TEXT  \\\n",
       "0  Admission Date:  [**2119-3-8**]     Discharge ...   \n",
       "1  Admission Date:  [**2115-9-20**]              ...   \n",
       "2  Admission Date: [**2159-4-9**]        Discharg...   \n",
       "3  Admission Date:  [**2127-9-4**]       Discharg...   \n",
       "4  Admission Date:  [**2157-7-21**]     Discharge...   \n",
       "\n",
       "                                     HOPI  \\\n",
       "0                            M D Dictated   \n",
       "1                  simvastatin coumadin B   \n",
       "2            Right breast cancer PHYSICAL   \n",
       "3         Distal pancreatic mass PHYSICAL   \n",
       "4   Upper gastrointestinal bleed PHYSICAL   \n",
       "\n",
       "                                  SENT_TOKENS  \\\n",
       "0                            [d, dictated, m]   \n",
       "1                  [b, coumadin, simvastatin]   \n",
       "2           [breast, cancer, physical, right]   \n",
       "3        [distal, mass, pancreatic, physical]   \n",
       "4  [bleed, gastrointestinal, physical, upper]   \n",
       "\n",
       "                       SENT_TOKENS_COMBO                   idx_tokens  \\\n",
       "0                           d dictated m          [5954, 6531, 12240]   \n",
       "1                 b coumadin simvastatin          [3034, 5613, 18335]   \n",
       "2           breast cancer physical right   [3764, 4100, 15230, 17458]   \n",
       "3        distal mass pancreatic physical  [6799, 12436, 14640, 15230]   \n",
       "4  bleed gastrointestinal physical upper   [3513, 9025, 15230, 20854]   \n",
       "\n",
       "   trunc_len               trunc_idx  ...  \\\n",
       "0          3        [487, 7132, 525]  ...   \n",
       "1          3       [599, 1684, 5846]  ...   \n",
       "2          4  [3441, 2171, 190, 212]  ...   \n",
       "3          4  [1092, 156, 1911, 190]  ...   \n",
       "4          4   [1007, 828, 190, 792]  ...   \n",
       "\n",
       "                             mean_gru_emb_tokens_y_1  \\\n",
       "0  [0.059478458, -0.045454238, -0.0017273542, 0.0...   \n",
       "1  [0.060690463, -0.04955192, -0.018655159, 0.079...   \n",
       "2  [0.06731365, -0.04114383, -0.018741077, 0.0937...   \n",
       "3  [0.06731365, -0.04114383, -0.018741077, 0.0937...   \n",
       "4  [0.059089392, -0.045568395, -0.0015976013, 0.0...   \n",
       "\n",
       "                                     pad_y_2_emb_300  \\\n",
       "0  [139, 111, 71, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "1  [57, 181, 245, 16, 152, 39, 178, 83, 54, 0, 0,...   \n",
       "2  [151, 164, 181, 33, 60, 259, 232, 16, 34, 0, 0...   \n",
       "3  [151, 164, 181, 70, 186, 16, 201, 0, 0, 0, 0, ...   \n",
       "4  [72, 181, 89, 240, 16, 79, 0, 0, 0, 0, 0, 0, 0...   \n",
       "\n",
       "                              y_2_pre_gru_emb_tokens  \\\n",
       "0  [[-0.06420536, -0.073057145, 0.039527323, -0.0...   \n",
       "1  [[0.05226636, 0.101625346, -0.045206733, -0.03...   \n",
       "2  [[0.07328868, 0.024494926, 0.038389012, 0.1365...   \n",
       "3  [[0.07328868, 0.024494926, 0.038389012, 0.1365...   \n",
       "4  [[-0.07873408, -0.05463389, 0.029613571, 0.052...   \n",
       "\n",
       "                             mean_gru_emb_tokens_y_2  \\\n",
       "0  [0.06914873, -0.039207477, -0.022554472, 0.102...   \n",
       "1  [0.073901184, -0.04959332, -0.007015997, 0.094...   \n",
       "2  [0.055725317, -0.050798643, -0.020149566, 0.10...   \n",
       "3  [0.05280675, -0.03909461, -0.010505624, 0.0953...   \n",
       "4  [0.06548796, -0.040969513, -0.019570673, 0.093...   \n",
       "\n",
       "                                     pad_y_3_emb_300  \\\n",
       "0  [6, 449, 367, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1  [118, 260, 468, 510, 99, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [93, 453, 468, 322, 222, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [93, 453, 468, 480, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "4  [60, 601, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                              y_3_pre_gru_emb_tokens  \\\n",
       "0  [[0.027833814, 0.0047603794, 0.016183794, -0.0...   \n",
       "1  [[-0.067778505, 0.12587744, -0.09941409, 0.086...   \n",
       "2  [[-0.0068983953, 0.13078071, -0.122653775, -0....   \n",
       "3  [[-0.0068983953, 0.13078071, -0.122653775, -0....   \n",
       "4  [[0.012612816, 0.06259193, 0.0023962534, -0.03...   \n",
       "\n",
       "                             mean_gru_emb_tokens_y_3  \\\n",
       "0  [0.07076735, -0.03935192, -0.02299309, 0.09065...   \n",
       "1  [0.0522563, -0.04090999, -0.0073791984, 0.0927...   \n",
       "2  [0.063069336, -0.05080911, -0.016955597, 0.086...   \n",
       "3  [0.06486169, -0.047038384, -0.016133888, 0.087...   \n",
       "4  [0.061512876, -0.041455172, -0.020982469, 0.09...   \n",
       "\n",
       "                                     pad_y_4_emb_300  \\\n",
       "0  [17, 731, 609, 917, 608, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [135, 771, 567, 420, 276, 836, 0, 0, 0, 0, 0, ...   \n",
       "2  [176, 739, 764, 368, 1019, 980, 0, 0, 0, 0, 0,...   \n",
       "3  [176, 739, 764, 932, 764, 785, 0, 0, 0, 0, 0, ...   \n",
       "4  [64, 771, 980, 547, 974, 1014, 569, 0, 0, 0, 0...   \n",
       "\n",
       "                              y_4_pre_gru_emb_tokens  \\\n",
       "0  [[-0.031063288, 0.018695444, 0.012389156, 0.05...   \n",
       "1  [[-0.04979415, 0.030295637, 0.015073402, 0.077...   \n",
       "2  [[-0.037677478, 0.020964323, 0.016462605, -0.0...   \n",
       "3  [[-0.037677478, 0.020964323, 0.016462605, -0.0...   \n",
       "4  [[-0.050165668, 0.005476545, 0.14402065, 0.079...   \n",
       "\n",
       "                             mean_gru_emb_tokens_y_4  \n",
       "0  [0.060728285, -0.037503116, -0.014770587, 0.11...  \n",
       "1  [0.07409605, -0.048002392, -0.026867218, 0.093...  \n",
       "2  [0.06460077, -0.048875496, -0.019074915, 0.098...  \n",
       "3  [0.058684308, -0.050492644, -0.02025734, 0.092...  \n",
       "4  [0.0946536, -0.058965012, -0.023040982, 0.0944...  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRU embed for labels\n",
    "\n",
    "dict_emb = {\n",
    "    \"y_l1_300\": lvl1_embed_300,\n",
    "    \"y_l2_300\": lvl2_embed_300,\n",
    "    \"y_l3_300\": lvl3_embed_300,\n",
    "    \"y_l4_300\": lvl4_embed_300\n",
    "}\n",
    "\n",
    "gru = torch.nn.GRU(input_size=300, hidden_size=100, num_layers=1, dropout=.3, batch_first=False, bidirectional=False)\n",
    "embed_sizes = [300]\n",
    "for level in levels:\n",
    "    for embed_size in embed_sizes:\n",
    "        embeddings = dict_emb.get(f'y_l{level}_{embed_size}')\n",
    "        for b in [train, valid, test]:\n",
    "            t = pad_sequences([torch.tensor(x) for x in b[f'y_l{level}_idx_{embed_size}']], padding='post')\n",
    "            b[f'pad_y_{level}_emb_{embed_size}'] = t.tolist()\n",
    "            b[f'y_{level}_pre_gru_emb_tokens'] = b[f'pad_y_{level}_emb_{embed_size}'].map(lambda t: [np.array(embeddings[w]) for w in t])\n",
    "            b[f'mean_gru_emb_tokens_y_{level}'] = b[f'y_{level}_pre_gru_emb_tokens'].map(lambda t: np.average(gru(torch.tensor(t))[0].detach().numpy(), axis=0)) \n",
    "\n",
    "\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1a127f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for gru models\n",
    "\n",
    "y_l1_dict = dict(zip(np.unique(train['y_l1'].values), np.arange(0, len(np.unique(train['y_l1'].values)), 1)))\n",
    "y_l1_trn = train['y_l1'].map(lambda x: y_l1_dict.get(x))\n",
    "y_l1_val = valid['y_l1'].map(lambda x: y_l1_dict.get(x))\n",
    "y_l1_tst = test['y_l1'].map(lambda x: y_l1_dict.get(x))\n",
    "\n",
    "y_l2_dict = dict(zip(np.unique(train['y_l2'].values), np.arange(0, len(np.unique(train['y_l2'].values)), 1)))\n",
    "y_l2_trn = train['y_l2'].map(lambda x: y_l2_dict.get(x)).fillna('Missing')\n",
    "y_l2_val = valid['y_l2'].map(lambda x: y_l2_dict.get(x)).fillna('Missing')\n",
    "y_l2_tst = test['y_l2'].map(lambda x: y_l2_dict.get(x)).fillna('Missing')\n",
    "\n",
    "# Only include lvl2 labels that are in the training set\n",
    "tst_miss = [y_l2_tst != \"Missing\"]\n",
    "tst_idx_2 = np.where(y_l2_tst != 'Missing')\n",
    "y_l2_tst = np.array(y_l2_tst[y_l2_tst != \"Missing\"]).astype(int)\n",
    "\n",
    "val_miss = [y_l2_val != \"Missing\"]\n",
    "val_idx_2 = np.where(y_l2_val != 'Missing')\n",
    "y_l2_val = np.array(y_l2_val[y_l2_val != \"Missing\"]).astype(int)\n",
    "\n",
    "\n",
    "train_miss = train['y_l3'].isna()\n",
    "train_l3 = train[~train_miss]\n",
    "y_l3_dict = dict(zip(set(train_l3['y_l3'].values), np.arange(0, len(set(train_l3['y_l3'].values)), 1)))\n",
    "\n",
    "y_l3_trn = train_l3['y_l3'].map(lambda x: y_l3_dict.get(x)).fillna('Missing')\n",
    "y_l3_val = valid['y_l3'].map(lambda x: y_l3_dict.get(x)).fillna('Missing')\n",
    "y_l3_tst = test['y_l3'].map(lambda x: y_l3_dict.get(x)).fillna('Missing')\n",
    "\n",
    "# Only include lvl2 labels that are in the training set\n",
    "tst_miss = [y_l3_tst != \"Missing\"]\n",
    "tst_idx_3 = np.where(y_l3_tst != 'Missing')\n",
    "y_l3_tst = np.array(y_l3_tst[y_l3_tst != \"Missing\"]).astype(int)\n",
    "\n",
    "val_miss = [y_l3_val != \"Missing\"]\n",
    "val_idx_3 = np.where(y_l3_val != 'Missing')\n",
    "y_l3_val = np.array(y_l3_val[y_l3_val != \"Missing\"]).astype(int)\n",
    "\n",
    "train_l4 = train[~train['y_l4'].isna()]\n",
    "\n",
    "# Get top X number of classes in the training data\n",
    "train_top = train_l4['y_l4'].value_counts()[0:32]\n",
    "\n",
    "# 16,869 data points after filtering for the top 32 classes \n",
    "train_l4 = train[train['y_l4'].isin(list(train_top.index))]\n",
    "\n",
    "y_l4_dict = dict(zip(set(train_l4['y_l4'].values), np.arange(0, len(set(train_l4['y_l4'].values)), 1)))\n",
    "\n",
    "train_l4 = train_l4[train['y_l4'].isin(list(train_top.index))]\n",
    "\n",
    "train_idx_4 = np.where(~train_l4['y_l4'].isna())\n",
    "train_l4 = train_l4[~train_l4['y_l4'].isna()]\n",
    "\n",
    "# Remove classes that are not in the dictionary\n",
    "y_l4_trn = train_l4['y_l4'].map(lambda x: y_l4_dict.get(x)).fillna('Missing')\n",
    "y_l4_val = valid['y_l4'].map(lambda x: y_l4_dict.get(x)).fillna('Missing')\n",
    "y_l4_tst = test['y_l4'].map(lambda x: y_l4_dict.get(x)).fillna('Missing')\n",
    "\n",
    "# Only include labels that are in the training set\n",
    "tst_miss = [y_l4_tst != \"Missing\"]\n",
    "tst_idx_4 = np.where(y_l4_tst != 'Missing')\n",
    "y_l4_tst = np.array(y_l4_tst[y_l4_tst != \"Missing\"]).astype(int)\n",
    "\n",
    "# Only include labels that are in the training set\n",
    "val_miss = [y_l4_val != \"Missing\"]\n",
    "val_idx_4 = np.where(y_l4_val != 'Missing')\n",
    "y_l4_val = np.array(y_l4_val[y_l4_val != \"Missing\"]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38577f9d",
   "metadata": {},
   "source": [
    "## GRU(X) - Mean(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf12f7e",
   "metadata": {},
   "source": [
    "### Predict level 1 icd codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "80df396f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From c:\\Users\\mab28\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.005\n",
      "\n",
      " Test Precision: 0.415\n",
      "\n",
      " Test Recall: 0.002\n"
     ]
    }
   ],
   "source": [
    "# Mean representation of labels\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 100\n",
    "\n",
    "# number of target labels\n",
    "num_labels = 100 #len(np.unique(y_l1_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 5000\n",
    "\n",
    "level = 1\n",
    "labels_marker = f'mean_y_{level}_emb'\n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "train_labels = preprocessing.normalize(np.array(pd.DataFrame(train[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "test_labels = preprocessing.normalize(np.array(pd.DataFrame(test[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "valid_labels = preprocessing.normalize(np.array(pd.DataFrame(valid[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e522d1",
   "metadata": {},
   "source": [
    "### Precict level 2 icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e59806bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.424\n",
      "\n",
      " Test Precision: 0.362\n",
      "\n",
      " Test Recall: 0.513\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 100\n",
    "\n",
    "# number of target labels\n",
    "num_labels = 100 #len(np.unique(y_l1_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 5000\n",
    "\n",
    "level = 2\n",
    "labels_marker = f'mean_y_{level}_emb'\n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "train_labels = preprocessing.normalize(np.array(pd.DataFrame(train[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "test_labels = preprocessing.normalize(np.array(pd.DataFrame(test[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "valid_labels = preprocessing.normalize(np.array(pd.DataFrame(valid[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368be6c1",
   "metadata": {},
   "source": [
    "### Precict level 3 icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bcc75561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.076\n",
      "\n",
      " Test Precision: 0.050\n",
      "\n",
      " Test Recall: 0.205\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 100\n",
    "\n",
    "# number of target labels\n",
    "num_labels = 100 #len(np.unique(y_l1_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 5000\n",
    "\n",
    "level = 3\n",
    "labels_marker = f'mean_y_{level}_emb'\n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "train_labels = preprocessing.normalize(np.array(pd.DataFrame(train[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "test_labels = preprocessing.normalize(np.array(pd.DataFrame(test[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "valid_labels = preprocessing.normalize(np.array(pd.DataFrame(valid[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60470b85",
   "metadata": {},
   "source": [
    "### Precict terminal icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eb717276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.090\n",
      "\n",
      " Test Precision: 0.056\n",
      "\n",
      " Test Recall: 0.234\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 100\n",
    "\n",
    "# number of target labels\n",
    "num_labels = 100 #len(np.unique(y_l1_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 5000\n",
    "\n",
    "level = 4\n",
    "labels_marker = f'mean_y_{level}_emb'\n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "train_labels = preprocessing.normalize(np.array(pd.DataFrame(train[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "test_labels = preprocessing.normalize(np.array(pd.DataFrame(test[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "valid_labels = preprocessing.normalize(np.array(pd.DataFrame(valid[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4117a",
   "metadata": {},
   "source": [
    "## GRU(X) - atomic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e5df54",
   "metadata": {},
   "source": [
    "### Predict level 1 icd codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6b7ce0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.463\n",
      "\n",
      " Test Precision: 0.483\n",
      "\n",
      " Test Recall: 0.522\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data for GRU models\n",
    "# GRU with softmax activation\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from keras import Sequential\n",
    "\n",
    "# # Pad the word embeddings so that they are all the same length\n",
    "# #t = pad_sequences([torch.tensor(x) for x in df['idx_tokens']], padding='post')\n",
    "# #df['pad_idx'] = t.tolist()\n",
    "# #df['emb_tokens'] = df['pad_idx'].map(lambda t: [np.array(embed.weight.data[w]) for w in t])\n",
    "# #df[\"stack_emb_tokens\"] = df['emb_tokens'].map(lambda t: torch.stack(t, dim = 0))\n",
    "\n",
    "# # Order the documents based on their length and then batch them accordingly\n",
    "# df['token length'] = df['idx_tokens'].map(lambda x: len(x))\n",
    "# sort = train.sort_values(by=['trunc_len'])\n",
    "\n",
    "# # Split into mini-batches\n",
    "# len(sort)/4000\n",
    "\n",
    "# Initialize bi-directional GRU with torch (ref: https://gist.github.com/ceshine/bed2dadca48fe4fe4b4600ccce2fd6e1)\n",
    "\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 100\n",
    "\n",
    "# number of target labels\n",
    "num_labels = len(np.unique(y_l1_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 50000\n",
    "\n",
    "  \n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "train_labels = one_hot(y_l1_trn, num_labels)\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "test_labels = one_hot(y_l1_tst, num_labels)\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "valid_labels = one_hot(y_l1_val, num_labels)\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e5df54",
   "metadata": {},
   "source": [
    "### Predict level 2 icd codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6b7ce0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.278\n",
      "\n",
      " Test Precision: 0.304\n",
      "\n",
      " Test Recall: 0.342\n"
     ]
    }
   ],
   "source": [
    "# Set the seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 100\n",
    "\n",
    "# number of target labels\n",
    "num_labels = len(np.unique(y_l2_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 50000\n",
    "  \n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "train_labels = one_hot(y_l2_trn, num_labels)\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['mean_gru_emb_tokens'][tst_idx_2[0]])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "test_labels = one_hot(y_l2_tst, num_labels)\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['mean_gru_emb_tokens'][val_idx_2[0]])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "valid_labels = one_hot(y_l2_val, num_labels)\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e5df54",
   "metadata": {},
   "source": [
    "### Predict level 3 icd codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6b7ce0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.153\n",
      "\n",
      " Test Precision: 0.167\n",
      "\n",
      " Test Recall: 0.220\n"
     ]
    }
   ],
   "source": [
    "# Set the seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 100\n",
    "\n",
    "# number of target labels\n",
    "num_labels = len(np.unique(y_l3_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 50000\n",
    "\n",
    "train_idx = np.where(train_miss != True)\n",
    "  \n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['mean_gru_emb_tokens'][train_idx[0]])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "train_labels = one_hot(y_l3_trn, num_labels)\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['mean_gru_emb_tokens'][tst_idx_3[0]])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "test_labels = one_hot(y_l3_tst, num_labels)\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['mean_gru_emb_tokens'][val_idx_3[0]])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "valid_labels = one_hot(y_l3_val, num_labels)\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e5df54",
   "metadata": {},
   "source": [
    "### Predict terminal icd codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6b7ce0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.061\n",
      "\n",
      " Test Precision: 0.038\n",
      "\n",
      " Test Recall: 0.167\n"
     ]
    }
   ],
   "source": [
    "# Set the seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 100\n",
    "\n",
    "# number of target labels\n",
    "num_labels = len(np.unique(y_l4_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 50000\n",
    "\n",
    "  \n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['mean_gru_emb_tokens'][train_idx_4[0]])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "train_labels = one_hot(y_l4_trn, num_labels)\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['mean_gru_emb_tokens'][tst_idx_4[0]])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "test_labels = one_hot(y_l4_tst, num_labels)\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['mean_gru_emb_tokens'][val_idx_4[0]])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "valid_labels = one_hot(y_l4_val, num_labels)\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cb0855",
   "metadata": {},
   "source": [
    "## GRU(X) - GRU(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d819ff",
   "metadata": {},
   "source": [
    "## Predict Level 1 Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e5df54",
   "metadata": {},
   "source": [
    "### Predict level 1 icd codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "80df396f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.000\n",
      "\n",
      " Test Precision: 0.000\n",
      "\n",
      " Test Recall: 0.000\n"
     ]
    }
   ],
   "source": [
    "# Mean representation of labels\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 100\n",
    "\n",
    "# number of target labels\n",
    "num_labels = 100\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 5000\n",
    "\n",
    "level = 1\n",
    "labels_marker = f'mean_gru_emb_tokens_y_{level}'\n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "train_labels = preprocessing.normalize(np.array(pd.DataFrame(train[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "test_labels = preprocessing.normalize(np.array(pd.DataFrame(test[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "valid_labels = preprocessing.normalize(np.array(pd.DataFrame(valid[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b15af",
   "metadata": {},
   "source": [
    "## Predict Level 2 Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e59806bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.153\n",
      "\n",
      " Test Precision: 0.130\n",
      "\n",
      " Test Recall: 0.187\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 100\n",
    "\n",
    "# number of target labels\n",
    "num_labels = 100\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 5000\n",
    "\n",
    "level = 2\n",
    "labels_marker = f'mean_gru_emb_tokens_y_{level}'\n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "train_labels = preprocessing.normalize(np.array(pd.DataFrame(train[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "test_labels = preprocessing.normalize(np.array(pd.DataFrame(test[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "valid_labels = preprocessing.normalize(np.array(pd.DataFrame(valid[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96f72b8",
   "metadata": {},
   "source": [
    "## Predict Level 3 Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bcc75561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.000\n",
      "\n",
      " Test Precision: 0.000\n",
      "\n",
      " Test Recall: 0.000\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 100\n",
    "\n",
    "# number of target labels\n",
    "num_labels = 100\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 5000\n",
    "\n",
    "level = 3\n",
    "labels_marker = f'mean_gru_emb_tokens_y_{level}'\n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "train_labels = preprocessing.normalize(np.array(pd.DataFrame(train[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "test_labels = preprocessing.normalize(np.array(pd.DataFrame(test[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "valid_labels = preprocessing.normalize(np.array(pd.DataFrame(valid[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9777c75b",
   "metadata": {},
   "source": [
    "## Predict Level 4 Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eb717276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.000\n",
      "\n",
      " Test Precision: 0.723\n",
      "\n",
      " Test Recall: 0.000\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 100\n",
    "\n",
    "# number of target labels\n",
    "num_labels = 100\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 5000\n",
    "\n",
    "level = 4\n",
    "labels_marker = f'mean_gru_emb_tokens_y_{level}'\n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "train_labels = preprocessing.normalize(np.array(pd.DataFrame(train[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "test_labels = preprocessing.normalize(np.array(pd.DataFrame(test[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "valid_labels = preprocessing.normalize(np.array(pd.DataFrame(valid[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "277e9aaf58bbfde54e1956aac8f2babd4c41f226386ab1e58381dbe6dfe73f30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
