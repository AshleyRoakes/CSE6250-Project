{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a331de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk import word_tokenize\n",
    "from nltk.internals import find_jars_within_path\n",
    "import nltk\n",
    "import sklearn.model_selection\n",
    "from collections import Counter\n",
    "import sklearn.feature_extraction.text as skt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pyhealth\n",
    "from pyhealth.medcode import InnerMap\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import gensim.models.word2vec as w2v\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import tensorflow as TF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c542936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#root = \"/Users/ashleyroakes/Desktop/\"\n",
    "root = \"./data/\"\n",
    "\n",
    "mim_root = root + \"mimic-iii-clinical-database-1.4/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b7b726",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "## Read in Discharge Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9078a11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of discharge summaries:  55177\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22532</td>\n",
       "      <td>167853.0</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2151-7-16**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13702</td>\n",
       "      <td>107527.0</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2118-6-2**]       Discharg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13702</td>\n",
       "      <td>167118.0</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2119-5-4**]              D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13702</td>\n",
       "      <td>196489.0</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2124-7-21**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26880</td>\n",
       "      <td>135453.0</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2162-3-3**]              D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID   HADM_ID           CATEGORY DESCRIPTION  \\\n",
       "0       22532  167853.0  Discharge summary      Report   \n",
       "1       13702  107527.0  Discharge summary      Report   \n",
       "2       13702  167118.0  Discharge summary      Report   \n",
       "3       13702  196489.0  Discharge summary      Report   \n",
       "4       26880  135453.0  Discharge summary      Report   \n",
       "\n",
       "                                                TEXT  \n",
       "0  Admission Date:  [**2151-7-16**]       Dischar...  \n",
       "1  Admission Date:  [**2118-6-2**]       Discharg...  \n",
       "2  Admission Date:  [**2119-5-4**]              D...  \n",
       "3  Admission Date:  [**2124-7-21**]              ...  \n",
       "4  Admission Date:  [**2162-3-3**]              D...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes = mim_root + \"NOTEEVENTS.csv.gz\"\n",
    "\n",
    "notes_df = pd.read_csv(notes, compression='gzip', error_bad_lines=False, \n",
    "                       usecols = ['SUBJECT_ID', 'HADM_ID', 'CATEGORY', 'DESCRIPTION','TEXT'])\\\n",
    "                      .query(\"CATEGORY == 'Discharge summary'\")\\\n",
    "                      .query(\"DESCRIPTION == 'Report'\")\n",
    "\n",
    "# Should be 55,177 records\n",
    "print(\"Number of discharge summaries: \", + len(notes_df))\n",
    "\n",
    "notes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757857a9",
   "metadata": {},
   "source": [
    "## Read in Patient Diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e16993f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>[25013, 3371, 5849, 5780, V5867, 25063, 5363, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>[53100, 2851, 07054, 5715, 45621, 53789, 4019,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100006</td>\n",
       "      <td>[49320, 51881, 486, 20300, 2761, 7850, 3090, V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100007</td>\n",
       "      <td>[56081, 5570, 9973, 486, 4019]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100009</td>\n",
       "      <td>[41401, 99604, 4142, 25000, 27800, V8535, 4148...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                          ICD9_CODE\n",
       "0   100001  [25013, 3371, 5849, 5780, V5867, 25063, 5363, ...\n",
       "1   100003  [53100, 2851, 07054, 5715, 45621, 53789, 4019,...\n",
       "2   100006  [49320, 51881, 486, 20300, 2761, 7850, 3090, V...\n",
       "3   100007                     [56081, 5570, 9973, 486, 4019]\n",
       "4   100009  [41401, 99604, 4142, 25000, 27800, V8535, 4148..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag = mim_root + \"DIAGNOSES_ICD.csv.gz\"\n",
    "\n",
    "diag_df = pd.read_csv(diag, compression='gzip', error_bad_lines=False)\\\n",
    "                    .dropna()\\\n",
    "                    .groupby('HADM_ID')['ICD9_CODE']\\\n",
    "                    .unique()\\\n",
    "                    .reset_index()\n",
    "\n",
    "diag_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5befa018",
   "metadata": {},
   "source": [
    "## Read in ICD9 Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e7e3472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>SHORT_TITLE</th>\n",
       "      <th>LONG_TITLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174</td>\n",
       "      <td>01166</td>\n",
       "      <td>TB pneumonia-oth test</td>\n",
       "      <td>Tuberculous pneumonia [any form], tubercle bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175</td>\n",
       "      <td>01170</td>\n",
       "      <td>TB pneumothorax-unspec</td>\n",
       "      <td>Tuberculous pneumothorax, unspecified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176</td>\n",
       "      <td>01171</td>\n",
       "      <td>TB pneumothorax-no exam</td>\n",
       "      <td>Tuberculous pneumothorax, bacteriological or h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>177</td>\n",
       "      <td>01172</td>\n",
       "      <td>TB pneumothorx-exam unkn</td>\n",
       "      <td>Tuberculous pneumothorax, bacteriological or h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>178</td>\n",
       "      <td>01173</td>\n",
       "      <td>TB pneumothorax-micro dx</td>\n",
       "      <td>Tuberculous pneumothorax, tubercle bacilli fou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROW_ID ICD9_CODE               SHORT_TITLE  \\\n",
       "0     174     01166     TB pneumonia-oth test   \n",
       "1     175     01170    TB pneumothorax-unspec   \n",
       "2     176     01171   TB pneumothorax-no exam   \n",
       "3     177     01172  TB pneumothorx-exam unkn   \n",
       "4     178     01173  TB pneumothorax-micro dx   \n",
       "\n",
       "                                          LONG_TITLE  \n",
       "0  Tuberculous pneumonia [any form], tubercle bac...  \n",
       "1              Tuberculous pneumothorax, unspecified  \n",
       "2  Tuberculous pneumothorax, bacteriological or h...  \n",
       "3  Tuberculous pneumothorax, bacteriological or h...  \n",
       "4  Tuberculous pneumothorax, tubercle bacilli fou...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icd = mim_root + \"D_ICD_DIAGNOSES.csv.gz\"\n",
    "icd_df = pd.read_csv(icd, compression='gzip', error_bad_lines=False)\n",
    "\n",
    "\n",
    "icd_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d7502",
   "metadata": {},
   "source": [
    "## Merge datasets by HADM_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb8a4e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55172"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(diag_df, notes_df, on='HADM_ID', how='inner')\n",
    "\n",
    "# Should be 55177-5 = 55172\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b193cbba",
   "metadata": {},
   "source": [
    "## Substitute special sequences & Filter HoPI sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cb0055",
   "metadata": {},
   "source": [
    "### Identify HOPI sections & Substitute Special Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f357cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_notes(st):\n",
    "    s  = \"History of Present Illness\"\n",
    "    s1 = \"HISTORY OF PRESENT ILLNESS:|HISTORY OF THE PRESENT ILLNESS:| \\\n",
    "          HISTORY OF PRESENT ILLNESS/CHIEF COMPLAINT:|HISTORY OF THE PRESENT ILLNESS/HOSPITAL COURSE:| \\\n",
    "          HISTORY OF THE PRESENT ILLNESS/HOSPITAL COURSE:|HISTORY OF PRESENT ILLNESS/HOSPITAL COURSE PRIOR TO TRANSFER:\"#|\\nHISTORY:\"\n",
    "\n",
    "    s2 = \"present illness:|Present Illness:|PRESENT ILLNESS:\"#|Chief Complaint:\"\n",
    "    \n",
    "    match  = re.search(s, st)\n",
    "    match1 = re.search(s1, st)\n",
    "    match2 = re.search(s2, st)\n",
    "    \n",
    "    if (match is not None) or (match1 is not None) or (match2 is not None):\n",
    "        if match is not None:\n",
    "            st = st.split(s, 1)[1]\n",
    "            e = \"\\n\\n\\n\"\n",
    "            n = st.split(e, 1)[0]\n",
    "            \n",
    "        elif match1 is not None: \n",
    "            st = st.split(match1[0], 1)[1]\n",
    "            #e = re.search(r\"\\n[\\s\\w]+:\", st)[0]\n",
    "            e = re.search(r\"\\b([A-Z]\\w*)+:\", st)[0]\n",
    "            n = st.split(e, 1)[0]\n",
    "            \n",
    "        elif match2 is not None: \n",
    "            st = st.split(match2[0], 1)[1]\n",
    "            #e = re.search(r\"\\n[\\s\\w]+:\", st)[0]\n",
    "            e = re.search(r\"\\b([A-Z]\\w*)+:\", st)[0]\n",
    "            n = st.split(e, 1)[0]\n",
    "        \n",
    "        # Replace special strings ([** **]) with \"\"\n",
    "\n",
    "        rep = re.findall(r\"\\[\\*\\*([a-zA-Z0-9\\s\\-\\(\\)]+)\\*\\*]\", n)\n",
    "\n",
    "        for i in range(len(rep)):\n",
    "            n = n.replace(rep[i], \"\")\n",
    "        \n",
    "        n = n.rsplit(' ', 1)[0]\n",
    "\n",
    "    else: \n",
    "        n = ''\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad9596a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addtitional Preprocessing\n",
    "#sort = df.sort_values(by=['trunc_len'])\n",
    "\n",
    "# Check which sequences are length of 1-5\n",
    "#short = sort[sort['trunc_len'] <= 10].reset_index(drop = True)\n",
    "\n",
    "# Remove these tokens: Past, Physical, Hospital, Dictated\n",
    "\n",
    "# Check which HOPI sections have this string: EAST HOSPITAL MEDICINE ATTENDING ADMISSION NOTE and remove them\n",
    "#df[df['HOPI'] == ' EAST HOSPITAL MEDICINE ATTENDING ADMISSION NOTE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "991b1670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs missing HOPI sections: 3689\n"
     ]
    }
   ],
   "source": [
    "df['HOPI'] = df[\"TEXT\"].map(lambda t: process_notes(t))\n",
    "\n",
    "df[\"HOPI\"] = df[\"HOPI\"].apply(lambda x: re.sub('\\[\\*\\*[^\\]]*\\*\\*\\]', '', x))\n",
    "df[\"HOPI\"] = df[\"HOPI\"].apply(lambda x: re.sub('<[^>]*>', '', x))\n",
    "df[\"HOPI\"] = df[\"HOPI\"].apply(lambda x: re.sub('[\\W]+', ' ', x))\n",
    "\n",
    "# Detect history of present illness in text (n = 2641 records without HoPI data)\n",
    "missing = len(df[df['HOPI'] == \"\"])\n",
    "print(\"Number of docs missing HOPI sections: \" + str(missing))\n",
    "\n",
    "df = df[df[\"HOPI\"] != \"\"].reset_index(drop = True)\n",
    "df['SENT_TOKENS'] = df[\"HOPI\"].map(lambda t: [p.lower() for p in nltk.RegexpTokenizer(r'\\w+').tokenize(t) if not p.isnumeric()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a132416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>HOPI</th>\n",
       "      <th>SENT_TOKENS</th>\n",
       "      <th>SENT_TOKENS_COMBO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>[25013, 3371, 5849, 5780, V5867, 25063, 5363, ...</td>\n",
       "      <td>58526</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2117-9-11**]              ...</td>\n",
       "      <td>35F w poorly controlled Type 1 diabetes melli...</td>\n",
       "      <td>[325mg, 35f, 3l, 3rd, 4mg, 5d, a, ag, also, am...</td>\n",
       "      <td>325mg 35f 3l 3rd 4mg 5d a ag also am an and an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>[53100, 2851, 07054, 5715, 45621, 53789, 4019,...</td>\n",
       "      <td>54610</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2150-4-17**]              ...</td>\n",
       "      <td>Mr is a 59M w HepC cirrhosis c b grade I II e...</td>\n",
       "      <td>[40mg, 4l, 59m, a, abdominal, about, abstain, ...</td>\n",
       "      <td>40mg 4l 59m a abdominal about abstain admissio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100006</td>\n",
       "      <td>[49320, 51881, 486, 20300, 2761, 7850, 3090, V...</td>\n",
       "      <td>9895</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2108-4-6**]       Discharg...</td>\n",
       "      <td>This is a 48 year old African American female...</td>\n",
       "      <td>[a, abdominal, abg, admitted, african, ago, al...</td>\n",
       "      <td>a abdominal abg admitted african ago albuterol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100007</td>\n",
       "      <td>[56081, 5570, 9973, 486, 4019]</td>\n",
       "      <td>23018</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2145-3-31**]              ...</td>\n",
       "      <td>Ms is a 73 year old female with a history of ...</td>\n",
       "      <td>[a, abdominal, and, appendectomy, back, began,...</td>\n",
       "      <td>a abdominal and appendectomy back began bowel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100009</td>\n",
       "      <td>[41401, 99604, 4142, 25000, 27800, V8535, 4148...</td>\n",
       "      <td>533</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2162-5-16**]              ...</td>\n",
       "      <td>60yo man with known coronary disease AMI in a...</td>\n",
       "      <td>[36mmhg, 60yo, a, admitted, ak, ami, and, angi...</td>\n",
       "      <td>36mmhg 60yo a admitted ak ami and angina anter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                          ICD9_CODE  SUBJECT_ID  \\\n",
       "0   100001  [25013, 3371, 5849, 5780, V5867, 25063, 5363, ...       58526   \n",
       "1   100003  [53100, 2851, 07054, 5715, 45621, 53789, 4019,...       54610   \n",
       "2   100006  [49320, 51881, 486, 20300, 2761, 7850, 3090, V...        9895   \n",
       "3   100007                     [56081, 5570, 9973, 486, 4019]       23018   \n",
       "4   100009  [41401, 99604, 4142, 25000, 27800, V8535, 4148...         533   \n",
       "\n",
       "            CATEGORY DESCRIPTION  \\\n",
       "0  Discharge summary      Report   \n",
       "1  Discharge summary      Report   \n",
       "2  Discharge summary      Report   \n",
       "3  Discharge summary      Report   \n",
       "4  Discharge summary      Report   \n",
       "\n",
       "                                                TEXT  \\\n",
       "0  Admission Date:  [**2117-9-11**]              ...   \n",
       "1  Admission Date:  [**2150-4-17**]              ...   \n",
       "2  Admission Date:  [**2108-4-6**]       Discharg...   \n",
       "3  Admission Date:  [**2145-3-31**]              ...   \n",
       "4  Admission Date:  [**2162-5-16**]              ...   \n",
       "\n",
       "                                                HOPI  \\\n",
       "0   35F w poorly controlled Type 1 diabetes melli...   \n",
       "1   Mr is a 59M w HepC cirrhosis c b grade I II e...   \n",
       "2   This is a 48 year old African American female...   \n",
       "3   Ms is a 73 year old female with a history of ...   \n",
       "4   60yo man with known coronary disease AMI in a...   \n",
       "\n",
       "                                         SENT_TOKENS  \\\n",
       "0  [325mg, 35f, 3l, 3rd, 4mg, 5d, a, ag, also, am...   \n",
       "1  [40mg, 4l, 59m, a, abdominal, about, abstain, ...   \n",
       "2  [a, abdominal, abg, admitted, african, ago, al...   \n",
       "3  [a, abdominal, and, appendectomy, back, began,...   \n",
       "4  [36mmhg, 60yo, a, admitted, ak, ami, and, angi...   \n",
       "\n",
       "                                   SENT_TOKENS_COMBO  \n",
       "0  325mg 35f 3l 3rd 4mg 5d a ag also am an and an...  \n",
       "1  40mg 4l 59m a abdominal about abstain admissio...  \n",
       "2  a abdominal abg admitted african ago albuterol...  \n",
       "3  a abdominal and appendectomy back began bowel ...  \n",
       "4  36mmhg 60yo a admitted ak ami and angina anter...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truncate to 500 tokens per HOPI\n",
    "df[\"SENT_TOKENS\"] = df[\"SENT_TOKENS\"].map(lambda c: np.unique(c)[0:500])\n",
    "\n",
    "df['SENT_TOKENS_COMBO'] = df[\"SENT_TOKENS\"].map(lambda t: \" \".join(t))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "194ab1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=200, alpha=0.025>', 'datetime': '2022-12-03T09:09:25.917535', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 1547918 words, keeping 33008 word types\n",
      "PROGRESS: at sentence #20000, processed 3096903 words, keeping 45077 word types\n",
      "PROGRESS: at sentence #30000, processed 4643864 words, keeping 54264 word types\n",
      "PROGRESS: at sentence #40000, processed 6211630 words, keeping 62260 word types\n",
      "PROGRESS: at sentence #50000, processed 7754844 words, keeping 69400 word types\n",
      "collected 70408 word types from a corpus of 7986792 raw words and 51483 sentences\n",
      "Creating a fresh vocabulary\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 21877 unique words (31.07% of original 70408, drops 48531)', 'datetime': '2022-12-03T09:09:27.394534', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 7914681 word corpus (99.10% of original 7986792, drops 72111)', 'datetime': '2022-12-03T09:09:27.395036', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "deleting the raw counts dictionary of 70408 items\n",
      "sample=0.001 downsamples 37 most-common words\n",
      "Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 7602769.587054234 word corpus (96.1%% of prior 7914681)', 'datetime': '2022-12-03T09:09:27.520534', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "estimated required memory for 21877 words and 200 dimensions: 45941700 bytes\n",
      "resetting layer weights\n",
      "Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-12-03T09:09:27.738534', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'training model with 4 workers on 21877 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-12-03T09:09:27.740035', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "EPOCH 0 - PROGRESS: at 16.28% examples, 1232013 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 0 - PROGRESS: at 33.71% examples, 1277990 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 0 - PROGRESS: at 50.73% examples, 1282503 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 0 - PROGRESS: at 66.70% examples, 1264063 words/s, in_qsize 8, out_qsize 0\n",
      "EPOCH 0 - PROGRESS: at 84.63% examples, 1281821 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 0: training on 7986792 raw words (7602707 effective words) took 5.9s, 1280899 effective words/s\n",
      "EPOCH 1 - PROGRESS: at 16.03% examples, 1212593 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 33.06% examples, 1251427 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 49.84% examples, 1260781 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 67.20% examples, 1272449 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 84.50% examples, 1280681 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 1: training on 7986792 raw words (7602459 effective words) took 5.9s, 1288338 effective words/s\n",
      "EPOCH 2 - PROGRESS: at 17.25% examples, 1300457 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 33.58% examples, 1264228 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 48.86% examples, 1230490 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 65.34% examples, 1234493 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 82.65% examples, 1247555 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 100.00% examples, 1259263 words/s, in_qsize 0, out_qsize 1\n",
      "EPOCH 2: training on 7986792 raw words (7601787 effective words) took 6.0s, 1259017 effective words/s\n",
      "EPOCH 3 - PROGRESS: at 16.64% examples, 1257337 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 33.99% examples, 1281195 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 50.76% examples, 1280858 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 67.95% examples, 1282330 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 83.54% examples, 1262249 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 3: training on 7986792 raw words (7603673 effective words) took 6.0s, 1265694 effective words/s\n",
      "EPOCH 4 - PROGRESS: at 17.02% examples, 1285961 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 4 - PROGRESS: at 34.38% examples, 1295191 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 4 - PROGRESS: at 53.17% examples, 1332271 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 4 - PROGRESS: at 72.27% examples, 1362141 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 4 - PROGRESS: at 91.56% examples, 1382055 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 4: training on 7986792 raw words (7601815 effective words) took 5.5s, 1384097 effective words/s\n",
      "Word2Vec lifecycle event {'msg': 'training on 39933960 raw words (38012441 effective words) took 29.4s, 1293065 effective words/s', 'datetime': '2022-12-03T09:09:57.137540', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "Word2Vec lifecycle event {'fname_or_handle': './model/model.w2v', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-12-03T09:09:57.138540', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'}\n",
      "not storing attribute cum_table\n",
      "saved ./model/model.w2v\n"
     ]
    }
   ],
   "source": [
    "sentences = df['SENT_TOKENS_COMBO'].map(lambda t: t.split()).values\n",
    "\n",
    "model = w2v.Word2Vec(vector_size=200, min_count=5, workers=4, epochs=5)\n",
    "\n",
    "model.build_vocab(sentences)\n",
    "\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "model.save('./model/model.w2v')\n",
    "\n",
    "wv = model.wv\n",
    "\n",
    "vocab = model.wv.key_to_index  \n",
    "\n",
    "ind2w = {i+1:w for i,w in enumerate(sorted(vocab))}\n",
    "w2ind = {w:i for i,w in ind2w.items()}\n",
    "\n",
    "PAD_CHAR = \"**PAD**\"\n",
    "\n",
    "def build_matrix(ind2w, wv):\n",
    "    W = np.zeros((len(ind2w)+1, len(wv.get_vector(wv.index_to_key[0])) ))\n",
    "    words = [PAD_CHAR]\n",
    "    W[0][:] = np.zeros(len(wv.get_vector(wv.index_to_key[0])))\n",
    "    for idx, word in ind2w.items():\n",
    "        if idx >= W.shape[0]:\n",
    "            break    \n",
    "        W[idx][:] = wv.get_vector(word)\n",
    "        words.append(word)\n",
    "    return W, words\n",
    "\n",
    "W, words = build_matrix(ind2w, wv)\n",
    "\n",
    "def save_embeddings(W, words, outfile):\n",
    "    with open(outfile, 'w') as o:\n",
    "        #pad token already included\n",
    "        for i in range(len(words)):\n",
    "            line = [words[i]]\n",
    "            line.extend([str(d) for d in W[i]])\n",
    "            o.write(\" \".join(line) + \"\\n\")\n",
    "\n",
    "outfile = './model/model.embed'\n",
    "save_embeddings(W, words, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4c34972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embed_file):\n",
    "    #also normalizes the embeddings\n",
    "    W = []\n",
    "    with open(embed_file) as ef:\n",
    "        for line in ef:\n",
    "            line = line.rstrip().split()\n",
    "            vec = np.array(line[1:]).astype(float)\n",
    "            vec = vec / float(np.linalg.norm(vec) + 1e-6)\n",
    "            W.append(vec)\n",
    "        vec = np.random.randn(len(W[-1]))\n",
    "        vec = vec / float(np.linalg.norm(vec) + 1e-6)\n",
    "        W.append(vec)\n",
    "    W = np.array(W)\n",
    "    return W\n",
    "\n",
    "loaded_embed = torch.Tensor(load_embeddings(outfile))\n",
    "\n",
    "embed = nn.Embedding(loaded_embed.size()[0], loaded_embed.size()[1], padding_idx=0)\n",
    "embed.weight.data = loaded_embed.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c110107a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>HOPI</th>\n",
       "      <th>SENT_TOKENS</th>\n",
       "      <th>SENT_TOKENS_COMBO</th>\n",
       "      <th>idx_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>[25013, 3371, 5849, 5780, V5867, 25063, 5363, ...</td>\n",
       "      <td>58526</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2117-9-11**]              ...</td>\n",
       "      <td>35F w poorly controlled Type 1 diabetes melli...</td>\n",
       "      <td>[325mg, 35f, 3l, 3rd, 4mg, 5d, a, ag, also, am...</td>\n",
       "      <td>325mg 35f 3l 3rd 4mg 5d a ag also am an and an...</td>\n",
       "      <td>[615, 640, 698, 715, 860, 995, 1424, 1881, 205...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>[53100, 2851, 07054, 5715, 45621, 53789, 4019,...</td>\n",
       "      <td>54610</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2150-4-17**]              ...</td>\n",
       "      <td>Mr is a 59M w HepC cirrhosis c b grade I II e...</td>\n",
       "      <td>[40mg, 4l, 59m, a, abdominal, about, abstain, ...</td>\n",
       "      <td>40mg 4l 59m a abdominal about abstain admissio...</td>\n",
       "      <td>[752, 853, 984, 1424, 1456, 1497, 1516, 1785, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100006</td>\n",
       "      <td>[49320, 51881, 486, 20300, 2761, 7850, 3090, V...</td>\n",
       "      <td>9895</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2108-4-6**]       Discharg...</td>\n",
       "      <td>This is a 48 year old African American female...</td>\n",
       "      <td>[a, abdominal, abg, admitted, african, ago, al...</td>\n",
       "      <td>a abdominal abg admitted african ago albuterol...</td>\n",
       "      <td>[1424, 1456, 1468, 1793, 1871, 1909, 1973, 205...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100007</td>\n",
       "      <td>[56081, 5570, 9973, 486, 4019]</td>\n",
       "      <td>23018</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2145-3-31**]              ...</td>\n",
       "      <td>Ms is a 73 year old female with a history of ...</td>\n",
       "      <td>[a, abdominal, and, appendectomy, back, began,...</td>\n",
       "      <td>a abdominal and appendectomy back began bowel ...</td>\n",
       "      <td>[1424, 1456, 2204, 2485, 3053, 3248, 3649, 459...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100009</td>\n",
       "      <td>[41401, 99604, 4142, 25000, 27800, V8535, 4148...</td>\n",
       "      <td>533</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2162-5-16**]              ...</td>\n",
       "      <td>60yo man with known coronary disease AMI in a...</td>\n",
       "      <td>[36mmhg, 60yo, a, admitted, ak, ami, and, angi...</td>\n",
       "      <td>36mmhg 60yo a admitted ak ami and angina anter...</td>\n",
       "      <td>[656, 1072, 1424, 1793, 1956, 2108, 2204, 2225...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                          ICD9_CODE  SUBJECT_ID  \\\n",
       "0   100001  [25013, 3371, 5849, 5780, V5867, 25063, 5363, ...       58526   \n",
       "1   100003  [53100, 2851, 07054, 5715, 45621, 53789, 4019,...       54610   \n",
       "2   100006  [49320, 51881, 486, 20300, 2761, 7850, 3090, V...        9895   \n",
       "3   100007                     [56081, 5570, 9973, 486, 4019]       23018   \n",
       "4   100009  [41401, 99604, 4142, 25000, 27800, V8535, 4148...         533   \n",
       "\n",
       "            CATEGORY DESCRIPTION  \\\n",
       "0  Discharge summary      Report   \n",
       "1  Discharge summary      Report   \n",
       "2  Discharge summary      Report   \n",
       "3  Discharge summary      Report   \n",
       "4  Discharge summary      Report   \n",
       "\n",
       "                                                TEXT  \\\n",
       "0  Admission Date:  [**2117-9-11**]              ...   \n",
       "1  Admission Date:  [**2150-4-17**]              ...   \n",
       "2  Admission Date:  [**2108-4-6**]       Discharg...   \n",
       "3  Admission Date:  [**2145-3-31**]              ...   \n",
       "4  Admission Date:  [**2162-5-16**]              ...   \n",
       "\n",
       "                                                HOPI  \\\n",
       "0   35F w poorly controlled Type 1 diabetes melli...   \n",
       "1   Mr is a 59M w HepC cirrhosis c b grade I II e...   \n",
       "2   This is a 48 year old African American female...   \n",
       "3   Ms is a 73 year old female with a history of ...   \n",
       "4   60yo man with known coronary disease AMI in a...   \n",
       "\n",
       "                                         SENT_TOKENS  \\\n",
       "0  [325mg, 35f, 3l, 3rd, 4mg, 5d, a, ag, also, am...   \n",
       "1  [40mg, 4l, 59m, a, abdominal, about, abstain, ...   \n",
       "2  [a, abdominal, abg, admitted, african, ago, al...   \n",
       "3  [a, abdominal, and, appendectomy, back, began,...   \n",
       "4  [36mmhg, 60yo, a, admitted, ak, ami, and, angi...   \n",
       "\n",
       "                                   SENT_TOKENS_COMBO  \\\n",
       "0  325mg 35f 3l 3rd 4mg 5d a ag also am an and an...   \n",
       "1  40mg 4l 59m a abdominal about abstain admissio...   \n",
       "2  a abdominal abg admitted african ago albuterol...   \n",
       "3  a abdominal and appendectomy back began bowel ...   \n",
       "4  36mmhg 60yo a admitted ak ami and angina anter...   \n",
       "\n",
       "                                          idx_tokens  \n",
       "0  [615, 640, 698, 715, 860, 995, 1424, 1881, 205...  \n",
       "1  [752, 853, 984, 1424, 1456, 1497, 1516, 1785, ...  \n",
       "2  [1424, 1456, 1468, 1793, 1871, 1909, 1973, 205...  \n",
       "3  [1424, 1456, 2204, 2485, 3053, 3248, 3649, 459...  \n",
       "4  [656, 1072, 1424, 1793, 1956, 2108, 2204, 2225...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding indexes\n",
    "df[\"idx_tokens\"] = df['SENT_TOKENS'].map(lambda t: [int(w2ind[w]) if w in w2ind else len(w2ind)+1 for w in t])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b841b468",
   "metadata": {},
   "source": [
    "## Plot a histogram of the Number of tokens in each HoPI document, after data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b72313d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"trunc_len\"] = df['SENT_TOKENS'].map(lambda c: len(c))\n",
    "\n",
    "plt.hist(df[\"trunc_len\"], np.arange(0, 501, 10))\n",
    "plt.xlabel(\"Number of Tokens\")\n",
    "plt.ylabel(\"Number of Documents\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f07c2f",
   "metadata": {},
   "source": [
    "## Split data in train, valid, and test sets\n",
    "### training (38,588 records, 69.9%), validation (5536 records, 10.0%) and testing (11,048 records, 20.0%) folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9cc17eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training: 36038, length of test: 10348, & length of valid: 5097\n"
     ]
    }
   ],
   "source": [
    "# Random split\n",
    "train, tst = sklearn.model_selection.train_test_split(df, test_size= 0.3, random_state=42)\n",
    "\n",
    "test, valid = sklearn.model_selection.train_test_split(tst, test_size= 0.33, random_state=42)\n",
    "\n",
    "train = train.reset_index(drop = True).drop([\"CATEGORY\", \"DESCRIPTION\"], axis = 1)\n",
    "test = test.reset_index(drop = True).drop([\"CATEGORY\", \"DESCRIPTION\"], axis = 1)\n",
    "valid = valid.reset_index(drop = True).drop([\"CATEGORY\", \"DESCRIPTION\"], axis = 1)\n",
    "\n",
    "print(f\"Length of training: {len(train)}, length of test: {len(test)}, & length of valid: {len(valid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6f8d07",
   "metadata": {},
   "source": [
    "## Count number of tokens in the training dataset (n = ~92,468 tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "790c81fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19017\n"
     ]
    }
   ],
   "source": [
    "# Count occurence of tokens that are in the training dataset\n",
    "n = len(np.unique(np.concatenate(train['SENT_TOKENS'].values)))\n",
    "occ = Counter(np.concatenate(train['SENT_TOKENS'].values))\n",
    "            \n",
    "# Tokens that occur >= 5 times are in the study vocabulary (should be ~19,503)\n",
    "vocab = [k for k,v in occ.items() if v >= 5]\n",
    "print(len(vocab))\n",
    "\n",
    "# Assign a unique integer ID for each token in the study vocabulary \n",
    "vocab_lookup = dict(zip(vocab, np.arange(0, len(vocab), 1)))\n",
    "\n",
    "# Convert each HoPI document to a 1D array of integers using this index\n",
    "train[\"trunc_idx\"] =  train['SENT_TOKENS'].map(lambda x: [vocab_lookup.get(i) for i in x if i in vocab_lookup.keys()])\n",
    "valid[\"trunc_idx\"] =  valid['SENT_TOKENS'].map(lambda x: [vocab_lookup.get(i) for i in x if i in vocab_lookup.keys()])\n",
    "test[\"trunc_idx\"] =  test['SENT_TOKENS'].map(lambda x: [vocab_lookup.get(i) for i in x if i in vocab_lookup.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b180a12e",
   "metadata": {},
   "source": [
    "# Document representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04e11ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent clinical notes documents as TF-IDF weights\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def tfidf(df):\n",
    "    pipe = Pipeline([('count', CountVectorizer(vocabulary = vocab, lowercase = False)),('tfid', TfidfTransformer(use_idf=True))]).fit(df[\"HOPI\"].values)\n",
    "    pipe['count'].transform(df[\"HOPI\"].values).toarray()\n",
    "    pipe['tfid'].idf_\n",
    "\n",
    "    return pipe.transform(df[\"HOPI\"].values)\n",
    "\n",
    "# Create sparse matrix of size number of docs x words in vocab\n",
    "tfidf_train = tfidf(train)\n",
    "tfidf_valid = tfidf(valid)\n",
    "tfidf_test = tfidf(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e46938b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document embedding representation\n",
    "train['emb'] = train['idx_tokens'].map(lambda t: [np.array(embed.weight.data[w]) for w in t])\n",
    "valid['emb'] = valid['idx_tokens'].map(lambda t: [np.array(embed.weight.data[w]) for w in t])\n",
    "test['emb'] = test['idx_tokens'].map(lambda t: [np.array(embed.weight.data[w]) for w in t])\n",
    "\n",
    "# Document mean embedding representation\n",
    "train['x_mean_emb'] = train['idx_tokens'].map(lambda t: np.average([np.array(embed.weight.data[w]) for w in t], axis = 0))\n",
    "valid['x_mean_emb'] = valid['idx_tokens'].map(lambda t: np.average([np.array(embed.weight.data[w]) for w in t], axis = 0))\n",
    "test['x_mean_emb'] = test['idx_tokens'].map(lambda t: np.average([np.array(embed.weight.data[w]) for w in t], axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb2746c",
   "metadata": {},
   "source": [
    "# Label representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c0afc9",
   "metadata": {},
   "source": [
    "### Identify the hierarchical labels for each icd9 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7305717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out the main ICD9 code (they are ordered for importance)\n",
    "train[\"ICD_Main\"] = train['ICD9_CODE'].map(lambda x: x[0])\n",
    "valid[\"ICD_Main\"] = valid['ICD9_CODE'].map(lambda x: x[0])\n",
    "test[\"ICD_Main\"] = test['ICD9_CODE'].map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9714dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pyhealth library to find code hierarchy for the train, test, and valid datasets\n",
    "icd9cm = InnerMap.load(\"ICD9CM\")\n",
    "\n",
    "def define_levels(x):\n",
    "    lvls = []\n",
    "    for l in x: \n",
    "        lvls.append(icd9cm.lookup(l))\n",
    "    return lvls\n",
    "\n",
    "def drop_top(X):\n",
    "    if '001-999.99' in X:\n",
    "        X.remove('001-999.99')\n",
    "    return X\n",
    "\n",
    "def get_codes(df):\n",
    "    #df['icd_desc'] = df['ICD_Main'].map(lambda x: icd9cm.lookup(x))\n",
    "    df['levels'] = df['ICD_Main'].map(lambda x:icd9cm.get_ancestors(x)[::-1])\n",
    "    df['levels'].map(lambda x: drop_top(x))\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        df['levels'][i].append(df['ICD_Main'][i])\n",
    "\n",
    "    df['levels_desc'] = df['levels'].map(lambda x: define_levels(x))\n",
    "\n",
    "get_codes(train)\n",
    "get_codes(valid)\n",
    "get_codes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65bc18b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the code descriptions to the dfs\n",
    "train = pd.concat([train, train['levels_desc'].apply(pd.Series)], axis = 1).drop(\"levels_desc\", axis = 1)\\\n",
    "    .rename(columns={0: \"y_l1\", 1: \"y_l2\", 2: \"y_l3\", 3: \"y_l4\", 4:\"y_l5\", 5:\"y_l6\"})\n",
    "valid = pd.concat([valid, valid['levels_desc'].apply(pd.Series)], axis = 1).drop(\"levels_desc\", axis = 1)\\\n",
    "    .rename(columns={0: \"y_l1\", 1: \"y_l2\", 2: \"y_l3\", 3: \"y_l4\", 4:\"y_l5\", 5:\"y_l6\"})\n",
    "test = pd.concat([test, test['levels_desc'].apply(pd.Series)], axis = 1).drop(\"levels_desc\", axis = 1)\\\n",
    "    .rename(columns={0: \"y_l1\", 1: \"y_l2\", 2: \"y_l3\", 3: \"y_l4\" , 4:\"y_l5\", 5:\"y_l6\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24c6ec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['y_l5', 'y_l6']\n",
    "train.drop(cols, inplace = True, axis = 1)\n",
    "valid.drop(cols, inplace = True, axis = 1)\n",
    "test.drop(cols, inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3e04392a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2022-12-03T14:17:56.911514', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 47483 words, keeping 59 word types\n",
      "PROGRESS: at sentence #20000, processed 97985 words, keeping 59 word types\n",
      "PROGRESS: at sentence #30000, processed 145475 words, keeping 59 word types\n",
      "collected 59 word types from a corpus of 162932 raw words and 33778 sentences\n",
      "Creating a fresh vocabulary\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 59 unique words (100.00% of original 59, drops 0)', 'datetime': '2022-12-03T14:17:56.958513', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 162932 word corpus (100.00% of original 162932, drops 0)', 'datetime': '2022-12-03T14:17:56.959512', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "deleting the raw counts dictionary of 59 items\n",
      "sample=0.001 downsamples 39 most-common words\n",
      "Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 34713.048943067755 word corpus (21.3%% of prior 162932)', 'datetime': '2022-12-03T14:17:56.964015', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "estimated required memory for 59 words and 100 dimensions: 76700 bytes\n",
      "resetting layer weights\n",
      "Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-12-03T14:17:56.974514', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'training model with 4 workers on 59 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-12-03T14:17:56.975515', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "EPOCH 0: training on 162932 raw words (34518 effective words) took 0.0s, 715887 effective words/s\n",
      "EPOCH 1: training on 162932 raw words (34696 effective words) took 0.0s, 733661 effective words/s\n",
      "EPOCH 2: training on 162932 raw words (34962 effective words) took 0.0s, 771778 effective words/s\n",
      "EPOCH 3: training on 162932 raw words (34820 effective words) took 0.0s, 944020 effective words/s\n",
      "EPOCH 4: training on 162932 raw words (34755 effective words) took 0.0s, 958209 effective words/s\n",
      "Word2Vec lifecycle event {'msg': 'training on 814660 raw words (173751 effective words) took 0.5s, 347170 effective words/s', 'datetime': '2022-12-03T14:17:57.477012', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "Word2Vec lifecycle event {'fname_or_handle': './model/model_lvl1_100.w2v', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-12-03T14:17:57.477514', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'}\n",
      "not storing attribute cum_table\n",
      "saved ./model/model_lvl1_100.w2v\n",
      "Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2022-12-03T14:18:01.175012', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 48827 words, keeping 326 word types\n",
      "PROGRESS: at sentence #20000, processed 99744 words, keeping 346 word types\n",
      "PROGRESS: at sentence #30000, processed 148629 words, keeping 352 word types\n",
      "collected 355 word types from a corpus of 166315 raw words and 33778 sentences\n",
      "Creating a fresh vocabulary\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 306 unique words (86.20% of original 355, drops 49)', 'datetime': '2022-12-03T14:18:01.219512', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 166201 word corpus (99.93% of original 166315, drops 114)', 'datetime': '2022-12-03T14:18:01.220014', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "deleting the raw counts dictionary of 355 items\n",
      "sample=0.001 downsamples 71 most-common words\n",
      "Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 65623.83878067764 word corpus (39.5%% of prior 166201)', 'datetime': '2022-12-03T14:18:01.224513', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "estimated required memory for 306 words and 100 dimensions: 397800 bytes\n",
      "resetting layer weights\n",
      "Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-12-03T14:18:01.232013', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'training model with 4 workers on 306 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-12-03T14:18:01.233013', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "EPOCH 0: training on 166315 raw words (65623 effective words) took 0.1s, 1141238 effective words/s\n",
      "EPOCH 1: training on 166315 raw words (66001 effective words) took 0.1s, 1073814 effective words/s\n",
      "EPOCH 2: training on 166315 raw words (65562 effective words) took 0.1s, 1071695 effective words/s\n",
      "EPOCH 3: training on 166315 raw words (65757 effective words) took 0.1s, 1106467 effective words/s\n",
      "EPOCH 4: training on 166315 raw words (65423 effective words) took 0.1s, 1123011 effective words/s\n",
      "Word2Vec lifecycle event {'msg': 'training on 831575 raw words (328366 effective words) took 0.4s, 828199 effective words/s', 'datetime': '2022-12-03T14:18:01.630012', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "Word2Vec lifecycle event {'fname_or_handle': './model/model_lvl2_100.w2v', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-12-03T14:18:01.630512', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'}\n",
      "not storing attribute cum_table\n",
      "saved ./model/model_lvl2_100.w2v\n",
      "Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2022-12-03T14:18:05.568511', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 47604 words, keeping 748 word types\n",
      "PROGRESS: at sentence #20000, processed 88450 words, keeping 892 word types\n",
      "PROGRESS: at sentence #30000, processed 127099 words, keeping 967 word types\n",
      "collected 985 word types from a corpus of 142741 raw words and 33778 sentences\n",
      "Creating a fresh vocabulary\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 625 unique words (63.45% of original 985, drops 360)', 'datetime': '2022-12-03T14:18:05.609512', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 142022 word corpus (99.50% of original 142741, drops 719)', 'datetime': '2022-12-03T14:18:05.610513', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "deleting the raw counts dictionary of 985 items\n",
      "sample=0.001 downsamples 84 most-common words\n",
      "Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 75874.9228846553 word corpus (53.4%% of prior 142022)', 'datetime': '2022-12-03T14:18:05.617514', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "estimated required memory for 625 words and 100 dimensions: 812500 bytes\n",
      "resetting layer weights\n",
      "Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-12-03T14:18:05.629014', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'training model with 4 workers on 625 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-12-03T14:18:05.630512', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "EPOCH 0: training on 142741 raw words (75841 effective words) took 0.1s, 1035159 effective words/s\n",
      "EPOCH 1: training on 142741 raw words (75845 effective words) took 0.1s, 1155325 effective words/s\n",
      "EPOCH 2: training on 142741 raw words (75821 effective words) took 0.0s, 1633461 effective words/s\n",
      "EPOCH 3: training on 142741 raw words (75707 effective words) took 0.1s, 1079034 effective words/s\n",
      "EPOCH 4: training on 142741 raw words (75897 effective words) took 0.1s, 1069849 effective words/s\n",
      "Word2Vec lifecycle event {'msg': 'training on 713705 raw words (379111 effective words) took 0.4s, 876960 effective words/s', 'datetime': '2022-12-03T14:18:06.063513', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "Word2Vec lifecycle event {'fname_or_handle': './model/model_lvl3_100.w2v', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-12-03T14:18:06.064015', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'}\n",
      "not storing attribute cum_table\n",
      "saved ./model/model_lvl3_100.w2v\n",
      "Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2022-12-03T14:18:09.595511', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 51158 words, keeping 1304 word types\n",
      "PROGRESS: at sentence #20000, processed 99672 words, keeping 1672 word types\n",
      "PROGRESS: at sentence #30000, processed 145763 words, keeping 1874 word types\n",
      "collected 1934 word types from a corpus of 162819 raw words and 33778 sentences\n",
      "Creating a fresh vocabulary\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1018 unique words (52.64% of original 1934, drops 916)', 'datetime': '2022-12-03T14:18:09.645012', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 161154 word corpus (98.98% of original 162819, drops 1665)', 'datetime': '2022-12-03T14:18:09.646015', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "deleting the raw counts dictionary of 1934 items\n",
      "sample=0.001 downsamples 79 most-common words\n",
      "Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 98054.12648388743 word corpus (60.8%% of prior 161154)', 'datetime': '2022-12-03T14:18:09.654511', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "estimated required memory for 1018 words and 100 dimensions: 1323400 bytes\n",
      "resetting layer weights\n",
      "Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-12-03T14:18:09.673013', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'training model with 4 workers on 1018 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-12-03T14:18:09.674513', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "EPOCH 0: training on 162819 raw words (98045 effective words) took 0.1s, 1163916 effective words/s\n",
      "EPOCH 1: training on 162819 raw words (98011 effective words) took 0.1s, 1134243 effective words/s\n",
      "EPOCH 2: training on 162819 raw words (98080 effective words) took 0.1s, 1126078 effective words/s\n",
      "EPOCH 3: training on 162819 raw words (98250 effective words) took 0.1s, 1039302 effective words/s\n",
      "EPOCH 4: training on 162819 raw words (98078 effective words) took 0.1s, 1394586 effective words/s\n",
      "Word2Vec lifecycle event {'msg': 'training on 814095 raw words (490464 effective words) took 0.5s, 966884 effective words/s', 'datetime': '2022-12-03T14:18:10.182012', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "Word2Vec lifecycle event {'fname_or_handle': './model/model_lvl4_100.w2v', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-12-03T14:18:10.182511', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'}\n",
      "not storing attribute cum_table\n",
      "saved ./model/model_lvl4_100.w2v\n",
      "Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.025>', 'datetime': '2022-12-03T14:18:16.068020', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 47483 words, keeping 59 word types\n",
      "PROGRESS: at sentence #20000, processed 97985 words, keeping 59 word types\n",
      "PROGRESS: at sentence #30000, processed 145475 words, keeping 59 word types\n",
      "collected 59 word types from a corpus of 162932 raw words and 33778 sentences\n",
      "Creating a fresh vocabulary\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 59 unique words (100.00% of original 59, drops 0)', 'datetime': '2022-12-03T14:18:16.108519', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 162932 word corpus (100.00% of original 162932, drops 0)', 'datetime': '2022-12-03T14:18:16.109020', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "deleting the raw counts dictionary of 59 items\n",
      "sample=0.001 downsamples 39 most-common words\n",
      "Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 34713.048943067755 word corpus (21.3%% of prior 162932)', 'datetime': '2022-12-03T14:18:16.111522', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "estimated required memory for 59 words and 300 dimensions: 171100 bytes\n",
      "resetting layer weights\n",
      "Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-12-03T14:18:16.118019', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'training model with 4 workers on 59 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-12-03T14:18:16.118520', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "EPOCH 0: training on 162932 raw words (34467 effective words) took 0.1s, 664456 effective words/s\n",
      "EPOCH 1: training on 162932 raw words (34916 effective words) took 0.0s, 814143 effective words/s\n",
      "EPOCH 2: training on 162932 raw words (34733 effective words) took 0.0s, 717625 effective words/s\n",
      "EPOCH 3: training on 162932 raw words (34740 effective words) took 0.0s, 777093 effective words/s\n",
      "EPOCH 4: training on 162932 raw words (34878 effective words) took 0.1s, 678785 effective words/s\n",
      "Word2Vec lifecycle event {'msg': 'training on 814660 raw words (173734 effective words) took 0.4s, 495185 effective words/s', 'datetime': '2022-12-03T14:18:16.470019', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "Word2Vec lifecycle event {'fname_or_handle': './model/model_lvl1_300.w2v', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-12-03T14:18:16.470520', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'}\n",
      "not storing attribute cum_table\n",
      "saved ./model/model_lvl1_300.w2v\n",
      "Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.025>', 'datetime': '2022-12-03T14:18:21.113520', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 48827 words, keeping 326 word types\n",
      "PROGRESS: at sentence #20000, processed 99744 words, keeping 346 word types\n",
      "PROGRESS: at sentence #30000, processed 148629 words, keeping 352 word types\n",
      "collected 355 word types from a corpus of 166315 raw words and 33778 sentences\n",
      "Creating a fresh vocabulary\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 306 unique words (86.20% of original 355, drops 49)', 'datetime': '2022-12-03T14:18:21.156518', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 166201 word corpus (99.93% of original 166315, drops 114)', 'datetime': '2022-12-03T14:18:21.157019', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "deleting the raw counts dictionary of 355 items\n",
      "sample=0.001 downsamples 71 most-common words\n",
      "Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 65623.83878067764 word corpus (39.5%% of prior 166201)', 'datetime': '2022-12-03T14:18:21.161519', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "estimated required memory for 306 words and 300 dimensions: 887400 bytes\n",
      "resetting layer weights\n",
      "Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-12-03T14:18:21.172018', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'training model with 4 workers on 306 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-12-03T14:18:21.173019', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "EPOCH 0: training on 166315 raw words (65747 effective words) took 0.1s, 973113 effective words/s\n",
      "EPOCH 1: training on 166315 raw words (65523 effective words) took 0.1s, 1009065 effective words/s\n",
      "EPOCH 2: training on 166315 raw words (65221 effective words) took 0.1s, 1039579 effective words/s\n",
      "EPOCH 3: training on 166315 raw words (65726 effective words) took 0.1s, 1145987 effective words/s\n",
      "EPOCH 4: training on 166315 raw words (65828 effective words) took 0.1s, 1291079 effective words/s\n",
      "Word2Vec lifecycle event {'msg': 'training on 831575 raw words (328045 effective words) took 0.4s, 778482 effective words/s', 'datetime': '2022-12-03T14:18:21.595020', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "Word2Vec lifecycle event {'fname_or_handle': './model/model_lvl2_300.w2v', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-12-03T14:18:21.596018', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'}\n",
      "not storing attribute cum_table\n",
      "saved ./model/model_lvl2_300.w2v\n",
      "Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.025>', 'datetime': '2022-12-03T14:18:26.935018', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 47604 words, keeping 748 word types\n",
      "PROGRESS: at sentence #20000, processed 88450 words, keeping 892 word types\n",
      "PROGRESS: at sentence #30000, processed 127099 words, keeping 967 word types\n",
      "collected 985 word types from a corpus of 142741 raw words and 33778 sentences\n",
      "Creating a fresh vocabulary\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 625 unique words (63.45% of original 985, drops 360)', 'datetime': '2022-12-03T14:18:26.977518', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 142022 word corpus (99.50% of original 142741, drops 719)', 'datetime': '2022-12-03T14:18:26.978519', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "deleting the raw counts dictionary of 985 items\n",
      "sample=0.001 downsamples 84 most-common words\n",
      "Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 75874.9228846553 word corpus (53.4%% of prior 142022)', 'datetime': '2022-12-03T14:18:26.985521', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "estimated required memory for 625 words and 300 dimensions: 1812500 bytes\n",
      "resetting layer weights\n",
      "Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-12-03T14:18:26.999519', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'training model with 4 workers on 625 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-12-03T14:18:27.000519', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "EPOCH 0: training on 142741 raw words (75774 effective words) took 0.1s, 989240 effective words/s\n",
      "EPOCH 1: training on 142741 raw words (75848 effective words) took 0.1s, 855063 effective words/s\n",
      "EPOCH 2: training on 142741 raw words (75730 effective words) took 0.1s, 1122679 effective words/s\n",
      "EPOCH 3: training on 142741 raw words (76009 effective words) took 0.1s, 1267360 effective words/s\n",
      "EPOCH 4: training on 142741 raw words (75863 effective words) took 0.1s, 1093680 effective words/s\n",
      "Word2Vec lifecycle event {'msg': 'training on 713705 raw words (379224 effective words) took 0.5s, 792342 effective words/s', 'datetime': '2022-12-03T14:18:27.479519', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "Word2Vec lifecycle event {'fname_or_handle': './model/model_lvl3_300.w2v', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-12-03T14:18:27.480519', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'}\n",
      "not storing attribute cum_table\n",
      "saved ./model/model_lvl3_300.w2v\n",
      "Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.025>', 'datetime': '2022-12-03T14:18:32.620019', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 51158 words, keeping 1304 word types\n",
      "PROGRESS: at sentence #20000, processed 99672 words, keeping 1672 word types\n",
      "PROGRESS: at sentence #30000, processed 145763 words, keeping 1874 word types\n",
      "collected 1934 word types from a corpus of 162819 raw words and 33778 sentences\n",
      "Creating a fresh vocabulary\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1018 unique words (52.64% of original 1934, drops 916)', 'datetime': '2022-12-03T14:18:32.671020', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 161154 word corpus (98.98% of original 162819, drops 1665)', 'datetime': '2022-12-03T14:18:32.672020', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "deleting the raw counts dictionary of 1934 items\n",
      "sample=0.001 downsamples 79 most-common words\n",
      "Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 98054.12648388743 word corpus (60.8%% of prior 161154)', 'datetime': '2022-12-03T14:18:32.683022', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "estimated required memory for 1018 words and 300 dimensions: 2952200 bytes\n",
      "resetting layer weights\n",
      "Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-12-03T14:18:32.701518', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'training model with 4 workers on 1018 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-12-03T14:18:32.702518', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "EPOCH 0: training on 162819 raw words (98067 effective words) took 0.1s, 999243 effective words/s\n",
      "EPOCH 1: training on 162819 raw words (98037 effective words) took 0.1s, 1013603 effective words/s\n",
      "EPOCH 2: training on 162819 raw words (98076 effective words) took 0.1s, 1003253 effective words/s\n",
      "EPOCH 3: training on 162819 raw words (98027 effective words) took 0.1s, 960695 effective words/s\n",
      "EPOCH 4: training on 162819 raw words (97922 effective words) took 0.1s, 1037165 effective words/s\n",
      "Word2Vec lifecycle event {'msg': 'training on 814095 raw words (490129 effective words) took 0.6s, 843998 effective words/s', 'datetime': '2022-12-03T14:18:33.283519', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "Word2Vec lifecycle event {'fname_or_handle': './model/model_lvl4_300.w2v', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-12-03T14:18:33.284518', 'gensim': '4.2.0', 'python': '3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'}\n",
      "not storing attribute cum_table\n",
      "saved ./model/model_lvl4_300.w2v\n"
     ]
    }
   ],
   "source": [
    "# Mean representation for each level of main icd9 code\n",
    "\n",
    "levels = [1, 2, 3, 4]\n",
    "\n",
    "def get_embeddings(level, size=100):\n",
    "\n",
    "    train.dropna(subset=[f'y_l{level}'], inplace = True)\n",
    "    valid.dropna(subset=[f'y_l{level}'], inplace = True)\n",
    "    test.dropna(subset=[f'y_l{level}'], inplace = True)\n",
    "\n",
    "    sentences = train[f'y_l{level}'].map(lambda t: t.split()).values\n",
    "    model = w2v.Word2Vec(vector_size = size, min_count = 5, workers = 4, epochs = 5)\n",
    "    model.build_vocab(sentences)\n",
    "    model.train(sentences, total_examples = model.corpus_count, epochs = model.epochs)\n",
    "    model.save(f'./model/model_lvl{level}_{size}.w2v')\n",
    "\n",
    "    wv = model.wv\n",
    "\n",
    "    vocab = model.wv.key_to_index  \n",
    "\n",
    "    ind2w = {i+1:w for i,w in enumerate(sorted(vocab))}\n",
    "    w2ind = {w:i for i,w in ind2w.items()}\n",
    "\n",
    "    PAD_CHAR = \"**PAD**\"\n",
    "\n",
    "    outfile = f'./model/model_lvl{level}_{size}.embed'\n",
    "    W, words = build_matrix(ind2w, wv)\n",
    "\n",
    "    build_matrix(ind2w, wv)\n",
    "    save_embeddings(W, words, outfile)\n",
    "\n",
    "    loaded_embed = torch.Tensor(load_embeddings(outfile))\n",
    "\n",
    "    embed = nn.Embedding(loaded_embed.size()[0], loaded_embed.size()[1], padding_idx = 0)\n",
    "    embed.weight.data = loaded_embed.clone()\n",
    "\n",
    "    train[f'y_l{level}_tokens_{size}'] = train[f'y_l{level}'].map(lambda t: t.split() if (len(t) >= 1) else t).values\n",
    "    valid[f'y_l{level}_tokens_{size}'] = valid[f'y_l{level}'].map(lambda t: t.split() if (len(t) >= 1) else t).values\n",
    "    test[f'y_l{level}_tokens_{size}'] = test[f'y_l{level}'].map(lambda t: t.split() if (len(t) >= 1) else t).values\n",
    "\n",
    "    train[f\"y_l{level}_idx_{size}\"] = train[f'y_l{level}_tokens_{size}'].map(lambda t: [int(w2ind[w]) if w in w2ind else len(w2ind)+1 for w in t])\n",
    "    valid[f\"y_l{level}_idx_{size}\"] = valid[f'y_l{level}_tokens_{size}'].map(lambda t: [int(w2ind[w]) if w in w2ind else len(w2ind)+1 for w in t])\n",
    "    test[f\"y_l{level}_idx_{size}\"] = test[f'y_l{level}_tokens_{size}'].map(lambda t: [int(w2ind[w]) if w in w2ind else len(w2ind)+1 for w in t])\n",
    "\n",
    "    train[f\"y_{level}_emb_{size}\"] = train[f'y_l{level}_idx_{size}'].map(lambda t: [np.array(embed.weight.data[w]) for w in t])\n",
    "    valid[f\"y_{level}_emb_{size}\"] = valid[f'y_l{level}_idx_{size}'].map(lambda t: [np.array(embed.weight.data[w]) for w in t])\n",
    "    test[f\"y_{level}_emb_{size}\"] = test[f'y_l{level}_idx_{size}'].map(lambda t: [np.array(embed.weight.data[w]) for w in t])\n",
    "\n",
    "    return embed.weight.data\n",
    "\n",
    "# DOES NOT RUN YET\n",
    "lvl1_embed = get_embeddings(1)\n",
    "lvl2_embed = get_embeddings(2)\n",
    "lvl3_embed = get_embeddings(3)\n",
    "lvl4_embed = get_embeddings(4)\n",
    "\n",
    "#need 300 var embed for gru\n",
    "lvl1_embed_300 = get_embeddings(1, 300)\n",
    "lvl2_embed_300 = get_embeddings(2, 300)\n",
    "lvl3_embed_300 = get_embeddings(3, 300)\n",
    "lvl4_embed_300 = get_embeddings(4, 300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3524d3",
   "metadata": {},
   "source": [
    "## Baseline Model: TFIDF-Atomic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00750564",
   "metadata": {},
   "source": [
    "#### TFIDF document weights were input into a multinomial logistic regression classifier in order to predict the 4 levels of icd hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0df4d0",
   "metadata": {},
   "source": [
    "### Predict level 1 icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32b285c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:07<00:00,  7.08s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6871122590400897,\n",
       " 'recall': 0.699748743718593,\n",
       " 'f1-score': 0.6768162770150218,\n",
       " 'support': 10348}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multinomial Logistic Regression from sklearn \n",
    "\n",
    "# L2 values to iterate over\n",
    "L2 = np.logspace(.01, 100, num = 50)\n",
    "L2 = [1.023292992280754] # In order to test different L2 values comment out this line\n",
    "\n",
    "# # Enumerate the l1 classes\n",
    "y_l1_dict = dict(zip(np.unique(train['y_l1'].values), np.arange(0, len(np.unique(train['y_l1'].values)), 1)))\n",
    "y_l1_trn = train['y_l1'].map(lambda x: y_l1_dict.get(x))\n",
    "y_l1_val = valid['y_l1'].map(lambda x: y_l1_dict.get(x))\n",
    "y_l1_tst = test['y_l1'].map(lambda x: y_l1_dict.get(x))\n",
    "\n",
    "def log_reg_val(y_trn, y_val, x_trn, x_val, L2):\n",
    "    \n",
    "    # Use the validation data to determine the best model parameter for C\n",
    "    scores = []\n",
    "    i = 1\n",
    "    for C in tqdm(L2): \n",
    "        \n",
    "        tfidf_atomic = LogisticRegression(multi_class = 'multinomial', penalty = 'l2', max_iter = 1500, solver = \"sag\", random_state = 42, C = C)\n",
    "        tfidf_atomic_L1 = tfidf_atomic.fit(x_trn, y_trn)\n",
    "        y_v_pred = tfidf_atomic_L1.predict(x_val)\n",
    "        report = metrics.classification_report(y_val, y_v_pred, digits = 3, output_dict=True)\n",
    "        scores.append((C, report['weighted avg']['recall']))\n",
    "\n",
    "    # C value for best model on valid data\n",
    "    best_C = max(scores,key=lambda x:x[1])[0]\n",
    "    return best_C\n",
    "\n",
    "best_C = log_reg_val(x_trn = tfidf_train, y_trn = y_l1_trn, L2 = L2, y_val = y_l1_val, x_val = tfidf_valid)\n",
    "\n",
    "# Get results on test data for best model\n",
    "tfidf_atomic = LogisticRegression(multi_class = 'multinomial', penalty = 'l2', max_iter = 1500, solver = \"sag\", random_state = 42, C = best_C)\n",
    "tfidf_atomic_l1 = tfidf_atomic.fit(tfidf_train, y_l1_trn)\n",
    "y_pred = tfidf_atomic_l1.predict(tfidf_test)\n",
    "L1_test_report = metrics.classification_report(y_l1_tst, y_pred, digits = 3, output_dict=True)\n",
    "L1_test_report['weighted avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "23aa8ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C for tfidf tier1: 1.023292992280754\n"
     ]
    }
   ],
   "source": [
    "#BEST C Found for tfidf tier1: 1.023292992280754\n",
    "print(f'best C for tfidf tier1: {best_C}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0860306",
   "metadata": {},
   "source": [
    "### Predict level 2 icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f8d7971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [01:39<00:00, 99.61s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.5401008291740531,\n",
       " 'recall': 0.5593580819798917,\n",
       " 'f1-score': 0.5199440875488708,\n",
       " 'support': 10344}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multinomial Logistic Regression from sklearn \n",
    "\n",
    "# L2 values to iterate over; in order to run all iterations uncomment the line below\n",
    "L2 = np.logspace(.01, 100, num = 50)\n",
    "L2 = [1.023292992280754] # In order to test different L2 values comment out this line\n",
    "\n",
    "# Enumerate the l2 classes\n",
    "y_l2_dict = dict(zip(np.unique(train['y_l2'].values), np.arange(0, len(np.unique(train['y_l2'].values)), 1)))\n",
    "y_l2_trn = train['y_l2'].map(lambda x: y_l2_dict.get(x)).fillna('Missing')\n",
    "y_l2_val = valid['y_l2'].map(lambda x: y_l2_dict.get(x)).fillna('Missing')\n",
    "y_l2_tst = test['y_l2'].map(lambda x: y_l2_dict.get(x)).fillna('Missing')\n",
    "\n",
    "# Only include lvl2 labels that are in the training set\n",
    "tst_miss = [y_l2_tst != \"Missing\"]\n",
    "tst_idx_2 = np.where(y_l2_tst != 'Missing')\n",
    "y_l2_tst = np.array(y_l2_tst[y_l2_tst != \"Missing\"]).astype(int)\n",
    "\n",
    "val_miss = [y_l2_val != \"Missing\"]\n",
    "val_idx_2 = np.where(y_l2_val != 'Missing')\n",
    "y_l2_val = np.array(y_l2_val[y_l2_val != \"Missing\"]).astype(int)\n",
    "\n",
    "best_C = log_reg_val(x_trn = tfidf_train, y_trn = y_l2_trn, L2 = L2, y_val = y_l2_val, x_val = tfidf_valid[list(val_idx_2[0]), :])\n",
    "\n",
    "# Get results on test data for best model\n",
    "tfidf_atomic = LogisticRegression(multi_class = 'multinomial', penalty = 'l2', max_iter = 1500, solver = \"sag\", random_state = 42, C = best_C)\n",
    "tfidf_atomic_l2 = tfidf_atomic.fit(tfidf_train, y_l2_trn)\n",
    "y_pred = tfidf_atomic_l2.predict(tfidf_test[list(tst_idx_2[0]), :])\n",
    "L2_test_report = metrics.classification_report(y_l2_tst, y_pred, digits = 3, output_dict=True)\n",
    "L2_test_report['weighted avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7fa5e732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C for tfidf tier2: 1.023292992280754\n"
     ]
    }
   ],
   "source": [
    "# BEST C Found for tfidf tier2: 1.023292992280754\n",
    "print(f'best C for tfidf tier2: {best_C}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d4d31",
   "metadata": {},
   "source": [
    "### Predict level 3 icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "22270f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [05:17<00:00, 317.86s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.40406743146821095,\n",
       " 'recall': 0.45400737434504174,\n",
       " 'f1-score': 0.3950405405347697,\n",
       " 'support': 10306}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multinomial Logistic Regression from sklearn \n",
    "\n",
    "# L2 values to iterate over; in order to run all iterations uncomment the line below\n",
    "L2 = np.logspace(.01, 100, num = 50)\n",
    "L2 = [1.023292992280754] # In order to test different L2 values comment out this line\n",
    "\n",
    "# Enumerate the l3 classes\n",
    "train_miss = train['y_l3'].isna()\n",
    "train_l3 = train[~train_miss]\n",
    "y_l3_dict = dict(zip(set(train_l3['y_l3'].values), np.arange(0, len(set(train_l3['y_l3'].values)), 1)))\n",
    "\n",
    "y_l3_trn = train_l3['y_l3'].map(lambda x: y_l3_dict.get(x)).fillna('Missing')\n",
    "y_l3_val = valid['y_l3'].map(lambda x: y_l3_dict.get(x)).fillna('Missing')\n",
    "y_l3_tst = test['y_l3'].map(lambda x: y_l3_dict.get(x)).fillna('Missing')\n",
    "\n",
    "# Only include lvl2 labels that are in the training set\n",
    "tst_miss = [y_l3_tst != \"Missing\"]\n",
    "tst_idx_3 = np.where(y_l3_tst != 'Missing')\n",
    "y_l3_tst = np.array(y_l3_tst[y_l3_tst != \"Missing\"]).astype(int)\n",
    "\n",
    "val_miss = [y_l3_val != \"Missing\"]\n",
    "val_idx_3 = np.where(y_l3_val != 'Missing')\n",
    "y_l3_val = np.array(y_l3_val[y_l3_val != \"Missing\"]).astype(int)\n",
    "\n",
    "best_C = log_reg_val(x_trn = tfidf_train[~train_miss,:], y_trn = y_l3_trn, L2 = L2, y_val = y_l3_val, x_val = tfidf_valid[list(val_idx_3[0]), :])\n",
    "\n",
    "# Get results on test data for best model\n",
    "tfidf_atomic = LogisticRegression(multi_class = 'multinomial', penalty = 'l2', max_iter = 1500, solver = \"sag\", random_state = 42, C = best_C)\n",
    "tfidf_atomic_l3 = tfidf_atomic.fit(tfidf_train[~train_miss,:], y_l3_trn)\n",
    "y_pred = tfidf_atomic_l3.predict(tfidf_test[list(tst_idx_3[0]), :])\n",
    "L3_test_report = metrics.classification_report(y_l3_tst, y_pred, digits = 3, output_dict=True)\n",
    "L3_test_report['weighted avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c40b11df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C for tfidf tier3: 1.023292992280754\n"
     ]
    }
   ],
   "source": [
    "# BEST C Found for tfidf tier3:\n",
    "print(f'best C for tfidf tier3: {best_C}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0e964f",
   "metadata": {},
   "source": [
    "### Predict terminal icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ec8a3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:02<00:00,  2.98s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.043578096512380295,\n",
       " 'recall': 0.1340547703180212,\n",
       " 'f1-score': 0.05319495151327312,\n",
       " 'support': 4528}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multinomial Logistic Regression from sklearn \n",
    "\n",
    "# L2 values to iterate over; in order to run all iterations uncomment the line below\n",
    "L2 = np.logspace(.01, 100, num = 10)\n",
    "L2 = [1.023292992280754] # In order to test different L2 values comment out this line\n",
    "\n",
    "# Due to long training time; only predict a subset of the level 4 classes\n",
    "train_l4 = train[~train['y_l4'].isna()]\n",
    "\n",
    "# Get top X number of classes in the training data\n",
    "train_top = train_l4['y_l4'].value_counts()[0:32]\n",
    "\n",
    "# 16,869 data points after filtering for the top 32 classes \n",
    "train_l4 = train[train['y_l4'].isin(list(train_top.index))]\n",
    "\n",
    "y_l4_dict = dict(zip(set(train_l4['y_l4'].values), np.arange(0, len(set(train_l4['y_l4'].values)), 1)))\n",
    "\n",
    "train_l4 = train_l4[train['y_l4'].isin(list(train_top.index))]\n",
    "\n",
    "train_idx_4 = np.where(~train_l4['y_l4'].isna())\n",
    "train_l4 = train_l4[~train_l4['y_l4'].isna()]\n",
    "\n",
    "# Remove classes that are not in the dictionary\n",
    "y_l4_trn = train_l4['y_l4'].map(lambda x: y_l4_dict.get(x)).fillna('Missing')\n",
    "y_l4_val = valid['y_l4'].map(lambda x: y_l4_dict.get(x)).fillna('Missing')\n",
    "y_l4_tst = test['y_l4'].map(lambda x: y_l4_dict.get(x)).fillna('Missing')\n",
    "\n",
    "# Only include labels that are in the training set\n",
    "tst_miss = [y_l4_tst != \"Missing\"]\n",
    "tst_idx_4 = np.where(y_l4_tst != 'Missing')\n",
    "y_l4_tst = np.array(y_l4_tst[y_l4_tst != \"Missing\"]).astype(int)\n",
    "\n",
    "# Only include labels that are in the training set\n",
    "val_miss = [y_l4_val != \"Missing\"]\n",
    "val_idx_4 = np.where(y_l4_val != 'Missing')\n",
    "y_l4_val = np.array(y_l4_val[y_l4_val != \"Missing\"]).astype(int)\n",
    "\n",
    "# Test different l2 regularization coefficiants on the validation dataset \n",
    "best_C = log_reg_val(x_trn = tfidf_train[train_idx_4[0],:], y_trn = y_l4_trn, L2 = L2, y_val = y_l4_val, x_val = tfidf_valid[list(val_idx_4[0]), :])\n",
    "\n",
    "# Get results on test data for best model\n",
    "tfidf_atomic = LogisticRegression(multi_class = 'multinomial', penalty = 'l2', max_iter = 1500, solver = \"sag\", random_state = 42, C = best_C)\n",
    "tfidf_atomic_l4 = tfidf_atomic.fit(tfidf_train[~train_idx_4[0],:], y_l4_trn)\n",
    "y_pred = tfidf_atomic_l4.predict(tfidf_test[tst_idx_4[0], :])\n",
    "L4_test_report = metrics.classification_report(y_l4_tst, y_pred, digits = 3, output_dict=True)\n",
    "L4_test_report['weighted avg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe63853",
   "metadata": {},
   "source": [
    "## Mean-atomic\n",
    "\n",
    "### The mean embedding representation for the documents was input into a tensorflow softmax classifier to represent the labels atomically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "959488fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define NN\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "from numpy.random import seed\n",
    "\n",
    "def one_hot(y, c):\n",
    "    \n",
    "    # y--> label/ground truth.\n",
    "    # c--> Number of classes.\n",
    "    \n",
    "    # A zero matrix of size (m, c)\n",
    "    y_hot = np.zeros((len(y), c))\n",
    "    \n",
    "    # Putting 1 for column where the label is,\n",
    "    # Using multidimensional indexing.\n",
    "    y_hot[np.arange(len(y)), y] = 1\n",
    "    \n",
    "    return y_hot\n",
    "\n",
    "def nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate):\n",
    "    # initialize a tensorflow graph\n",
    "    graph = tf.Graph()\n",
    "  \n",
    "    with graph.as_default():\n",
    "        \"\"\"\n",
    "        defining all the nodes\n",
    "        \"\"\"\n",
    "    \n",
    "        # Inputs\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, num_features))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "        # Variables.\n",
    "        weights = tf.Variable(tf.truncated_normal([num_features, num_labels]))\n",
    "        biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "        # Training computation.\n",
    "        logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                            labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "        # Optimizer.\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "        # Predictions for the training, validation, and test data.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "        test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "\n",
    "    # utility function to calculate F1, precision, recall\n",
    "    def sm_metrics(predictions, labels):\n",
    "        y_pred = np.argmax(predictions, axis=1)\n",
    "        y_labs = np.argmax(labels, axis=1)\n",
    "        f1 = f1_score(y_labs, y_pred, average = 'weighted')\n",
    "        prec = precision_score(y_labs, y_pred, average = 'weighted')\n",
    "        rec = recall_score(y_labs, y_pred, average = 'weighted')\n",
    "        return f1, prec, rec\n",
    "    \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        # initialize weights and biases\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"Initialized\")\n",
    "    \n",
    "        for step in range(num_steps):\n",
    "            # pick a randomized offset\n",
    "            offset = np.random.randint(0, train_labels.shape[0] - batch_size - 1)\n",
    "    \n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "            # Prepare the feed dict\n",
    "            feed_dict = {tf_train_dataset : batch_data,\n",
    "                        tf_train_labels : batch_labels}\n",
    "    \n",
    "            # run one step of computation\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction],\n",
    "                                            feed_dict=feed_dict)\n",
    "    \n",
    "        print(\"\\n Test F1 Score: {:.3f}\".format(\n",
    "            sm_metrics(test_prediction.eval(), test_labels)[0]))\n",
    "        print(\"\\n Test Precision: {:.3f}\".format(\n",
    "            sm_metrics(test_prediction.eval(), test_labels)[1]))\n",
    "        print(\"\\n Test Recall: {:.3f}\".format(\n",
    "            sm_metrics(test_prediction.eval(), test_labels)[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138db98d",
   "metadata": {},
   "source": [
    "### Precict level 1 icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5433c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.566\n",
      "\n",
      " Test Precision: 0.558\n",
      "\n",
      " Test Recall: 0.598\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 200\n",
    "\n",
    "# number of target labels\n",
    "num_labels = len(np.unique(y_l1_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 5000\n",
    "\n",
    "\n",
    "  \n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['x_mean_emb'])['x_mean_emb'].apply(pd.Series)))\n",
    "train_labels = one_hot(y_l1_trn, num_labels)\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['x_mean_emb'])['x_mean_emb'].apply(pd.Series)))\n",
    "test_labels = one_hot(y_l1_tst, num_labels)\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['x_mean_emb'])['x_mean_emb'].apply(pd.Series)))\n",
    "valid_labels = one_hot(y_l1_val, num_labels)\n",
    "  \n",
    "\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e521018",
   "metadata": {},
   "source": [
    "### Precict level 2 icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e85b0200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.368\n",
      "\n",
      " Test Precision: 0.369\n",
      "\n",
      " Test Recall: 0.417\n"
     ]
    }
   ],
   "source": [
    "# Set the seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 200\n",
    "\n",
    "# number of target labels\n",
    "num_labels = len(np.unique(y_l2_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 5000\n",
    "  \n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['x_mean_emb'])['x_mean_emb'].apply(pd.Series)))\n",
    "train_labels = one_hot(y_l2_trn, num_labels)\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['x_mean_emb'][tst_idx_2[0]])['x_mean_emb'].apply(pd.Series)))\n",
    "test_labels = one_hot(y_l2_tst, num_labels)\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['x_mean_emb'][val_idx_2[0]])['x_mean_emb'].apply(pd.Series)))\n",
    "valid_labels = one_hot(y_l2_val, num_labels)\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7770d577",
   "metadata": {},
   "source": [
    "### Precict level 3 icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "06652bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.250\n",
      "\n",
      " Test Precision: 0.242\n",
      "\n",
      " Test Recall: 0.308\n"
     ]
    }
   ],
   "source": [
    "# Set the seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 200\n",
    "\n",
    "# number of target labels\n",
    "num_labels = len(np.unique(y_l3_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 5000\n",
    "\n",
    "train_idx = np.where(train_miss != True)\n",
    "  \n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['x_mean_emb'][train_idx[0]])['x_mean_emb'].apply(pd.Series)))\n",
    "train_labels = one_hot(y_l3_trn, num_labels)\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['x_mean_emb'][tst_idx_3[0]])['x_mean_emb'].apply(pd.Series)))\n",
    "test_labels = one_hot(y_l3_tst, num_labels)\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['x_mean_emb'][val_idx_3[0]])['x_mean_emb'].apply(pd.Series)))\n",
    "valid_labels = one_hot(y_l3_val, num_labels)\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b2f6a",
   "metadata": {},
   "source": [
    "### Precict terminal icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ec8b1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.047\n",
      "\n",
      " Test Precision: 0.054\n",
      "\n",
      " Test Recall: 0.155\n"
     ]
    }
   ],
   "source": [
    "# Set the seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 200\n",
    "\n",
    "# number of target labels\n",
    "num_labels = len(np.unique(y_l4_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 5000\n",
    "\n",
    "train_idx_4 = np.where(~train_l4['y_l4'].isna())\n",
    "train_l4 = train_l4[~train_l4['y_l4'].isna()]\n",
    "\n",
    "y_l4_dict = dict(zip(set(train_l4['y_l4'].values), np.arange(0, len(set(train_l4['y_l4'].values)), 1)))\n",
    "\n",
    "# Remove classes that are not in the dictionary\n",
    "y_l4_trn = train_l4['y_l4'].map(lambda x: y_l4_dict.get(x)).fillna('Missing')\n",
    "y_l4_val = valid['y_l4'].map(lambda x: y_l4_dict.get(x)).fillna('Missing')\n",
    "y_l4_tst = test['y_l4'].map(lambda x: y_l4_dict.get(x)).fillna('Missing')\n",
    "\n",
    "# Only include labels that are in the training set\n",
    "tst_idx_4 = np.where(y_l4_tst != 'Missing')\n",
    "y_l4_tst = np.array(y_l4_tst[y_l4_tst != \"Missing\"]).astype(int)\n",
    "\n",
    "# Only include labels that are in the training set\n",
    "val_idx_4 = np.where(y_l4_val != 'Missing')\n",
    "y_l4_val = np.array(y_l4_val[y_l4_val != \"Missing\"]).astype(int)\n",
    "  \n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['x_mean_emb'][train_idx_4[0]])['x_mean_emb'].apply(pd.Series)))\n",
    "train_labels = one_hot(y_l4_trn, num_labels)\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['x_mean_emb'][tst_idx_4[0]])['x_mean_emb'].apply(pd.Series)))\n",
    "test_labels = one_hot(y_l4_tst, num_labels)\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['x_mean_emb'][val_idx_4[0]])['x_mean_emb'].apply(pd.Series)))\n",
    "valid_labels = one_hot(y_l4_val, num_labels)\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c087ff3",
   "metadata": {},
   "source": [
    "# GRU Neural Network Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224952c2",
   "metadata": {},
   "source": [
    "### Prepare the Document data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "64689688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>HOPI</th>\n",
       "      <th>SENT_TOKENS</th>\n",
       "      <th>SENT_TOKENS_COMBO</th>\n",
       "      <th>idx_tokens</th>\n",
       "      <th>trunc_len</th>\n",
       "      <th>trunc_idx</th>\n",
       "      <th>...</th>\n",
       "      <th>y_l3_idx_300</th>\n",
       "      <th>y_3_emb_300</th>\n",
       "      <th>y_l4_tokens_300</th>\n",
       "      <th>y_l4_idx_300</th>\n",
       "      <th>y_4_emb_300</th>\n",
       "      <th>mean_y_1_emb</th>\n",
       "      <th>mean_y_2_emb</th>\n",
       "      <th>mean_y_3_emb</th>\n",
       "      <th>mean_y_4_emb</th>\n",
       "      <th>pad_y_1_emb_300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>172162</td>\n",
       "      <td>[99811, 2851, 5990, 56982, 42832, 5781, 42731,...</td>\n",
       "      <td>43529</td>\n",
       "      <td>Admission Date:  [**2115-9-20**]              ...</td>\n",
       "      <td>simvastatin coumadin B</td>\n",
       "      <td>[b, coumadin, simvastatin]</td>\n",
       "      <td>b coumadin simvastatin</td>\n",
       "      <td>[3034, 5613, 18335]</td>\n",
       "      <td>3</td>\n",
       "      <td>[599, 1684, 5846]</td>\n",
       "      <td>...</td>\n",
       "      <td>[118, 260, 468, 510, 99]</td>\n",
       "      <td>[[-0.070492014, 0.08166887, -0.08236186, 0.019...</td>\n",
       "      <td>[Hemorrhage, or, hematoma, complicating, a, pr...</td>\n",
       "      <td>[135, 771, 567, 420, 276, 836]</td>\n",
       "      <td>[[-0.07630981, -0.01891837, 0.029481728, 0.067...</td>\n",
       "      <td>[0.08410206, -0.08915926, -0.007887115, 0.0842...</td>\n",
       "      <td>[0.049000613, 0.124341995, 0.0381095, 0.133280...</td>\n",
       "      <td>[-0.10026093, 0.07895817, 0.041625906, 0.08159...</td>\n",
       "      <td>[0.0936941, 0.18062985, 0.08456796, 0.05860461...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121167</td>\n",
       "      <td>[41071, 41401, 4280, 4289, 4168, 42731, 25040,...</td>\n",
       "      <td>11161</td>\n",
       "      <td>Admission Date:  [**2119-3-8**]     Discharge ...</td>\n",
       "      <td>M D Dictated</td>\n",
       "      <td>[d, dictated, m]</td>\n",
       "      <td>d dictated m</td>\n",
       "      <td>[5954, 6531, 12240]</td>\n",
       "      <td>3</td>\n",
       "      <td>[487, 7132, 525]</td>\n",
       "      <td>...</td>\n",
       "      <td>[6, 449, 367]</td>\n",
       "      <td>[[0.017918952, -0.026299225, -0.0011831599, -0...</td>\n",
       "      <td>[Acute, myocardial, infarction,, subendocardia...</td>\n",
       "      <td>[17, 731, 609, 917, 608]</td>\n",
       "      <td>[[-0.06275281, 0.034771625, -0.00079423486, 0....</td>\n",
       "      <td>[-0.047526598, 0.05695837, 0.046640683, 0.0844...</td>\n",
       "      <td>[-0.091704376, 0.101866506, 0.0010930622, -0.0...</td>\n",
       "      <td>[0.04243465, 0.096725516, -0.020863669, -0.000...</td>\n",
       "      <td>[-0.17148994, 0.038766213, 0.08449237, 0.10921...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>185502</td>\n",
       "      <td>[1572, 1962]</td>\n",
       "      <td>9403</td>\n",
       "      <td>Admission Date:  [**2127-9-4**]       Discharg...</td>\n",
       "      <td>Distal pancreatic mass PHYSICAL</td>\n",
       "      <td>[distal, mass, pancreatic, physical]</td>\n",
       "      <td>distal mass pancreatic physical</td>\n",
       "      <td>[6799, 12436, 14640, 15230]</td>\n",
       "      <td>4</td>\n",
       "      <td>[1092, 156, 1911, 190]</td>\n",
       "      <td>...</td>\n",
       "      <td>[93, 453, 468, 480]</td>\n",
       "      <td>[[-0.032632485, 0.0983419, -0.13715059, -0.056...</td>\n",
       "      <td>[Malignant, neoplasm, of, tail, of, pancreas]</td>\n",
       "      <td>[176, 739, 764, 932, 764, 785]</td>\n",
       "      <td>[[-0.102827616, 0.046675693, 0.050170206, -0.0...</td>\n",
       "      <td>[-0.1599842, 0.16103004, -0.13086131, -0.04455...</td>\n",
       "      <td>[0.07937019, 0.08743847, 0.0049281376, 0.05704...</td>\n",
       "      <td>[-0.03318695, 0.10974249, -0.000789281, 0.0044...</td>\n",
       "      <td>[-0.010925144, 0.00046655908, 0.11926437, -0.0...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>135752</td>\n",
       "      <td>[53551, 2851, 42731, 53290, 04186, 4019, V1046]</td>\n",
       "      <td>10861</td>\n",
       "      <td>Admission Date:  [**2143-7-6**]       Discharg...</td>\n",
       "      <td>Upper gastrointestinal bleed PHYSICAL</td>\n",
       "      <td>[bleed, gastrointestinal, physical, upper]</td>\n",
       "      <td>bleed gastrointestinal physical upper</td>\n",
       "      <td>[3513, 9025, 15230, 20854]</td>\n",
       "      <td>4</td>\n",
       "      <td>[1007, 828, 190, 792]</td>\n",
       "      <td>...</td>\n",
       "      <td>[61, 185, 291]</td>\n",
       "      <td>[[0.05790018, -0.04345593, -0.0029073094, 0.08...</td>\n",
       "      <td>[Unspecified, gastritis, and, gastroduodenitis]</td>\n",
       "      <td>[263, 548, 304, 549]</td>\n",
       "      <td>[[0.050144576, 0.042814985, -0.06284214, -0.10...</td>\n",
       "      <td>[-0.049221464, 0.05471442, 0.04873963, 0.08467...</td>\n",
       "      <td>[-0.02281745, 0.14163129, -0.012033534, 0.0138...</td>\n",
       "      <td>[0.03242548, -0.054779034, -0.12693162, -0.067...</td>\n",
       "      <td>[0.069495015, 0.12684956, 0.029800836, 0.00118...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110176</td>\n",
       "      <td>[53140, 2851, 25000, 2449, 4240, 2720, V113]</td>\n",
       "      <td>17540</td>\n",
       "      <td>Admission Date:  [**2157-7-21**]     Discharge...</td>\n",
       "      <td>Upper gastrointestinal bleed PHYSICAL</td>\n",
       "      <td>[bleed, gastrointestinal, physical, upper]</td>\n",
       "      <td>bleed gastrointestinal physical upper</td>\n",
       "      <td>[3513, 9025, 15230, 20854]</td>\n",
       "      <td>4</td>\n",
       "      <td>[1007, 828, 190, 792]</td>\n",
       "      <td>...</td>\n",
       "      <td>[60, 601]</td>\n",
       "      <td>[[0.018469978, 0.017918635, -0.009278733, 0.00...</td>\n",
       "      <td>[Chronic, or, unspecified, gastric, ulcer, wit...</td>\n",
       "      <td>[64, 771, 980, 547, 974, 1014, 569]</td>\n",
       "      <td>[[-0.09219703, 0.011353552, 0.1069955, 0.02280...</td>\n",
       "      <td>[-0.049221464, 0.05471442, 0.04873963, 0.08467...</td>\n",
       "      <td>[-0.02281745, 0.14163129, -0.012033534, 0.0138...</td>\n",
       "      <td>[-0.07016368, 0.10291995, -0.0067881383, 0.037...</td>\n",
       "      <td>[-0.01074038, 0.08132404, -0.021112952, 0.0939...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                          ICD9_CODE  SUBJECT_ID  \\\n",
       "0   172162  [99811, 2851, 5990, 56982, 42832, 5781, 42731,...       43529   \n",
       "1   121167  [41071, 41401, 4280, 4289, 4168, 42731, 25040,...       11161   \n",
       "2   185502                                       [1572, 1962]        9403   \n",
       "3   135752    [53551, 2851, 42731, 53290, 04186, 4019, V1046]       10861   \n",
       "4   110176       [53140, 2851, 25000, 2449, 4240, 2720, V113]       17540   \n",
       "\n",
       "                                                TEXT  \\\n",
       "0  Admission Date:  [**2115-9-20**]              ...   \n",
       "1  Admission Date:  [**2119-3-8**]     Discharge ...   \n",
       "2  Admission Date:  [**2127-9-4**]       Discharg...   \n",
       "3  Admission Date:  [**2143-7-6**]       Discharg...   \n",
       "4  Admission Date:  [**2157-7-21**]     Discharge...   \n",
       "\n",
       "                                     HOPI  \\\n",
       "0                  simvastatin coumadin B   \n",
       "1                            M D Dictated   \n",
       "2         Distal pancreatic mass PHYSICAL   \n",
       "3   Upper gastrointestinal bleed PHYSICAL   \n",
       "4   Upper gastrointestinal bleed PHYSICAL   \n",
       "\n",
       "                                  SENT_TOKENS  \\\n",
       "0                  [b, coumadin, simvastatin]   \n",
       "1                            [d, dictated, m]   \n",
       "2        [distal, mass, pancreatic, physical]   \n",
       "3  [bleed, gastrointestinal, physical, upper]   \n",
       "4  [bleed, gastrointestinal, physical, upper]   \n",
       "\n",
       "                       SENT_TOKENS_COMBO                   idx_tokens  \\\n",
       "0                 b coumadin simvastatin          [3034, 5613, 18335]   \n",
       "1                           d dictated m          [5954, 6531, 12240]   \n",
       "2        distal mass pancreatic physical  [6799, 12436, 14640, 15230]   \n",
       "3  bleed gastrointestinal physical upper   [3513, 9025, 15230, 20854]   \n",
       "4  bleed gastrointestinal physical upper   [3513, 9025, 15230, 20854]   \n",
       "\n",
       "   trunc_len               trunc_idx  ...              y_l3_idx_300  \\\n",
       "0          3       [599, 1684, 5846]  ...  [118, 260, 468, 510, 99]   \n",
       "1          3        [487, 7132, 525]  ...             [6, 449, 367]   \n",
       "2          4  [1092, 156, 1911, 190]  ...       [93, 453, 468, 480]   \n",
       "3          4   [1007, 828, 190, 792]  ...            [61, 185, 291]   \n",
       "4          4   [1007, 828, 190, 792]  ...                 [60, 601]   \n",
       "\n",
       "                                         y_3_emb_300  \\\n",
       "0  [[-0.070492014, 0.08166887, -0.08236186, 0.019...   \n",
       "1  [[0.017918952, -0.026299225, -0.0011831599, -0...   \n",
       "2  [[-0.032632485, 0.0983419, -0.13715059, -0.056...   \n",
       "3  [[0.05790018, -0.04345593, -0.0029073094, 0.08...   \n",
       "4  [[0.018469978, 0.017918635, -0.009278733, 0.00...   \n",
       "\n",
       "                                     y_l4_tokens_300  \\\n",
       "0  [Hemorrhage, or, hematoma, complicating, a, pr...   \n",
       "1  [Acute, myocardial, infarction,, subendocardia...   \n",
       "2      [Malignant, neoplasm, of, tail, of, pancreas]   \n",
       "3    [Unspecified, gastritis, and, gastroduodenitis]   \n",
       "4  [Chronic, or, unspecified, gastric, ulcer, wit...   \n",
       "\n",
       "                          y_l4_idx_300  \\\n",
       "0       [135, 771, 567, 420, 276, 836]   \n",
       "1             [17, 731, 609, 917, 608]   \n",
       "2       [176, 739, 764, 932, 764, 785]   \n",
       "3                 [263, 548, 304, 549]   \n",
       "4  [64, 771, 980, 547, 974, 1014, 569]   \n",
       "\n",
       "                                         y_4_emb_300  \\\n",
       "0  [[-0.07630981, -0.01891837, 0.029481728, 0.067...   \n",
       "1  [[-0.06275281, 0.034771625, -0.00079423486, 0....   \n",
       "2  [[-0.102827616, 0.046675693, 0.050170206, -0.0...   \n",
       "3  [[0.050144576, 0.042814985, -0.06284214, -0.10...   \n",
       "4  [[-0.09219703, 0.011353552, 0.1069955, 0.02280...   \n",
       "\n",
       "                                        mean_y_1_emb  \\\n",
       "0  [0.08410206, -0.08915926, -0.007887115, 0.0842...   \n",
       "1  [-0.047526598, 0.05695837, 0.046640683, 0.0844...   \n",
       "2  [-0.1599842, 0.16103004, -0.13086131, -0.04455...   \n",
       "3  [-0.049221464, 0.05471442, 0.04873963, 0.08467...   \n",
       "4  [-0.049221464, 0.05471442, 0.04873963, 0.08467...   \n",
       "\n",
       "                                        mean_y_2_emb  \\\n",
       "0  [0.049000613, 0.124341995, 0.0381095, 0.133280...   \n",
       "1  [-0.091704376, 0.101866506, 0.0010930622, -0.0...   \n",
       "2  [0.07937019, 0.08743847, 0.0049281376, 0.05704...   \n",
       "3  [-0.02281745, 0.14163129, -0.012033534, 0.0138...   \n",
       "4  [-0.02281745, 0.14163129, -0.012033534, 0.0138...   \n",
       "\n",
       "                                        mean_y_3_emb  \\\n",
       "0  [-0.10026093, 0.07895817, 0.041625906, 0.08159...   \n",
       "1  [0.04243465, 0.096725516, -0.020863669, -0.000...   \n",
       "2  [-0.03318695, 0.10974249, -0.000789281, 0.0044...   \n",
       "3  [0.03242548, -0.054779034, -0.12693162, -0.067...   \n",
       "4  [-0.07016368, 0.10291995, -0.0067881383, 0.037...   \n",
       "\n",
       "                                        mean_y_4_emb  \\\n",
       "0  [0.0936941, 0.18062985, 0.08456796, 0.05860461...   \n",
       "1  [-0.17148994, 0.038766213, 0.08449237, 0.10921...   \n",
       "2  [-0.010925144, 0.00046655908, 0.11926437, -0.0...   \n",
       "3  [0.069495015, 0.12684956, 0.029800836, 0.00118...   \n",
       "4  [-0.01074038, 0.08132404, -0.021112952, 0.0939...   \n",
       "\n",
       "                                     pad_y_1_emb_300  \n",
       "0  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "1  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "3  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "4  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the training data and split it buckets of ~4000 tokens per bucket \n",
    "train['trunc_len'] = train['idx_tokens'].map(lambda c: len(c))\n",
    "sort = train.sort_values(by = ['trunc_len'])\n",
    "\n",
    "trn_bucket1 = sort[0:4000].reset_index(drop = True)\n",
    "trn_bucket2 = sort[4001:8001].reset_index(drop = True)\n",
    "trn_bucket3 = sort[8002:12002].reset_index(drop = True)\n",
    "trn_bucket4 = sort[12003:16003].reset_index(drop = True)\n",
    "trn_bucket5 = sort[16004:20004].reset_index(drop = True)\n",
    "trn_bucket6 = sort[20005:24005].reset_index(drop = True)\n",
    "trn_bucket7 = sort[24006:28006].reset_index(drop = True)\n",
    "trn_bucket8 = sort[28007:32007].reset_index(drop = True)\n",
    "trn_bucket9 = sort[32008:33008].reset_index(drop = True)\n",
    "trn_bucket10 = sort[33009:34009].reset_index(drop = True)\n",
    "# trn_bucket11 = sort[34010:35010].reset_index(drop = True)\n",
    "# trn_bucket12 = sort[35011:36008].reset_index(drop = True)\n",
    "\n",
    "valid['trunc_len'] = valid['idx_tokens'].map(lambda c: len(c))\n",
    "val_sort = valid.sort_values(by = ['trunc_len'])\n",
    "val_bucket1 = val_sort[0:4000].reset_index(drop = True)\n",
    "val_bucket2 = val_sort[4001:].reset_index(drop = True)\n",
    "\n",
    "\n",
    "test['trunc_len'] = test['idx_tokens'].map(lambda c: len(c))\n",
    "test_sort = test.sort_values(by = ['trunc_len'])\n",
    "test_bucket1 = test_sort[0:4000].reset_index(drop = True)\n",
    "test_bucket2 = test_sort[4001:8001].reset_index(drop = True)\n",
    "test_bucket3 = test_sort[8002:].reset_index(drop = True)\n",
    "\n",
    "# train_buckets = [trn_bucket1, trn_bucket2, trn_bucket3, trn_bucket4, trn_bucket5, trn_bucket6, trn_bucket7, trn_bucket8, trn_bucket9, trn_bucket10, trn_bucket11, trn_bucket12]\n",
    "train_buckets = [trn_bucket1, trn_bucket2, trn_bucket3, trn_bucket4, trn_bucket5, trn_bucket6, trn_bucket7, trn_bucket8, trn_bucket9, trn_bucket10]\n",
    "\n",
    "valid_buckets = [val_bucket1, val_bucket2]\n",
    "\n",
    "test_buckets = [test_bucket1, test_bucket2, test_bucket3]\n",
    "\n",
    "\n",
    "buckets = train_buckets + valid_buckets + test_buckets\n",
    "\n",
    "# Add paddings to docs in each bucket\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras import Sequential\n",
    "\n",
    "bi_grus = torch.nn.GRU(input_size=200, hidden_size=50, num_layers=1, dropout=.3, batch_first=False, bidirectional=True)\n",
    "\n",
    "# Pad the word embeddings in each bucket so that they are all the same length\n",
    "for b in train_buckets: \n",
    "    t = pad_sequences([torch.tensor(x) for x in b['idx_tokens']], padding='post')\n",
    "    b['pad_idx'] = t.tolist()\n",
    "    b['emb_tokens'] = b['pad_idx'].map(lambda t: [np.array(embed.weight.data[w]) for w in t])\n",
    "    b['gru_emb_tokens'] = b['emb_tokens'].map(lambda t: bi_grus(torch.tensor(t))[0].detach().numpy())\n",
    "\n",
    "\n",
    "trn_bucket1.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "825a6472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>HOPI</th>\n",
       "      <th>SENT_TOKENS</th>\n",
       "      <th>SENT_TOKENS_COMBO</th>\n",
       "      <th>idx_tokens</th>\n",
       "      <th>trunc_len</th>\n",
       "      <th>trunc_idx</th>\n",
       "      <th>...</th>\n",
       "      <th>y_l3_tokens_300</th>\n",
       "      <th>y_l3_idx_300</th>\n",
       "      <th>y_3_emb_300</th>\n",
       "      <th>y_l4_tokens_300</th>\n",
       "      <th>y_l4_idx_300</th>\n",
       "      <th>y_4_emb_300</th>\n",
       "      <th>mean_y_1_emb</th>\n",
       "      <th>mean_y_2_emb</th>\n",
       "      <th>mean_y_3_emb</th>\n",
       "      <th>mean_y_4_emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>155817</td>\n",
       "      <td>[41401, 42731, 4139, 9975, 78820, 2766, 3963, ...</td>\n",
       "      <td>19218</td>\n",
       "      <td>Admission Date:  [**2150-12-21**]       Discha...</td>\n",
       "      <td>M D Dictated</td>\n",
       "      <td>[d, dictated, m]</td>\n",
       "      <td>d dictated m</td>\n",
       "      <td>[5954, 6531, 12240]</td>\n",
       "      <td>3</td>\n",
       "      <td>[487, 7132, 525]</td>\n",
       "      <td>...</td>\n",
       "      <td>[Other, forms, of, chronic, ischemic, heart, d...</td>\n",
       "      <td>[118, 328, 468, 251, 387, 346, 280]</td>\n",
       "      <td>[[-0.070492014, 0.08166887, -0.08236186, 0.019...</td>\n",
       "      <td>[Coronary, atherosclerosis]</td>\n",
       "      <td>[76, 345]</td>\n",
       "      <td>[[-0.076981634, 0.06613417, 0.029547844, 0.023...</td>\n",
       "      <td>[-0.047526598, 0.05695837, 0.046640683, 0.0844...</td>\n",
       "      <td>[-0.091704376, 0.101866506, 0.0010930622, -0.0...</td>\n",
       "      <td>[-0.14592369, 0.11662416, -0.02554985, 0.08584...</td>\n",
       "      <td>[0.07375743, 0.0807147, 0.03924676, -0.0451159...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>158782</td>\n",
       "      <td>[27401, 42823, 2762, 41400, 78820, 4280, 25082...</td>\n",
       "      <td>40911</td>\n",
       "      <td>Admission Date:  [**2122-2-20**]              ...</td>\n",
       "      <td>EAST HOSPITAL MEDICINE ATTENDING ADMISSION NOTE</td>\n",
       "      <td>[admission, attending, east, hospital, medicin...</td>\n",
       "      <td>admission attending east hospital medicine note</td>\n",
       "      <td>[1785, 2875, 7249, 10002, 12566, 13831]</td>\n",
       "      <td>6</td>\n",
       "      <td>[679, 3437, 9760, 20, 729, 535]</td>\n",
       "      <td>...</td>\n",
       "      <td>[Gout]</td>\n",
       "      <td>[626]</td>\n",
       "      <td>[[0.0524441, -0.02220519, -0.06365412, -0.0774...</td>\n",
       "      <td>[Gouty, arthropathy]</td>\n",
       "      <td>[1019, 338]</td>\n",
       "      <td>[[-0.010246767, -0.01102714, -0.030377874, -0....</td>\n",
       "      <td>[0.053493697, -0.041262418, 0.0017242441, 0.07...</td>\n",
       "      <td>[-0.030272882, 0.10090023, 0.043690603, -0.035...</td>\n",
       "      <td>[0.041243035, -0.061357144, 0.016727474, -0.08...</td>\n",
       "      <td>[0.114566006, 0.08965349, 0.028740734, 0.08113...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>147821</td>\n",
       "      <td>[1888, 1981, 2639, 19882, 4019]</td>\n",
       "      <td>11193</td>\n",
       "      <td>Admission Date:  [**2133-1-20**]       Dischar...</td>\n",
       "      <td>Transitional cell carcioma of the bladder PHY...</td>\n",
       "      <td>[bladder, carcioma, cell, of, physical, the, t...</td>\n",
       "      <td>bladder carcioma cell of physical the transiti...</td>\n",
       "      <td>[3489, 21878, 4424, 14079, 15230, 19852, 20329]</td>\n",
       "      <td>7</td>\n",
       "      <td>[2240, 85, 29, 190, 46, 5622]</td>\n",
       "      <td>...</td>\n",
       "      <td>[Malignant, neoplasm, of, bladder]</td>\n",
       "      <td>[93, 453, 468, 217]</td>\n",
       "      <td>[[-0.032632485, 0.0983419, -0.13715059, -0.056...</td>\n",
       "      <td>[Malignant, neoplasm, of, other, specified, si...</td>\n",
       "      <td>[176, 739, 764, 780, 898, 887, 764, 357]</td>\n",
       "      <td>[[-0.102827616, 0.046675693, 0.050170206, -0.0...</td>\n",
       "      <td>[-0.1599842, 0.16103004, -0.13086131, -0.04455...</td>\n",
       "      <td>[0.07540788, 0.07400739, 0.008412799, 0.076379...</td>\n",
       "      <td>[-0.04314007, 0.11763805, -0.01063144, -0.0123...</td>\n",
       "      <td>[-0.04676784, -0.01674367, 0.10057579, -0.0128...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>172825</td>\n",
       "      <td>[73600, 4019]</td>\n",
       "      <td>1629</td>\n",
       "      <td>Admission Date: [**2158-4-6**]        Discharg...</td>\n",
       "      <td>Gunshot wound to the right hand PHYSICAL</td>\n",
       "      <td>[gunshot, hand, physical, right, the, to, wound]</td>\n",
       "      <td>gunshot hand physical right the to wound</td>\n",
       "      <td>[9412, 9481, 15230, 17458, 19852, 20108, 21700]</td>\n",
       "      <td>7</td>\n",
       "      <td>[2752, 1345, 190, 212, 46, 47, 286]</td>\n",
       "      <td>...</td>\n",
       "      <td>[Other, acquired, deformities, of, limbs]</td>\n",
       "      <td>[118, 170, 269, 468, 404]</td>\n",
       "      <td>[[-0.070492014, 0.08166887, -0.08236186, 0.019...</td>\n",
       "      <td>[Acquired, deformities, of, forearm,, excludin...</td>\n",
       "      <td>[16, 447, 764, 539, 1019, 1019]</td>\n",
       "      <td>[[-0.070184216, 0.047474142, 0.03269997, -0.04...</td>\n",
       "      <td>[0.007700827, -0.013504177, 0.045051254, 0.061...</td>\n",
       "      <td>[0.056160342, 0.07937857, -0.0058114543, 0.010...</td>\n",
       "      <td>[-0.07950522, 0.033617858, -0.022739127, 0.059...</td>\n",
       "      <td>[0.08234508, 0.06837807, 0.047677133, 0.039281...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>121183</td>\n",
       "      <td>[V3000, 76515, 769, 7707, 7742, 7793, 1123, 69...</td>\n",
       "      <td>7657</td>\n",
       "      <td>Admission Date: [**2172-3-14**]        Dischar...</td>\n",
       "      <td>A 29 week gestation infant admitted with resp...</td>\n",
       "      <td>[a, admitted, distress, gestation, infant, mat...</td>\n",
       "      <td>a admitted distress gestation infant maternal ...</td>\n",
       "      <td>[1424, 1793, 6821, 9116, 10751, 12459, 17222, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>[0, 299, 2292, 617, 626, 6029, 207, 445, 248]</td>\n",
       "      <td>...</td>\n",
       "      <td>[Single, liveborn]</td>\n",
       "      <td>[141, 407]</td>\n",
       "      <td>[[0.048772953, 0.033942036, -0.025983516, -0.0...</td>\n",
       "      <td>[Single, liveborn,, born, in, hospital]</td>\n",
       "      <td>[235, 671, 365, 604, 578]</td>\n",
       "      <td>[[-0.039175306, 0.031033965, 0.039253235, -0.0...</td>\n",
       "      <td>[0.12333105, 0.00020353997, 0.0308367, 0.11656...</td>\n",
       "      <td>[-0.048329808, 0.08315235, 0.008233227, 0.0701...</td>\n",
       "      <td>[0.0685036, 0.10682133, -0.005800104, 0.083534...</td>\n",
       "      <td>[0.07594408, 0.15636308, -0.036152676, 0.06416...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                          ICD9_CODE  SUBJECT_ID  \\\n",
       "0   155817  [41401, 42731, 4139, 9975, 78820, 2766, 3963, ...       19218   \n",
       "1   158782  [27401, 42823, 2762, 41400, 78820, 4280, 25082...       40911   \n",
       "2   147821                    [1888, 1981, 2639, 19882, 4019]       11193   \n",
       "3   172825                                      [73600, 4019]        1629   \n",
       "4   121183  [V3000, 76515, 769, 7707, 7742, 7793, 1123, 69...        7657   \n",
       "\n",
       "                                                TEXT  \\\n",
       "0  Admission Date:  [**2150-12-21**]       Discha...   \n",
       "1  Admission Date:  [**2122-2-20**]              ...   \n",
       "2  Admission Date:  [**2133-1-20**]       Dischar...   \n",
       "3  Admission Date: [**2158-4-6**]        Discharg...   \n",
       "4  Admission Date: [**2172-3-14**]        Dischar...   \n",
       "\n",
       "                                                HOPI  \\\n",
       "0                                       M D Dictated   \n",
       "1    EAST HOSPITAL MEDICINE ATTENDING ADMISSION NOTE   \n",
       "2   Transitional cell carcioma of the bladder PHY...   \n",
       "3           Gunshot wound to the right hand PHYSICAL   \n",
       "4   A 29 week gestation infant admitted with resp...   \n",
       "\n",
       "                                         SENT_TOKENS  \\\n",
       "0                                   [d, dictated, m]   \n",
       "1  [admission, attending, east, hospital, medicin...   \n",
       "2  [bladder, carcioma, cell, of, physical, the, t...   \n",
       "3   [gunshot, hand, physical, right, the, to, wound]   \n",
       "4  [a, admitted, distress, gestation, infant, mat...   \n",
       "\n",
       "                                   SENT_TOKENS_COMBO  \\\n",
       "0                                       d dictated m   \n",
       "1    admission attending east hospital medicine note   \n",
       "2  bladder carcioma cell of physical the transiti...   \n",
       "3           gunshot hand physical right the to wound   \n",
       "4  a admitted distress gestation infant maternal ...   \n",
       "\n",
       "                                          idx_tokens  trunc_len  \\\n",
       "0                                [5954, 6531, 12240]          3   \n",
       "1            [1785, 2875, 7249, 10002, 12566, 13831]          6   \n",
       "2    [3489, 21878, 4424, 14079, 15230, 19852, 20329]          7   \n",
       "3    [9412, 9481, 15230, 17458, 19852, 20108, 21700]          7   \n",
       "4  [1424, 1793, 6821, 9116, 10751, 12459, 17222, ...          9   \n",
       "\n",
       "                                       trunc_idx  ...  \\\n",
       "0                               [487, 7132, 525]  ...   \n",
       "1                [679, 3437, 9760, 20, 729, 535]  ...   \n",
       "2                  [2240, 85, 29, 190, 46, 5622]  ...   \n",
       "3            [2752, 1345, 190, 212, 46, 47, 286]  ...   \n",
       "4  [0, 299, 2292, 617, 626, 6029, 207, 445, 248]  ...   \n",
       "\n",
       "                                     y_l3_tokens_300  \\\n",
       "0  [Other, forms, of, chronic, ischemic, heart, d...   \n",
       "1                                             [Gout]   \n",
       "2                 [Malignant, neoplasm, of, bladder]   \n",
       "3          [Other, acquired, deformities, of, limbs]   \n",
       "4                                 [Single, liveborn]   \n",
       "\n",
       "                          y_l3_idx_300  \\\n",
       "0  [118, 328, 468, 251, 387, 346, 280]   \n",
       "1                                [626]   \n",
       "2                  [93, 453, 468, 217]   \n",
       "3            [118, 170, 269, 468, 404]   \n",
       "4                           [141, 407]   \n",
       "\n",
       "                                         y_3_emb_300  \\\n",
       "0  [[-0.070492014, 0.08166887, -0.08236186, 0.019...   \n",
       "1  [[0.0524441, -0.02220519, -0.06365412, -0.0774...   \n",
       "2  [[-0.032632485, 0.0983419, -0.13715059, -0.056...   \n",
       "3  [[-0.070492014, 0.08166887, -0.08236186, 0.019...   \n",
       "4  [[0.048772953, 0.033942036, -0.025983516, -0.0...   \n",
       "\n",
       "                                     y_l4_tokens_300  \\\n",
       "0                        [Coronary, atherosclerosis]   \n",
       "1                               [Gouty, arthropathy]   \n",
       "2  [Malignant, neoplasm, of, other, specified, si...   \n",
       "3  [Acquired, deformities, of, forearm,, excludin...   \n",
       "4            [Single, liveborn,, born, in, hospital]   \n",
       "\n",
       "                               y_l4_idx_300  \\\n",
       "0                                 [76, 345]   \n",
       "1                               [1019, 338]   \n",
       "2  [176, 739, 764, 780, 898, 887, 764, 357]   \n",
       "3           [16, 447, 764, 539, 1019, 1019]   \n",
       "4                 [235, 671, 365, 604, 578]   \n",
       "\n",
       "                                         y_4_emb_300  \\\n",
       "0  [[-0.076981634, 0.06613417, 0.029547844, 0.023...   \n",
       "1  [[-0.010246767, -0.01102714, -0.030377874, -0....   \n",
       "2  [[-0.102827616, 0.046675693, 0.050170206, -0.0...   \n",
       "3  [[-0.070184216, 0.047474142, 0.03269997, -0.04...   \n",
       "4  [[-0.039175306, 0.031033965, 0.039253235, -0.0...   \n",
       "\n",
       "                                        mean_y_1_emb  \\\n",
       "0  [-0.047526598, 0.05695837, 0.046640683, 0.0844...   \n",
       "1  [0.053493697, -0.041262418, 0.0017242441, 0.07...   \n",
       "2  [-0.1599842, 0.16103004, -0.13086131, -0.04455...   \n",
       "3  [0.007700827, -0.013504177, 0.045051254, 0.061...   \n",
       "4  [0.12333105, 0.00020353997, 0.0308367, 0.11656...   \n",
       "\n",
       "                                        mean_y_2_emb  \\\n",
       "0  [-0.091704376, 0.101866506, 0.0010930622, -0.0...   \n",
       "1  [-0.030272882, 0.10090023, 0.043690603, -0.035...   \n",
       "2  [0.07540788, 0.07400739, 0.008412799, 0.076379...   \n",
       "3  [0.056160342, 0.07937857, -0.0058114543, 0.010...   \n",
       "4  [-0.048329808, 0.08315235, 0.008233227, 0.0701...   \n",
       "\n",
       "                                        mean_y_3_emb  \\\n",
       "0  [-0.14592369, 0.11662416, -0.02554985, 0.08584...   \n",
       "1  [0.041243035, -0.061357144, 0.016727474, -0.08...   \n",
       "2  [-0.04314007, 0.11763805, -0.01063144, -0.0123...   \n",
       "3  [-0.07950522, 0.033617858, -0.022739127, 0.059...   \n",
       "4  [0.0685036, 0.10682133, -0.005800104, 0.083534...   \n",
       "\n",
       "                                        mean_y_4_emb  \n",
       "0  [0.07375743, 0.0807147, 0.03924676, -0.0451159...  \n",
       "1  [0.114566006, 0.08965349, 0.028740734, 0.08113...  \n",
       "2  [-0.04676784, -0.01674367, 0.10057579, -0.0128...  \n",
       "3  [0.08234508, 0.06837807, 0.047677133, 0.039281...  \n",
       "4  [0.07594408, 0.15636308, -0.036152676, 0.06416...  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up valid and test\n",
    "valid_and_test_buckets = valid_buckets + test_buckets\n",
    "\n",
    "for b in valid_and_test_buckets: \n",
    "    t = pad_sequences([torch.tensor(x) for x in b['idx_tokens']], padding='post')\n",
    "    b['pad_idx'] = t.tolist()\n",
    "    b['emb_tokens'] = b['pad_idx'].map(lambda t: [np.array(embed.weight.data[w]) for w in t])\n",
    "    b['gru_emb_tokens'] = b['emb_tokens'].map(lambda t: bi_grus(torch.tensor(t))[0].detach().numpy())\n",
    "\n",
    "\n",
    "test_bucket1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2f0b16",
   "metadata": {},
   "source": [
    "## prep data 2 get mean of gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f8c32a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>HOPI</th>\n",
       "      <th>SENT_TOKENS</th>\n",
       "      <th>SENT_TOKENS_COMBO</th>\n",
       "      <th>idx_tokens</th>\n",
       "      <th>trunc_len</th>\n",
       "      <th>trunc_idx</th>\n",
       "      <th>...</th>\n",
       "      <th>y_l3_idx_300</th>\n",
       "      <th>y_3_emb_300</th>\n",
       "      <th>y_l4_tokens_300</th>\n",
       "      <th>y_l4_idx_300</th>\n",
       "      <th>y_4_emb_300</th>\n",
       "      <th>mean_y_1_emb</th>\n",
       "      <th>mean_y_2_emb</th>\n",
       "      <th>mean_y_3_emb</th>\n",
       "      <th>mean_y_4_emb</th>\n",
       "      <th>pad_y_1_emb_300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>172162</td>\n",
       "      <td>[99811, 2851, 5990, 56982, 42832, 5781, 42731,...</td>\n",
       "      <td>43529</td>\n",
       "      <td>Admission Date:  [**2115-9-20**]              ...</td>\n",
       "      <td>simvastatin coumadin B</td>\n",
       "      <td>[b, coumadin, simvastatin]</td>\n",
       "      <td>b coumadin simvastatin</td>\n",
       "      <td>[3034, 5613, 18335]</td>\n",
       "      <td>3</td>\n",
       "      <td>[599, 1684, 5846]</td>\n",
       "      <td>...</td>\n",
       "      <td>[118, 260, 468, 510, 99]</td>\n",
       "      <td>[[-0.070492014, 0.08166887, -0.08236186, 0.019...</td>\n",
       "      <td>[Hemorrhage, or, hematoma, complicating, a, pr...</td>\n",
       "      <td>[135, 771, 567, 420, 276, 836]</td>\n",
       "      <td>[[-0.07630981, -0.01891837, 0.029481728, 0.067...</td>\n",
       "      <td>[0.08410206, -0.08915926, -0.007887115, 0.0842...</td>\n",
       "      <td>[0.049000613, 0.124341995, 0.0381095, 0.133280...</td>\n",
       "      <td>[-0.10026093, 0.07895817, 0.041625906, 0.08159...</td>\n",
       "      <td>[0.0936941, 0.18062985, 0.08456796, 0.05860461...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121167</td>\n",
       "      <td>[41071, 41401, 4280, 4289, 4168, 42731, 25040,...</td>\n",
       "      <td>11161</td>\n",
       "      <td>Admission Date:  [**2119-3-8**]     Discharge ...</td>\n",
       "      <td>M D Dictated</td>\n",
       "      <td>[d, dictated, m]</td>\n",
       "      <td>d dictated m</td>\n",
       "      <td>[5954, 6531, 12240]</td>\n",
       "      <td>3</td>\n",
       "      <td>[487, 7132, 525]</td>\n",
       "      <td>...</td>\n",
       "      <td>[6, 449, 367]</td>\n",
       "      <td>[[0.017918952, -0.026299225, -0.0011831599, -0...</td>\n",
       "      <td>[Acute, myocardial, infarction,, subendocardia...</td>\n",
       "      <td>[17, 731, 609, 917, 608]</td>\n",
       "      <td>[[-0.06275281, 0.034771625, -0.00079423486, 0....</td>\n",
       "      <td>[-0.047526598, 0.05695837, 0.046640683, 0.0844...</td>\n",
       "      <td>[-0.091704376, 0.101866506, 0.0010930622, -0.0...</td>\n",
       "      <td>[0.04243465, 0.096725516, -0.020863669, -0.000...</td>\n",
       "      <td>[-0.17148994, 0.038766213, 0.08449237, 0.10921...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>185502</td>\n",
       "      <td>[1572, 1962]</td>\n",
       "      <td>9403</td>\n",
       "      <td>Admission Date:  [**2127-9-4**]       Discharg...</td>\n",
       "      <td>Distal pancreatic mass PHYSICAL</td>\n",
       "      <td>[distal, mass, pancreatic, physical]</td>\n",
       "      <td>distal mass pancreatic physical</td>\n",
       "      <td>[6799, 12436, 14640, 15230]</td>\n",
       "      <td>4</td>\n",
       "      <td>[1092, 156, 1911, 190]</td>\n",
       "      <td>...</td>\n",
       "      <td>[93, 453, 468, 480]</td>\n",
       "      <td>[[-0.032632485, 0.0983419, -0.13715059, -0.056...</td>\n",
       "      <td>[Malignant, neoplasm, of, tail, of, pancreas]</td>\n",
       "      <td>[176, 739, 764, 932, 764, 785]</td>\n",
       "      <td>[[-0.102827616, 0.046675693, 0.050170206, -0.0...</td>\n",
       "      <td>[-0.1599842, 0.16103004, -0.13086131, -0.04455...</td>\n",
       "      <td>[0.07937019, 0.08743847, 0.0049281376, 0.05704...</td>\n",
       "      <td>[-0.03318695, 0.10974249, -0.000789281, 0.0044...</td>\n",
       "      <td>[-0.010925144, 0.00046655908, 0.11926437, -0.0...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>135752</td>\n",
       "      <td>[53551, 2851, 42731, 53290, 04186, 4019, V1046]</td>\n",
       "      <td>10861</td>\n",
       "      <td>Admission Date:  [**2143-7-6**]       Discharg...</td>\n",
       "      <td>Upper gastrointestinal bleed PHYSICAL</td>\n",
       "      <td>[bleed, gastrointestinal, physical, upper]</td>\n",
       "      <td>bleed gastrointestinal physical upper</td>\n",
       "      <td>[3513, 9025, 15230, 20854]</td>\n",
       "      <td>4</td>\n",
       "      <td>[1007, 828, 190, 792]</td>\n",
       "      <td>...</td>\n",
       "      <td>[61, 185, 291]</td>\n",
       "      <td>[[0.05790018, -0.04345593, -0.0029073094, 0.08...</td>\n",
       "      <td>[Unspecified, gastritis, and, gastroduodenitis]</td>\n",
       "      <td>[263, 548, 304, 549]</td>\n",
       "      <td>[[0.050144576, 0.042814985, -0.06284214, -0.10...</td>\n",
       "      <td>[-0.049221464, 0.05471442, 0.04873963, 0.08467...</td>\n",
       "      <td>[-0.02281745, 0.14163129, -0.012033534, 0.0138...</td>\n",
       "      <td>[0.03242548, -0.054779034, -0.12693162, -0.067...</td>\n",
       "      <td>[0.069495015, 0.12684956, 0.029800836, 0.00118...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110176</td>\n",
       "      <td>[53140, 2851, 25000, 2449, 4240, 2720, V113]</td>\n",
       "      <td>17540</td>\n",
       "      <td>Admission Date:  [**2157-7-21**]     Discharge...</td>\n",
       "      <td>Upper gastrointestinal bleed PHYSICAL</td>\n",
       "      <td>[bleed, gastrointestinal, physical, upper]</td>\n",
       "      <td>bleed gastrointestinal physical upper</td>\n",
       "      <td>[3513, 9025, 15230, 20854]</td>\n",
       "      <td>4</td>\n",
       "      <td>[1007, 828, 190, 792]</td>\n",
       "      <td>...</td>\n",
       "      <td>[60, 601]</td>\n",
       "      <td>[[0.018469978, 0.017918635, -0.009278733, 0.00...</td>\n",
       "      <td>[Chronic, or, unspecified, gastric, ulcer, wit...</td>\n",
       "      <td>[64, 771, 980, 547, 974, 1014, 569]</td>\n",
       "      <td>[[-0.09219703, 0.011353552, 0.1069955, 0.02280...</td>\n",
       "      <td>[-0.049221464, 0.05471442, 0.04873963, 0.08467...</td>\n",
       "      <td>[-0.02281745, 0.14163129, -0.012033534, 0.0138...</td>\n",
       "      <td>[-0.07016368, 0.10291995, -0.0067881383, 0.037...</td>\n",
       "      <td>[-0.01074038, 0.08132404, -0.021112952, 0.0939...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                          ICD9_CODE  SUBJECT_ID  \\\n",
       "0   172162  [99811, 2851, 5990, 56982, 42832, 5781, 42731,...       43529   \n",
       "1   121167  [41071, 41401, 4280, 4289, 4168, 42731, 25040,...       11161   \n",
       "2   185502                                       [1572, 1962]        9403   \n",
       "3   135752    [53551, 2851, 42731, 53290, 04186, 4019, V1046]       10861   \n",
       "4   110176       [53140, 2851, 25000, 2449, 4240, 2720, V113]       17540   \n",
       "\n",
       "                                                TEXT  \\\n",
       "0  Admission Date:  [**2115-9-20**]              ...   \n",
       "1  Admission Date:  [**2119-3-8**]     Discharge ...   \n",
       "2  Admission Date:  [**2127-9-4**]       Discharg...   \n",
       "3  Admission Date:  [**2143-7-6**]       Discharg...   \n",
       "4  Admission Date:  [**2157-7-21**]     Discharge...   \n",
       "\n",
       "                                     HOPI  \\\n",
       "0                  simvastatin coumadin B   \n",
       "1                            M D Dictated   \n",
       "2         Distal pancreatic mass PHYSICAL   \n",
       "3   Upper gastrointestinal bleed PHYSICAL   \n",
       "4   Upper gastrointestinal bleed PHYSICAL   \n",
       "\n",
       "                                  SENT_TOKENS  \\\n",
       "0                  [b, coumadin, simvastatin]   \n",
       "1                            [d, dictated, m]   \n",
       "2        [distal, mass, pancreatic, physical]   \n",
       "3  [bleed, gastrointestinal, physical, upper]   \n",
       "4  [bleed, gastrointestinal, physical, upper]   \n",
       "\n",
       "                       SENT_TOKENS_COMBO                   idx_tokens  \\\n",
       "0                 b coumadin simvastatin          [3034, 5613, 18335]   \n",
       "1                           d dictated m          [5954, 6531, 12240]   \n",
       "2        distal mass pancreatic physical  [6799, 12436, 14640, 15230]   \n",
       "3  bleed gastrointestinal physical upper   [3513, 9025, 15230, 20854]   \n",
       "4  bleed gastrointestinal physical upper   [3513, 9025, 15230, 20854]   \n",
       "\n",
       "   trunc_len               trunc_idx  ...              y_l3_idx_300  \\\n",
       "0          3       [599, 1684, 5846]  ...  [118, 260, 468, 510, 99]   \n",
       "1          3        [487, 7132, 525]  ...             [6, 449, 367]   \n",
       "2          4  [1092, 156, 1911, 190]  ...       [93, 453, 468, 480]   \n",
       "3          4   [1007, 828, 190, 792]  ...            [61, 185, 291]   \n",
       "4          4   [1007, 828, 190, 792]  ...                 [60, 601]   \n",
       "\n",
       "                                         y_3_emb_300  \\\n",
       "0  [[-0.070492014, 0.08166887, -0.08236186, 0.019...   \n",
       "1  [[0.017918952, -0.026299225, -0.0011831599, -0...   \n",
       "2  [[-0.032632485, 0.0983419, -0.13715059, -0.056...   \n",
       "3  [[0.05790018, -0.04345593, -0.0029073094, 0.08...   \n",
       "4  [[0.018469978, 0.017918635, -0.009278733, 0.00...   \n",
       "\n",
       "                                     y_l4_tokens_300  \\\n",
       "0  [Hemorrhage, or, hematoma, complicating, a, pr...   \n",
       "1  [Acute, myocardial, infarction,, subendocardia...   \n",
       "2      [Malignant, neoplasm, of, tail, of, pancreas]   \n",
       "3    [Unspecified, gastritis, and, gastroduodenitis]   \n",
       "4  [Chronic, or, unspecified, gastric, ulcer, wit...   \n",
       "\n",
       "                          y_l4_idx_300  \\\n",
       "0       [135, 771, 567, 420, 276, 836]   \n",
       "1             [17, 731, 609, 917, 608]   \n",
       "2       [176, 739, 764, 932, 764, 785]   \n",
       "3                 [263, 548, 304, 549]   \n",
       "4  [64, 771, 980, 547, 974, 1014, 569]   \n",
       "\n",
       "                                         y_4_emb_300  \\\n",
       "0  [[-0.07630981, -0.01891837, 0.029481728, 0.067...   \n",
       "1  [[-0.06275281, 0.034771625, -0.00079423486, 0....   \n",
       "2  [[-0.102827616, 0.046675693, 0.050170206, -0.0...   \n",
       "3  [[0.050144576, 0.042814985, -0.06284214, -0.10...   \n",
       "4  [[-0.09219703, 0.011353552, 0.1069955, 0.02280...   \n",
       "\n",
       "                                        mean_y_1_emb  \\\n",
       "0  [0.08410206, -0.08915926, -0.007887115, 0.0842...   \n",
       "1  [-0.047526598, 0.05695837, 0.046640683, 0.0844...   \n",
       "2  [-0.1599842, 0.16103004, -0.13086131, -0.04455...   \n",
       "3  [-0.049221464, 0.05471442, 0.04873963, 0.08467...   \n",
       "4  [-0.049221464, 0.05471442, 0.04873963, 0.08467...   \n",
       "\n",
       "                                        mean_y_2_emb  \\\n",
       "0  [0.049000613, 0.124341995, 0.0381095, 0.133280...   \n",
       "1  [-0.091704376, 0.101866506, 0.0010930622, -0.0...   \n",
       "2  [0.07937019, 0.08743847, 0.0049281376, 0.05704...   \n",
       "3  [-0.02281745, 0.14163129, -0.012033534, 0.0138...   \n",
       "4  [-0.02281745, 0.14163129, -0.012033534, 0.0138...   \n",
       "\n",
       "                                        mean_y_3_emb  \\\n",
       "0  [-0.10026093, 0.07895817, 0.041625906, 0.08159...   \n",
       "1  [0.04243465, 0.096725516, -0.020863669, -0.000...   \n",
       "2  [-0.03318695, 0.10974249, -0.000789281, 0.0044...   \n",
       "3  [0.03242548, -0.054779034, -0.12693162, -0.067...   \n",
       "4  [-0.07016368, 0.10291995, -0.0067881383, 0.037...   \n",
       "\n",
       "                                        mean_y_4_emb  \\\n",
       "0  [0.0936941, 0.18062985, 0.08456796, 0.05860461...   \n",
       "1  [-0.17148994, 0.038766213, 0.08449237, 0.10921...   \n",
       "2  [-0.010925144, 0.00046655908, 0.11926437, -0.0...   \n",
       "3  [0.069495015, 0.12684956, 0.029800836, 0.00118...   \n",
       "4  [-0.01074038, 0.08132404, -0.021112952, 0.0939...   \n",
       "\n",
       "                                     pad_y_1_emb_300  \n",
       "0  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "1  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "3  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "4  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preping adding mean gru of doc \n",
    "for b in buckets: \n",
    "    b['mean_gru_emb_tokens'] = b['gru_emb_tokens'].map(lambda t: np.average(t, axis = 0))\n",
    "    # print(b['mean_gru_emb_tokens'][0])\n",
    "\n",
    "\n",
    "trn_bucket1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4d0b1c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset train test and valid with the combined\n",
    "train = pd.concat(train_buckets, ignore_index=True)\n",
    "valid = pd.concat(valid_buckets, ignore_index=True)\n",
    "test = pd.concat(test_buckets, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373daa93",
   "metadata": {},
   "source": [
    "# Prep the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0d5825c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>HOPI</th>\n",
       "      <th>SENT_TOKENS</th>\n",
       "      <th>SENT_TOKENS_COMBO</th>\n",
       "      <th>idx_tokens</th>\n",
       "      <th>trunc_len</th>\n",
       "      <th>trunc_idx</th>\n",
       "      <th>...</th>\n",
       "      <th>pad_y_3_emb_300</th>\n",
       "      <th>y_3_pre_gru_emb_tokens</th>\n",
       "      <th>mean_gru_emb_tokens_y_3</th>\n",
       "      <th>pad_y_4_emb_300</th>\n",
       "      <th>y_4_pre_gru_emb_tokens</th>\n",
       "      <th>mean_gru_emb_tokens_y_4</th>\n",
       "      <th>pad_y_1_emb_100</th>\n",
       "      <th>pad_y_2_emb_100</th>\n",
       "      <th>pad_y_3_emb_100</th>\n",
       "      <th>pad_y_4_emb_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>172162</td>\n",
       "      <td>[99811, 2851, 5990, 56982, 42832, 5781, 42731,...</td>\n",
       "      <td>43529</td>\n",
       "      <td>Admission Date:  [**2115-9-20**]              ...</td>\n",
       "      <td>simvastatin coumadin B</td>\n",
       "      <td>[b, coumadin, simvastatin]</td>\n",
       "      <td>b coumadin simvastatin</td>\n",
       "      <td>[3034, 5613, 18335]</td>\n",
       "      <td>3</td>\n",
       "      <td>[599, 1684, 5846]</td>\n",
       "      <td>...</td>\n",
       "      <td>[118, 260, 468, 510, 99, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[-0.070492014, 0.08166887, -0.08236186, 0.019...</td>\n",
       "      <td>[[0.0322607, 0.044095006, 0.012705359, -0.0126...</td>\n",
       "      <td>[135, 771, 567, 420, 276, 836, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[-0.07630981, -0.01891837, 0.029481728, 0.067...</td>\n",
       "      <td>[[-0.010776768, 0.05216178, -0.053062614, 0.07...</td>\n",
       "      <td>[30, 1, 44, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[57, 181, 245, 16, 152, 39, 178, 83, 54, 0, 0,...</td>\n",
       "      <td>[118, 260, 468, 510, 99, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[135, 771, 567, 420, 276, 836, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121167</td>\n",
       "      <td>[41071, 41401, 4280, 4289, 4168, 42731, 25040,...</td>\n",
       "      <td>11161</td>\n",
       "      <td>Admission Date:  [**2119-3-8**]     Discharge ...</td>\n",
       "      <td>M D Dictated</td>\n",
       "      <td>[d, dictated, m]</td>\n",
       "      <td>d dictated m</td>\n",
       "      <td>[5954, 6531, 12240]</td>\n",
       "      <td>3</td>\n",
       "      <td>[487, 7132, 525]</td>\n",
       "      <td>...</td>\n",
       "      <td>[6, 449, 367, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0.017918952, -0.026299225, -0.0011831599, -0...</td>\n",
       "      <td>[[0.043424934, 0.037664473, 0.024131961, 0.040...</td>\n",
       "      <td>[17, 731, 609, 917, 608, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[-0.06275281, 0.034771625, -0.00079423486, 0....</td>\n",
       "      <td>[[0.059792526, 0.05119934, -0.001219137, 0.028...</td>\n",
       "      <td>[17, 38, 57, 9, 56, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[139, 111, 71, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[6, 449, 367, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[17, 731, 609, 917, 608, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>185502</td>\n",
       "      <td>[1572, 1962]</td>\n",
       "      <td>9403</td>\n",
       "      <td>Admission Date:  [**2127-9-4**]       Discharg...</td>\n",
       "      <td>Distal pancreatic mass PHYSICAL</td>\n",
       "      <td>[distal, mass, pancreatic, physical]</td>\n",
       "      <td>distal mass pancreatic physical</td>\n",
       "      <td>[6799, 12436, 14640, 15230]</td>\n",
       "      <td>4</td>\n",
       "      <td>[1092, 156, 1911, 190]</td>\n",
       "      <td>...</td>\n",
       "      <td>[93, 453, 468, 480, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[-0.032632485, 0.0983419, -0.13715059, -0.056...</td>\n",
       "      <td>[[-2.3140368e-05, 0.03251061, 0.032071225, 0.0...</td>\n",
       "      <td>[176, 739, 764, 932, 764, 785, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[-0.102827616, 0.046675693, 0.050170206, -0.0...</td>\n",
       "      <td>[[0.033576686, 0.069294505, -0.06897034, 0.046...</td>\n",
       "      <td>[34, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[151, 164, 181, 70, 186, 16, 201, 0, 0, 0, 0, ...</td>\n",
       "      <td>[93, 453, 468, 480, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[176, 739, 764, 932, 764, 785, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>135752</td>\n",
       "      <td>[53551, 2851, 42731, 53290, 04186, 4019, V1046]</td>\n",
       "      <td>10861</td>\n",
       "      <td>Admission Date:  [**2143-7-6**]       Discharg...</td>\n",
       "      <td>Upper gastrointestinal bleed PHYSICAL</td>\n",
       "      <td>[bleed, gastrointestinal, physical, upper]</td>\n",
       "      <td>bleed gastrointestinal physical upper</td>\n",
       "      <td>[3513, 9025, 15230, 20854]</td>\n",
       "      <td>4</td>\n",
       "      <td>[1007, 828, 190, 792]</td>\n",
       "      <td>...</td>\n",
       "      <td>[61, 185, 291, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[[0.05790018, -0.04345593, -0.0029073094, 0.08...</td>\n",
       "      <td>[[0.004676984, 0.016346907, -0.036972657, 0.03...</td>\n",
       "      <td>[263, 548, 304, 549, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[[0.050144576, 0.042814985, -0.06284214, -0.10...</td>\n",
       "      <td>[[-0.014313143, 0.03751046, 0.025673231, 0.034...</td>\n",
       "      <td>[17, 38, 57, 16, 56, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[72, 181, 89, 240, 16, 79, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[61, 185, 291, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[263, 548, 304, 549, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110176</td>\n",
       "      <td>[53140, 2851, 25000, 2449, 4240, 2720, V113]</td>\n",
       "      <td>17540</td>\n",
       "      <td>Admission Date:  [**2157-7-21**]     Discharge...</td>\n",
       "      <td>Upper gastrointestinal bleed PHYSICAL</td>\n",
       "      <td>[bleed, gastrointestinal, physical, upper]</td>\n",
       "      <td>bleed gastrointestinal physical upper</td>\n",
       "      <td>[3513, 9025, 15230, 20854]</td>\n",
       "      <td>4</td>\n",
       "      <td>[1007, 828, 190, 792]</td>\n",
       "      <td>...</td>\n",
       "      <td>[60, 601, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0.018469978, 0.017918635, -0.009278733, 0.00...</td>\n",
       "      <td>[[0.0155503135, 0.035892613, -0.07867216, 0.04...</td>\n",
       "      <td>[64, 771, 980, 547, 974, 1014, 569, 0, 0, 0, 0...</td>\n",
       "      <td>[[-0.09219703, 0.011353552, 0.1069955, 0.02280...</td>\n",
       "      <td>[[0.02496347, 0.033883367, -0.03861289, 0.0359...</td>\n",
       "      <td>[17, 38, 57, 16, 56, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[72, 181, 89, 240, 16, 79, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[60, 601, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[64, 771, 980, 547, 974, 1014, 569, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                          ICD9_CODE  SUBJECT_ID  \\\n",
       "0   172162  [99811, 2851, 5990, 56982, 42832, 5781, 42731,...       43529   \n",
       "1   121167  [41071, 41401, 4280, 4289, 4168, 42731, 25040,...       11161   \n",
       "2   185502                                       [1572, 1962]        9403   \n",
       "3   135752    [53551, 2851, 42731, 53290, 04186, 4019, V1046]       10861   \n",
       "4   110176       [53140, 2851, 25000, 2449, 4240, 2720, V113]       17540   \n",
       "\n",
       "                                                TEXT  \\\n",
       "0  Admission Date:  [**2115-9-20**]              ...   \n",
       "1  Admission Date:  [**2119-3-8**]     Discharge ...   \n",
       "2  Admission Date:  [**2127-9-4**]       Discharg...   \n",
       "3  Admission Date:  [**2143-7-6**]       Discharg...   \n",
       "4  Admission Date:  [**2157-7-21**]     Discharge...   \n",
       "\n",
       "                                     HOPI  \\\n",
       "0                  simvastatin coumadin B   \n",
       "1                            M D Dictated   \n",
       "2         Distal pancreatic mass PHYSICAL   \n",
       "3   Upper gastrointestinal bleed PHYSICAL   \n",
       "4   Upper gastrointestinal bleed PHYSICAL   \n",
       "\n",
       "                                  SENT_TOKENS  \\\n",
       "0                  [b, coumadin, simvastatin]   \n",
       "1                            [d, dictated, m]   \n",
       "2        [distal, mass, pancreatic, physical]   \n",
       "3  [bleed, gastrointestinal, physical, upper]   \n",
       "4  [bleed, gastrointestinal, physical, upper]   \n",
       "\n",
       "                       SENT_TOKENS_COMBO                   idx_tokens  \\\n",
       "0                 b coumadin simvastatin          [3034, 5613, 18335]   \n",
       "1                           d dictated m          [5954, 6531, 12240]   \n",
       "2        distal mass pancreatic physical  [6799, 12436, 14640, 15230]   \n",
       "3  bleed gastrointestinal physical upper   [3513, 9025, 15230, 20854]   \n",
       "4  bleed gastrointestinal physical upper   [3513, 9025, 15230, 20854]   \n",
       "\n",
       "   trunc_len               trunc_idx  ...  \\\n",
       "0          3       [599, 1684, 5846]  ...   \n",
       "1          3        [487, 7132, 525]  ...   \n",
       "2          4  [1092, 156, 1911, 190]  ...   \n",
       "3          4   [1007, 828, 190, 792]  ...   \n",
       "4          4   [1007, 828, 190, 792]  ...   \n",
       "\n",
       "                                     pad_y_3_emb_300  \\\n",
       "0  [118, 260, 468, 510, 99, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [6, 449, 367, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2  [93, 453, 468, 480, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "3  [61, 185, 291, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "4  [60, 601, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                              y_3_pre_gru_emb_tokens  \\\n",
       "0  [[-0.070492014, 0.08166887, -0.08236186, 0.019...   \n",
       "1  [[0.017918952, -0.026299225, -0.0011831599, -0...   \n",
       "2  [[-0.032632485, 0.0983419, -0.13715059, -0.056...   \n",
       "3  [[0.05790018, -0.04345593, -0.0029073094, 0.08...   \n",
       "4  [[0.018469978, 0.017918635, -0.009278733, 0.00...   \n",
       "\n",
       "                             mean_gru_emb_tokens_y_3  \\\n",
       "0  [[0.0322607, 0.044095006, 0.012705359, -0.0126...   \n",
       "1  [[0.043424934, 0.037664473, 0.024131961, 0.040...   \n",
       "2  [[-2.3140368e-05, 0.03251061, 0.032071225, 0.0...   \n",
       "3  [[0.004676984, 0.016346907, -0.036972657, 0.03...   \n",
       "4  [[0.0155503135, 0.035892613, -0.07867216, 0.04...   \n",
       "\n",
       "                                     pad_y_4_emb_300  \\\n",
       "0  [135, 771, 567, 420, 276, 836, 0, 0, 0, 0, 0, ...   \n",
       "1  [17, 731, 609, 917, 608, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [176, 739, 764, 932, 764, 785, 0, 0, 0, 0, 0, ...   \n",
       "3  [263, 548, 304, 549, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "4  [64, 771, 980, 547, 974, 1014, 569, 0, 0, 0, 0...   \n",
       "\n",
       "                              y_4_pre_gru_emb_tokens  \\\n",
       "0  [[-0.07630981, -0.01891837, 0.029481728, 0.067...   \n",
       "1  [[-0.06275281, 0.034771625, -0.00079423486, 0....   \n",
       "2  [[-0.102827616, 0.046675693, 0.050170206, -0.0...   \n",
       "3  [[0.050144576, 0.042814985, -0.06284214, -0.10...   \n",
       "4  [[-0.09219703, 0.011353552, 0.1069955, 0.02280...   \n",
       "\n",
       "                             mean_gru_emb_tokens_y_4  \\\n",
       "0  [[-0.010776768, 0.05216178, -0.053062614, 0.07...   \n",
       "1  [[0.059792526, 0.05119934, -0.001219137, 0.028...   \n",
       "2  [[0.033576686, 0.069294505, -0.06897034, 0.046...   \n",
       "3  [[-0.014313143, 0.03751046, 0.025673231, 0.034...   \n",
       "4  [[0.02496347, 0.033883367, -0.03861289, 0.0359...   \n",
       "\n",
       "                             pad_y_1_emb_100  \\\n",
       "0     [30, 1, 44, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1   [17, 38, 57, 9, 56, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2      [34, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "3  [17, 38, 57, 16, 56, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4  [17, 38, 57, 16, 56, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                     pad_y_2_emb_100  \\\n",
       "0  [57, 181, 245, 16, 152, 39, 178, 83, 54, 0, 0,...   \n",
       "1  [139, 111, 71, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "2  [151, 164, 181, 70, 186, 16, 201, 0, 0, 0, 0, ...   \n",
       "3  [72, 181, 89, 240, 16, 79, 0, 0, 0, 0, 0, 0, 0...   \n",
       "4  [72, 181, 89, 240, 16, 79, 0, 0, 0, 0, 0, 0, 0...   \n",
       "\n",
       "                                     pad_y_3_emb_100  \\\n",
       "0  [118, 260, 468, 510, 99, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [6, 449, 367, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2  [93, 453, 468, 480, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "3  [61, 185, 291, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "4  [60, 601, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                     pad_y_4_emb_100  \n",
       "0  [135, 771, 567, 420, 276, 836, 0, 0, 0, 0, 0, ...  \n",
       "1  [17, 731, 609, 917, 608, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [176, 739, 764, 932, 764, 785, 0, 0, 0, 0, 0, ...  \n",
       "3  [263, 548, 304, 549, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "4  [64, 771, 980, 547, 974, 1014, 569, 0, 0, 0, 0...  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean embed for labels\n",
    "\n",
    "levels = [1, 2, 3, 4]\n",
    "embed_sizes = [100]\n",
    "\n",
    "dict_emb = {\n",
    "    \"y_l1\": lvl1_embed,\n",
    "    \"y_l2\": lvl2_embed,\n",
    "    \"y_l3\": lvl3_embed,\n",
    "    \"y_l4\": lvl4_embed\n",
    "}\n",
    "\n",
    "# for level in levels:\n",
    "#     for embed_size in embed_sizes:\n",
    "#         embeddings = dict_emb.get(f'y_l{level}_{embed_size}')\n",
    "#         for b in [train, valid, test]:\n",
    "#             t = pad_sequences([torch.tensor(x) for x in b[f'y_l{level}_idx_{embed_size}']], padding='post')\n",
    "#             b[f'pad_y_{level}_emb_{embed_size}'] = t.tolist()\n",
    "#             b[f'y_{level}_pre_gru_emb_tokens'] = b[f'pad_y_{level}_emb_{embed_size}'].map(lambda t: [np.array(embeddings[w]) for w in t])\n",
    "\n",
    "for level in levels:\n",
    "    for embed_size in embed_sizes:\n",
    "        embeddings = dict_emb.get(f'y_l{level}')\n",
    "        for b in [train, valid, test]:\n",
    "            t = pad_sequences([torch.tensor(x) for x in b[f'y_l{level}_idx_{embed_size}']], padding='post')\n",
    "            b[f'pad_y_{level}_emb_{embed_size}'] = t.tolist()\n",
    "            b[f'mean_y_{level}_emb'] = b[f'pad_y_{level}_emb_{embed_size}'].map(lambda t: np.average([np.array(embeddings[w]) for w in t], axis = 0))\n",
    "        # train[f\"mean_y_{level}_emb\"] = train[f\"y_{level}_emb_{embed_size}\"].map(lambda t: np.average(t, axis = 0))\n",
    "        # valid[f\"mean_y_{level}_emb\"] = valid[f\"y_{level}_emb_{embed_size}\"].map(lambda t: np.average(t, axis = 0))\n",
    "        # test[f\"mean_y_{level}_emb\"] = test[f\"y_{level}_emb_{embed_size}\"].map(lambda t: np.average(t, axis = 0))\n",
    "\n",
    "train.head(5)       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3c8b0f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>HOPI</th>\n",
       "      <th>SENT_TOKENS</th>\n",
       "      <th>SENT_TOKENS_COMBO</th>\n",
       "      <th>idx_tokens</th>\n",
       "      <th>trunc_len</th>\n",
       "      <th>trunc_idx</th>\n",
       "      <th>...</th>\n",
       "      <th>pad_y_3_emb_300</th>\n",
       "      <th>y_3_pre_gru_emb_tokens</th>\n",
       "      <th>mean_gru_emb_tokens_y_3</th>\n",
       "      <th>pad_y_4_emb_300</th>\n",
       "      <th>y_4_pre_gru_emb_tokens</th>\n",
       "      <th>mean_gru_emb_tokens_y_4</th>\n",
       "      <th>pad_y_1_emb_100</th>\n",
       "      <th>pad_y_2_emb_100</th>\n",
       "      <th>pad_y_3_emb_100</th>\n",
       "      <th>pad_y_4_emb_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>172162</td>\n",
       "      <td>[99811, 2851, 5990, 56982, 42832, 5781, 42731,...</td>\n",
       "      <td>43529</td>\n",
       "      <td>Admission Date:  [**2115-9-20**]              ...</td>\n",
       "      <td>simvastatin coumadin B</td>\n",
       "      <td>[b, coumadin, simvastatin]</td>\n",
       "      <td>b coumadin simvastatin</td>\n",
       "      <td>[3034, 5613, 18335]</td>\n",
       "      <td>3</td>\n",
       "      <td>[599, 1684, 5846]</td>\n",
       "      <td>...</td>\n",
       "      <td>[118, 260, 468, 510, 99, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[-0.070492014, 0.08166887, -0.08236186, 0.019...</td>\n",
       "      <td>[-0.030569527, -0.062341407, -0.08312965, -0.0...</td>\n",
       "      <td>[135, 771, 567, 420, 276, 836, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[-0.07630981, -0.01891837, 0.029481728, 0.067...</td>\n",
       "      <td>[-0.016000582, -0.07141718, -0.07300361, -0.05...</td>\n",
       "      <td>[30, 1, 44, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[57, 181, 245, 16, 152, 39, 178, 83, 54, 0, 0,...</td>\n",
       "      <td>[118, 260, 468, 510, 99, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[135, 771, 567, 420, 276, 836, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121167</td>\n",
       "      <td>[41071, 41401, 4280, 4289, 4168, 42731, 25040,...</td>\n",
       "      <td>11161</td>\n",
       "      <td>Admission Date:  [**2119-3-8**]     Discharge ...</td>\n",
       "      <td>M D Dictated</td>\n",
       "      <td>[d, dictated, m]</td>\n",
       "      <td>d dictated m</td>\n",
       "      <td>[5954, 6531, 12240]</td>\n",
       "      <td>3</td>\n",
       "      <td>[487, 7132, 525]</td>\n",
       "      <td>...</td>\n",
       "      <td>[6, 449, 367, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0.017918952, -0.026299225, -0.0011831599, -0...</td>\n",
       "      <td>[-0.02851449, -0.05629733, -0.07191625, -0.055...</td>\n",
       "      <td>[17, 731, 609, 917, 608, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[-0.06275281, 0.034771625, -0.00079423486, 0....</td>\n",
       "      <td>[-0.02474647, -0.052335624, -0.07753406, -0.05...</td>\n",
       "      <td>[17, 38, 57, 9, 56, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[139, 111, 71, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[6, 449, 367, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[17, 731, 609, 917, 608, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>185502</td>\n",
       "      <td>[1572, 1962]</td>\n",
       "      <td>9403</td>\n",
       "      <td>Admission Date:  [**2127-9-4**]       Discharg...</td>\n",
       "      <td>Distal pancreatic mass PHYSICAL</td>\n",
       "      <td>[distal, mass, pancreatic, physical]</td>\n",
       "      <td>distal mass pancreatic physical</td>\n",
       "      <td>[6799, 12436, 14640, 15230]</td>\n",
       "      <td>4</td>\n",
       "      <td>[1092, 156, 1911, 190]</td>\n",
       "      <td>...</td>\n",
       "      <td>[93, 453, 468, 480, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[-0.032632485, 0.0983419, -0.13715059, -0.056...</td>\n",
       "      <td>[-0.022838058, -0.058503646, -0.07725458, -0.0...</td>\n",
       "      <td>[176, 739, 764, 932, 764, 785, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[-0.102827616, 0.046675693, 0.050170206, -0.0...</td>\n",
       "      <td>[-0.024991103, -0.044592947, -0.08717536, -0.0...</td>\n",
       "      <td>[34, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[151, 164, 181, 70, 186, 16, 201, 0, 0, 0, 0, ...</td>\n",
       "      <td>[93, 453, 468, 480, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[176, 739, 764, 932, 764, 785, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>135752</td>\n",
       "      <td>[53551, 2851, 42731, 53290, 04186, 4019, V1046]</td>\n",
       "      <td>10861</td>\n",
       "      <td>Admission Date:  [**2143-7-6**]       Discharg...</td>\n",
       "      <td>Upper gastrointestinal bleed PHYSICAL</td>\n",
       "      <td>[bleed, gastrointestinal, physical, upper]</td>\n",
       "      <td>bleed gastrointestinal physical upper</td>\n",
       "      <td>[3513, 9025, 15230, 20854]</td>\n",
       "      <td>4</td>\n",
       "      <td>[1007, 828, 190, 792]</td>\n",
       "      <td>...</td>\n",
       "      <td>[61, 185, 291, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[[0.05790018, -0.04345593, -0.0029073094, 0.08...</td>\n",
       "      <td>[-0.02298629, -0.052870058, -0.07068156, -0.05...</td>\n",
       "      <td>[263, 548, 304, 549, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[[0.050144576, 0.042814985, -0.06284214, -0.10...</td>\n",
       "      <td>[-0.022747597, -0.06391313, -0.07859696, -0.06...</td>\n",
       "      <td>[17, 38, 57, 16, 56, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[72, 181, 89, 240, 16, 79, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[61, 185, 291, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[263, 548, 304, 549, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110176</td>\n",
       "      <td>[53140, 2851, 25000, 2449, 4240, 2720, V113]</td>\n",
       "      <td>17540</td>\n",
       "      <td>Admission Date:  [**2157-7-21**]     Discharge...</td>\n",
       "      <td>Upper gastrointestinal bleed PHYSICAL</td>\n",
       "      <td>[bleed, gastrointestinal, physical, upper]</td>\n",
       "      <td>bleed gastrointestinal physical upper</td>\n",
       "      <td>[3513, 9025, 15230, 20854]</td>\n",
       "      <td>4</td>\n",
       "      <td>[1007, 828, 190, 792]</td>\n",
       "      <td>...</td>\n",
       "      <td>[60, 601, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0.018469978, 0.017918635, -0.009278733, 0.00...</td>\n",
       "      <td>[-0.029771583, -0.057078615, -0.07720239, -0.0...</td>\n",
       "      <td>[64, 771, 980, 547, 974, 1014, 569, 0, 0, 0, 0...</td>\n",
       "      <td>[[-0.09219703, 0.011353552, 0.1069955, 0.02280...</td>\n",
       "      <td>[-0.024302892, -0.06424128, -0.073939376, -0.0...</td>\n",
       "      <td>[17, 38, 57, 16, 56, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[72, 181, 89, 240, 16, 79, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[60, 601, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[64, 771, 980, 547, 974, 1014, 569, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                          ICD9_CODE  SUBJECT_ID  \\\n",
       "0   172162  [99811, 2851, 5990, 56982, 42832, 5781, 42731,...       43529   \n",
       "1   121167  [41071, 41401, 4280, 4289, 4168, 42731, 25040,...       11161   \n",
       "2   185502                                       [1572, 1962]        9403   \n",
       "3   135752    [53551, 2851, 42731, 53290, 04186, 4019, V1046]       10861   \n",
       "4   110176       [53140, 2851, 25000, 2449, 4240, 2720, V113]       17540   \n",
       "\n",
       "                                                TEXT  \\\n",
       "0  Admission Date:  [**2115-9-20**]              ...   \n",
       "1  Admission Date:  [**2119-3-8**]     Discharge ...   \n",
       "2  Admission Date:  [**2127-9-4**]       Discharg...   \n",
       "3  Admission Date:  [**2143-7-6**]       Discharg...   \n",
       "4  Admission Date:  [**2157-7-21**]     Discharge...   \n",
       "\n",
       "                                     HOPI  \\\n",
       "0                  simvastatin coumadin B   \n",
       "1                            M D Dictated   \n",
       "2         Distal pancreatic mass PHYSICAL   \n",
       "3   Upper gastrointestinal bleed PHYSICAL   \n",
       "4   Upper gastrointestinal bleed PHYSICAL   \n",
       "\n",
       "                                  SENT_TOKENS  \\\n",
       "0                  [b, coumadin, simvastatin]   \n",
       "1                            [d, dictated, m]   \n",
       "2        [distal, mass, pancreatic, physical]   \n",
       "3  [bleed, gastrointestinal, physical, upper]   \n",
       "4  [bleed, gastrointestinal, physical, upper]   \n",
       "\n",
       "                       SENT_TOKENS_COMBO                   idx_tokens  \\\n",
       "0                 b coumadin simvastatin          [3034, 5613, 18335]   \n",
       "1                           d dictated m          [5954, 6531, 12240]   \n",
       "2        distal mass pancreatic physical  [6799, 12436, 14640, 15230]   \n",
       "3  bleed gastrointestinal physical upper   [3513, 9025, 15230, 20854]   \n",
       "4  bleed gastrointestinal physical upper   [3513, 9025, 15230, 20854]   \n",
       "\n",
       "   trunc_len               trunc_idx  ...  \\\n",
       "0          3       [599, 1684, 5846]  ...   \n",
       "1          3        [487, 7132, 525]  ...   \n",
       "2          4  [1092, 156, 1911, 190]  ...   \n",
       "3          4   [1007, 828, 190, 792]  ...   \n",
       "4          4   [1007, 828, 190, 792]  ...   \n",
       "\n",
       "                                     pad_y_3_emb_300  \\\n",
       "0  [118, 260, 468, 510, 99, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [6, 449, 367, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2  [93, 453, 468, 480, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "3  [61, 185, 291, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "4  [60, 601, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                              y_3_pre_gru_emb_tokens  \\\n",
       "0  [[-0.070492014, 0.08166887, -0.08236186, 0.019...   \n",
       "1  [[0.017918952, -0.026299225, -0.0011831599, -0...   \n",
       "2  [[-0.032632485, 0.0983419, -0.13715059, -0.056...   \n",
       "3  [[0.05790018, -0.04345593, -0.0029073094, 0.08...   \n",
       "4  [[0.018469978, 0.017918635, -0.009278733, 0.00...   \n",
       "\n",
       "                             mean_gru_emb_tokens_y_3  \\\n",
       "0  [-0.030569527, -0.062341407, -0.08312965, -0.0...   \n",
       "1  [-0.02851449, -0.05629733, -0.07191625, -0.055...   \n",
       "2  [-0.022838058, -0.058503646, -0.07725458, -0.0...   \n",
       "3  [-0.02298629, -0.052870058, -0.07068156, -0.05...   \n",
       "4  [-0.029771583, -0.057078615, -0.07720239, -0.0...   \n",
       "\n",
       "                                     pad_y_4_emb_300  \\\n",
       "0  [135, 771, 567, 420, 276, 836, 0, 0, 0, 0, 0, ...   \n",
       "1  [17, 731, 609, 917, 608, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [176, 739, 764, 932, 764, 785, 0, 0, 0, 0, 0, ...   \n",
       "3  [263, 548, 304, 549, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "4  [64, 771, 980, 547, 974, 1014, 569, 0, 0, 0, 0...   \n",
       "\n",
       "                              y_4_pre_gru_emb_tokens  \\\n",
       "0  [[-0.07630981, -0.01891837, 0.029481728, 0.067...   \n",
       "1  [[-0.06275281, 0.034771625, -0.00079423486, 0....   \n",
       "2  [[-0.102827616, 0.046675693, 0.050170206, -0.0...   \n",
       "3  [[0.050144576, 0.042814985, -0.06284214, -0.10...   \n",
       "4  [[-0.09219703, 0.011353552, 0.1069955, 0.02280...   \n",
       "\n",
       "                             mean_gru_emb_tokens_y_4  \\\n",
       "0  [-0.016000582, -0.07141718, -0.07300361, -0.05...   \n",
       "1  [-0.02474647, -0.052335624, -0.07753406, -0.05...   \n",
       "2  [-0.024991103, -0.044592947, -0.08717536, -0.0...   \n",
       "3  [-0.022747597, -0.06391313, -0.07859696, -0.06...   \n",
       "4  [-0.024302892, -0.06424128, -0.073939376, -0.0...   \n",
       "\n",
       "                             pad_y_1_emb_100  \\\n",
       "0     [30, 1, 44, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1   [17, 38, 57, 9, 56, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2      [34, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "3  [17, 38, 57, 16, 56, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4  [17, 38, 57, 16, 56, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                     pad_y_2_emb_100  \\\n",
       "0  [57, 181, 245, 16, 152, 39, 178, 83, 54, 0, 0,...   \n",
       "1  [139, 111, 71, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "2  [151, 164, 181, 70, 186, 16, 201, 0, 0, 0, 0, ...   \n",
       "3  [72, 181, 89, 240, 16, 79, 0, 0, 0, 0, 0, 0, 0...   \n",
       "4  [72, 181, 89, 240, 16, 79, 0, 0, 0, 0, 0, 0, 0...   \n",
       "\n",
       "                                     pad_y_3_emb_100  \\\n",
       "0  [118, 260, 468, 510, 99, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [6, 449, 367, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2  [93, 453, 468, 480, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "3  [61, 185, 291, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "4  [60, 601, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                     pad_y_4_emb_100  \n",
       "0  [135, 771, 567, 420, 276, 836, 0, 0, 0, 0, 0, ...  \n",
       "1  [17, 731, 609, 917, 608, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [176, 739, 764, 932, 764, 785, 0, 0, 0, 0, 0, ...  \n",
       "3  [263, 548, 304, 549, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "4  [64, 771, 980, 547, 974, 1014, 569, 0, 0, 0, 0...  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRU embed for labels\n",
    "\n",
    "dict_emb = {\n",
    "    \"y_l1_300\": lvl1_embed_300,\n",
    "    \"y_l2_300\": lvl2_embed_300,\n",
    "    \"y_l3_300\": lvl3_embed_300,\n",
    "    \"y_l4_300\": lvl4_embed_300\n",
    "}\n",
    "\n",
    "gru = torch.nn.GRU(input_size=300, hidden_size=100, num_layers=1, dropout=.3, batch_first=False, bidirectional=False)\n",
    "embed_sizes = [300]\n",
    "for level in levels:\n",
    "    for embed_size in embed_sizes:\n",
    "        embeddings = dict_emb.get(f'y_l{level}_{embed_size}')\n",
    "        for b in [train, valid, test]:\n",
    "            t = pad_sequences([torch.tensor(x) for x in b[f'y_l{level}_idx_{embed_size}']], padding='post')\n",
    "            b[f'pad_y_{level}_emb_{embed_size}'] = t.tolist()\n",
    "            b[f'y_{level}_pre_gru_emb_tokens'] = b[f'pad_y_{level}_emb_{embed_size}'].map(lambda t: [np.array(embeddings[w]) for w in t])\n",
    "            b[f'mean_gru_emb_tokens_y_{level}'] = b[f'y_{level}_pre_gru_emb_tokens'].map(lambda t: np.average(gru(torch.tensor(t))[0].detach().numpy(), axis=0)) \n",
    "\n",
    "\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ea357193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick pickles\n",
    "training_pickle_file = './model/training.pkl'\n",
    "valid_pickle_file = './model/valid.pkl'\n",
    "test_pickle_file = './model/test.pkl'\n",
    "\n",
    "train.to_pickle(training_pickle_file)\n",
    "valid.to_pickle(valid_pickle_file)\n",
    "test.to_pickle(test_pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7231d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pickles\n",
    "training_pickle_file = './model/training.pkl'\n",
    "valid_pickle_file = './model/valid.pkl'\n",
    "test_pickle_file = './model/test.pkl'\n",
    "\n",
    "train = pd.read_pickle(training_pickle_file)\n",
    "valid = pd.read_pickle(valid_pickle_file)\n",
    "test = pd.read_pickle(test_pickle_file)\n",
    "\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1a127f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for gru models\n",
    "\n",
    "y_l1_dict = dict(zip(np.unique(train['y_l1'].values), np.arange(0, len(np.unique(train['y_l1'].values)), 1)))\n",
    "y_l1_trn = train['y_l1'].map(lambda x: y_l1_dict.get(x))\n",
    "y_l1_val = valid['y_l1'].map(lambda x: y_l1_dict.get(x))\n",
    "y_l1_tst = test['y_l1'].map(lambda x: y_l1_dict.get(x))\n",
    "\n",
    "y_l2_dict = dict(zip(np.unique(train['y_l2'].values), np.arange(0, len(np.unique(train['y_l2'].values)), 1)))\n",
    "y_l2_trn = train['y_l2'].map(lambda x: y_l2_dict.get(x)).fillna('Missing')\n",
    "y_l2_val = valid['y_l2'].map(lambda x: y_l2_dict.get(x)).fillna('Missing')\n",
    "y_l2_tst = test['y_l2'].map(lambda x: y_l2_dict.get(x)).fillna('Missing')\n",
    "\n",
    "# Only include lvl2 labels that are in the training set\n",
    "tst_miss = [y_l2_tst != \"Missing\"]\n",
    "tst_idx_2 = np.where(y_l2_tst != 'Missing')\n",
    "y_l2_tst = np.array(y_l2_tst[y_l2_tst != \"Missing\"]).astype(int)\n",
    "\n",
    "val_miss = [y_l2_val != \"Missing\"]\n",
    "val_idx_2 = np.where(y_l2_val != 'Missing')\n",
    "y_l2_val = np.array(y_l2_val[y_l2_val != \"Missing\"]).astype(int)\n",
    "\n",
    "\n",
    "train_miss = train['y_l3'].isna()\n",
    "train_l3 = train[~train_miss]\n",
    "y_l3_dict = dict(zip(set(train_l3['y_l3'].values), np.arange(0, len(set(train_l3['y_l3'].values)), 1)))\n",
    "\n",
    "y_l3_trn = train_l3['y_l3'].map(lambda x: y_l3_dict.get(x)).fillna('Missing')\n",
    "y_l3_val = valid['y_l3'].map(lambda x: y_l3_dict.get(x)).fillna('Missing')\n",
    "y_l3_tst = test['y_l3'].map(lambda x: y_l3_dict.get(x)).fillna('Missing')\n",
    "\n",
    "# Only include lvl2 labels that are in the training set\n",
    "tst_miss = [y_l3_tst != \"Missing\"]\n",
    "tst_idx_3 = np.where(y_l3_tst != 'Missing')\n",
    "y_l3_tst = np.array(y_l3_tst[y_l3_tst != \"Missing\"]).astype(int)\n",
    "\n",
    "val_miss = [y_l3_val != \"Missing\"]\n",
    "val_idx_3 = np.where(y_l3_val != 'Missing')\n",
    "y_l3_val = np.array(y_l3_val[y_l3_val != \"Missing\"]).astype(int)\n",
    "\n",
    "train_l4 = train[~train['y_l4'].isna()]\n",
    "\n",
    "# Get top X number of classes in the training data\n",
    "train_top = train_l4['y_l4'].value_counts()[0:32]\n",
    "\n",
    "# 16,869 data points after filtering for the top 32 classes \n",
    "train_l4 = train[train['y_l4'].isin(list(train_top.index))]\n",
    "\n",
    "y_l4_dict = dict(zip(set(train_l4['y_l4'].values), np.arange(0, len(set(train_l4['y_l4'].values)), 1)))\n",
    "\n",
    "train_l4 = train_l4[train['y_l4'].isin(list(train_top.index))]\n",
    "\n",
    "train_idx_4 = np.where(~train_l4['y_l4'].isna())\n",
    "train_l4 = train_l4[~train_l4['y_l4'].isna()]\n",
    "\n",
    "# Remove classes that are not in the dictionary\n",
    "y_l4_trn = train_l4['y_l4'].map(lambda x: y_l4_dict.get(x)).fillna('Missing')\n",
    "y_l4_val = valid['y_l4'].map(lambda x: y_l4_dict.get(x)).fillna('Missing')\n",
    "y_l4_tst = test['y_l4'].map(lambda x: y_l4_dict.get(x)).fillna('Missing')\n",
    "\n",
    "# Only include labels that are in the training set\n",
    "tst_miss = [y_l4_tst != \"Missing\"]\n",
    "tst_idx_4 = np.where(y_l4_tst != 'Missing')\n",
    "y_l4_tst = np.array(y_l4_tst[y_l4_tst != \"Missing\"]).astype(int)\n",
    "\n",
    "# Only include labels that are in the training set\n",
    "val_miss = [y_l4_val != \"Missing\"]\n",
    "val_idx_4 = np.where(y_l4_val != 'Missing')\n",
    "y_l4_val = np.array(y_l4_val[y_l4_val != \"Missing\"]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38577f9d",
   "metadata": {},
   "source": [
    "## GRU(X) - Mean(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf12f7e",
   "metadata": {},
   "source": [
    "### Predict level 1 icd codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "80df396f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m valid_dataset \u001b[39m=\u001b[39m preprocessing\u001b[39m.\u001b[39mnormalize(np\u001b[39m.\u001b[39marray(pd\u001b[39m.\u001b[39mDataFrame(valid[\u001b[39m'\u001b[39m\u001b[39mmean_gru_emb_tokens\u001b[39m\u001b[39m'\u001b[39m])[\u001b[39m'\u001b[39m\u001b[39mmean_gru_emb_tokens\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(pd\u001b[39m.\u001b[39mSeries)))\n\u001b[0;32m     30\u001b[0m valid_labels \u001b[39m=\u001b[39m preprocessing\u001b[39m.\u001b[39mnormalize(np\u001b[39m.\u001b[39marray(pd\u001b[39m.\u001b[39mDataFrame(valid[labels_marker])[labels_marker]\u001b[39m.\u001b[39mapply(pd\u001b[39m.\u001b[39mSeries)))\n\u001b[1;32m---> 32\u001b[0m nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  \n",
      "Cell \u001b[1;32mIn[31], line 83\u001b[0m, in \u001b[0;36mnn_softmax\u001b[1;34m(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)\u001b[0m\n\u001b[0;32m     79\u001b[0m     feed_dict \u001b[39m=\u001b[39m {tf_train_dataset : batch_data,\n\u001b[0;32m     80\u001b[0m                 tf_train_labels : batch_labels}\n\u001b[0;32m     82\u001b[0m     \u001b[39m# run one step of computation\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m     _, l, predictions \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39;49mrun([optimizer, loss, train_prediction],\n\u001b[0;32m     84\u001b[0m                                     feed_dict\u001b[39m=\u001b[39;49mfeed_dict)\n\u001b[0;32m     86\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Test F1 Score: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     87\u001b[0m     sm_metrics(test_prediction\u001b[39m.\u001b[39meval(), test_labels)[\u001b[39m0\u001b[39m]))\n\u001b[0;32m     88\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Test Precision: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     89\u001b[0m     sm_metrics(test_prediction\u001b[39m.\u001b[39meval(), test_labels)[\u001b[39m1\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\mab28\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39mNone\u001b[39;49;00m, fetches, feed_dict, options_ptr,\n\u001b[0;32m    969\u001b[0m                      run_metadata_ptr)\n\u001b[0;32m    970\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[0;32m    971\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32mc:\\Users\\mab28\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1191\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[39m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[0;32m   1189\u001b[0m \u001b[39m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[39mif\u001b[39;00m final_fetches \u001b[39mor\u001b[39;00m final_targets \u001b[39mor\u001b[39;00m (handle \u001b[39mand\u001b[39;00m feed_dict_tensor):\n\u001b[1;32m-> 1191\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_run(handle, final_targets, final_fetches,\n\u001b[0;32m   1192\u001b[0m                          feed_dict_tensor, options, run_metadata)\n\u001b[0;32m   1193\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1194\u001b[0m   results \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\mab28\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1371\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1368\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[0;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1371\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m   1372\u001b[0m                        run_metadata)\n\u001b[0;32m   1373\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[1;32mc:\\Users\\mab28\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1378\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_call\u001b[39m(\u001b[39mself\u001b[39m, fn, \u001b[39m*\u001b[39margs):\n\u001b[0;32m   1377\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m   1379\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOpError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1380\u001b[0m     message \u001b[39m=\u001b[39m compat\u001b[39m.\u001b[39mas_text(e\u001b[39m.\u001b[39mmessage)\n",
      "File \u001b[1;32mc:\\Users\\mab28\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1361\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[0;32m   1359\u001b[0m   \u001b[39m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[0;32m   1360\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extend_graph()\n\u001b[1;32m-> 1361\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m   1362\u001b[0m                                   target_list, run_metadata)\n",
      "File \u001b[1;32mc:\\Users\\mab28\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1454\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1452\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_tf_sessionrun\u001b[39m(\u001b[39mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1453\u001b[0m                         run_metadata):\n\u001b[1;32m-> 1454\u001b[0m   \u001b[39mreturn\u001b[39;00m tf_session\u001b[39m.\u001b[39;49mTF_SessionRun_wrapper(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session, options, feed_dict,\n\u001b[0;32m   1455\u001b[0m                                           fetch_list, target_list,\n\u001b[0;32m   1456\u001b[0m                                           run_metadata)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Mean representation of labels\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 100\n",
    "\n",
    "# number of target labels\n",
    "num_labels = 100 #len(np.unique(y_l1_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 50000\n",
    "\n",
    "level = 1\n",
    "labels_marker = f'mean_y_{level}_emb'\n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "train_labels = preprocessing.normalize(np.array(pd.DataFrame(train[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "test_labels = preprocessing.normalize(np.array(pd.DataFrame(test[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "valid_labels = preprocessing.normalize(np.array(pd.DataFrame(valid[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e522d1",
   "metadata": {},
   "source": [
    "### Precict level 2 icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59806bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.000\n",
      "\n",
      " Test Precision: 0.000\n",
      "\n",
      " Test Recall: 0.000\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 100\n",
    "\n",
    "# number of target labels\n",
    "num_labels = 100 #len(np.unique(y_l1_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 50000\n",
    "\n",
    "level = 2\n",
    "labels_marker = f'mean_y_{level}_emb'\n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "train_labels = preprocessing.normalize(np.array(pd.DataFrame(train[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "test_labels = preprocessing.normalize(np.array(pd.DataFrame(test[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "valid_labels = preprocessing.normalize(np.array(pd.DataFrame(valid[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368be6c1",
   "metadata": {},
   "source": [
    "### Precict level 3 icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc75561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.002\n",
      "\n",
      " Test Precision: 0.015\n",
      "\n",
      " Test Recall: 0.001\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 100\n",
    "\n",
    "# number of target labels\n",
    "num_labels = 100 #len(np.unique(y_l1_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 50000\n",
    "\n",
    "level = 3\n",
    "labels_marker = f'mean_y_{level}_emb'\n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "train_labels = preprocessing.normalize(np.array(pd.DataFrame(train[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "test_labels = preprocessing.normalize(np.array(pd.DataFrame(test[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "valid_labels = preprocessing.normalize(np.array(pd.DataFrame(valid[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60470b85",
   "metadata": {},
   "source": [
    "### Precict terminal icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb717276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (128, 100) for Tensor Placeholder_1:0, which has shape (128, 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m valid_dataset \u001b[39m=\u001b[39m preprocessing\u001b[39m.\u001b[39mnormalize(np\u001b[39m.\u001b[39marray(pd\u001b[39m.\u001b[39mDataFrame(valid[\u001b[39m'\u001b[39m\u001b[39mmean_gru_emb_tokens\u001b[39m\u001b[39m'\u001b[39m])[\u001b[39m'\u001b[39m\u001b[39mmean_gru_emb_tokens\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(pd\u001b[39m.\u001b[39mSeries)))\n\u001b[0;32m     28\u001b[0m valid_labels \u001b[39m=\u001b[39m preprocessing\u001b[39m.\u001b[39mnormalize(np\u001b[39m.\u001b[39marray(pd\u001b[39m.\u001b[39mDataFrame(valid[labels_marker])[labels_marker]\u001b[39m.\u001b[39mapply(pd\u001b[39m.\u001b[39mSeries)))\n\u001b[1;32m---> 30\u001b[0m nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  \n",
      "Cell \u001b[1;32mIn[31], line 83\u001b[0m, in \u001b[0;36mnn_softmax\u001b[1;34m(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)\u001b[0m\n\u001b[0;32m     79\u001b[0m     feed_dict \u001b[39m=\u001b[39m {tf_train_dataset : batch_data,\n\u001b[0;32m     80\u001b[0m                 tf_train_labels : batch_labels}\n\u001b[0;32m     82\u001b[0m     \u001b[39m# run one step of computation\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m     _, l, predictions \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39;49mrun([optimizer, loss, train_prediction],\n\u001b[0;32m     84\u001b[0m                                     feed_dict\u001b[39m=\u001b[39;49mfeed_dict)\n\u001b[0;32m     86\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Test F1 Score: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     87\u001b[0m     sm_metrics(test_prediction\u001b[39m.\u001b[39meval(), test_labels)[\u001b[39m0\u001b[39m]))\n\u001b[0;32m     88\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Test Precision: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     89\u001b[0m     sm_metrics(test_prediction\u001b[39m.\u001b[39meval(), test_labels)[\u001b[39m1\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\mab28\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39mNone\u001b[39;49;00m, fetches, feed_dict, options_ptr,\n\u001b[0;32m    969\u001b[0m                      run_metadata_ptr)\n\u001b[0;32m    970\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[0;32m    971\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32mc:\\Users\\mab28\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1165\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1161\u001b[0m   np_val \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(subfeed_val, dtype\u001b[39m=\u001b[39msubfeed_dtype)\n\u001b[0;32m   1163\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m is_tensor_handle_feed \u001b[39mand\u001b[39;00m\n\u001b[0;32m   1164\u001b[0m     \u001b[39mnot\u001b[39;00m subfeed_t\u001b[39m.\u001b[39mget_shape()\u001b[39m.\u001b[39mis_compatible_with(np_val\u001b[39m.\u001b[39mshape)):\n\u001b[1;32m-> 1165\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1166\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCannot feed value of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(np_val\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m for Tensor \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1167\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msubfeed_t\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m, which has shape \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1168\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(subfeed_t\u001b[39m.\u001b[39mget_shape())\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1169\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mis_feedable(subfeed_t):\n\u001b[0;32m   1170\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTensor \u001b[39m\u001b[39m{\u001b[39;00msubfeed_t\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m may not be fed.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (128, 100) for Tensor Placeholder_1:0, which has shape (128, 19)"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 100\n",
    "\n",
    "# number of target labels\n",
    "num_labels = 100 #len(np.unique(y_l1_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 50000\n",
    "\n",
    "level = 4\n",
    "labels_marker = f'mean_y_{level}_emb'\n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "train_labels = preprocessing.normalize(np.array(pd.DataFrame(train[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "test_labels = preprocessing.normalize(np.array(pd.DataFrame(test[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "valid_labels = preprocessing.normalize(np.array(pd.DataFrame(valid[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4117a",
   "metadata": {},
   "source": [
    "## GRU(X) - atomic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e5df54",
   "metadata": {},
   "source": [
    "### Predict level 1 icd codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6b7ce0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.463\n",
      "\n",
      " Test Precision: 0.483\n",
      "\n",
      " Test Recall: 0.522\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data for GRU models\n",
    "# GRU with softmax activation\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from keras import Sequential\n",
    "\n",
    "# # Pad the word embeddings so that they are all the same length\n",
    "# #t = pad_sequences([torch.tensor(x) for x in df['idx_tokens']], padding='post')\n",
    "# #df['pad_idx'] = t.tolist()\n",
    "# #df['emb_tokens'] = df['pad_idx'].map(lambda t: [np.array(embed.weight.data[w]) for w in t])\n",
    "# #df[\"stack_emb_tokens\"] = df['emb_tokens'].map(lambda t: torch.stack(t, dim = 0))\n",
    "\n",
    "# # Order the documents based on their length and then batch them accordingly\n",
    "# df['token length'] = df['idx_tokens'].map(lambda x: len(x))\n",
    "# sort = train.sort_values(by=['trunc_len'])\n",
    "\n",
    "# # Split into mini-batches\n",
    "# len(sort)/4000\n",
    "\n",
    "# Initialize bi-directional GRU with torch (ref: https://gist.github.com/ceshine/bed2dadca48fe4fe4b4600ccce2fd6e1)\n",
    "\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 100\n",
    "\n",
    "# number of target labels\n",
    "num_labels = len(np.unique(y_l1_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 50000\n",
    "\n",
    "  \n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "train_labels = one_hot(y_l1_trn, num_labels)\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "test_labels = one_hot(y_l1_tst, num_labels)\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "valid_labels = one_hot(y_l1_val, num_labels)\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e5df54",
   "metadata": {},
   "source": [
    "### Predict level 2 icd codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6b7ce0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.278\n",
      "\n",
      " Test Precision: 0.304\n",
      "\n",
      " Test Recall: 0.342\n"
     ]
    }
   ],
   "source": [
    "# Set the seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 100\n",
    "\n",
    "# number of target labels\n",
    "num_labels = len(np.unique(y_l2_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 50000\n",
    "  \n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "train_labels = one_hot(y_l2_trn, num_labels)\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['mean_gru_emb_tokens'][tst_idx_2[0]])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "test_labels = one_hot(y_l2_tst, num_labels)\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['mean_gru_emb_tokens'][val_idx_2[0]])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "valid_labels = one_hot(y_l2_val, num_labels)\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e5df54",
   "metadata": {},
   "source": [
    "### Predict level 3 icd codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6b7ce0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.153\n",
      "\n",
      " Test Precision: 0.167\n",
      "\n",
      " Test Recall: 0.220\n"
     ]
    }
   ],
   "source": [
    "# Set the seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 100\n",
    "\n",
    "# number of target labels\n",
    "num_labels = len(np.unique(y_l3_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 50000\n",
    "\n",
    "train_idx = np.where(train_miss != True)\n",
    "  \n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['mean_gru_emb_tokens'][train_idx[0]])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "train_labels = one_hot(y_l3_trn, num_labels)\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['mean_gru_emb_tokens'][tst_idx_3[0]])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "test_labels = one_hot(y_l3_tst, num_labels)\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['mean_gru_emb_tokens'][val_idx_3[0]])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "valid_labels = one_hot(y_l3_val, num_labels)\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e5df54",
   "metadata": {},
   "source": [
    "### Predict terminal icd codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6b7ce0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.061\n",
      "\n",
      " Test Precision: 0.038\n",
      "\n",
      " Test Recall: 0.167\n"
     ]
    }
   ],
   "source": [
    "# Set the seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 100\n",
    "\n",
    "# number of target labels\n",
    "num_labels = len(np.unique(y_l4_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 50000\n",
    "\n",
    "  \n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['mean_gru_emb_tokens'][train_idx_4[0]])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "train_labels = one_hot(y_l4_trn, num_labels)\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['mean_gru_emb_tokens'][tst_idx_4[0]])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "test_labels = one_hot(y_l4_tst, num_labels)\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['mean_gru_emb_tokens'][val_idx_4[0]])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "valid_labels = one_hot(y_l4_val, num_labels)\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cb0855",
   "metadata": {},
   "source": [
    "## GRU(X) - GRU(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d819ff",
   "metadata": {},
   "source": [
    "## Predict Level 1 Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "80df396f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.000\n",
      "\n",
      " Test Precision: 0.000\n",
      "\n",
      " Test Recall: 0.000\n"
     ]
    }
   ],
   "source": [
    "# Mean representation of labels\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 100\n",
    "\n",
    "# number of target labels\n",
    "num_labels = 100\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 5\n",
    "\n",
    "level = 1\n",
    "labels_marker = f'mean_gru_emb_tokens_y_{level}'\n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "train_labels = preprocessing.normalize(np.array(pd.DataFrame(train[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "test_labels = preprocessing.normalize(np.array(pd.DataFrame(test[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "valid_labels = preprocessing.normalize(np.array(pd.DataFrame(valid[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b15af",
   "metadata": {},
   "source": [
    "## Predict Level 2 Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59806bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 100\n",
    "\n",
    "# number of target labels\n",
    "num_labels = 100\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 5000\n",
    "\n",
    "level = 2\n",
    "labels_marker = f'mean_gru_emb_tokens_y_{level}'\n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "train_labels = preprocessing.normalize(np.array(pd.DataFrame(train[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "test_labels = preprocessing.normalize(np.array(pd.DataFrame(test[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "valid_labels = preprocessing.normalize(np.array(pd.DataFrame(valid[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96f72b8",
   "metadata": {},
   "source": [
    "## Predict Level 3 Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc75561",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 100\n",
    "\n",
    "# number of target labels\n",
    "num_labels = 100\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 5000\n",
    "\n",
    "level = 3\n",
    "labels_marker = f'mean_gru_emb_tokens_y_{level}'\n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "train_labels = preprocessing.normalize(np.array(pd.DataFrame(train[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "test_labels = preprocessing.normalize(np.array(pd.DataFrame(test[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "valid_labels = preprocessing.normalize(np.array(pd.DataFrame(valid[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9777c75b",
   "metadata": {},
   "source": [
    "## Predict Level 4 Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb717276",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "TF.random.set_seed(seed)\n",
    "\n",
    "# number of features\n",
    "num_features = 100\n",
    "\n",
    "# number of target labels\n",
    "num_labels = 100\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 5000\n",
    "\n",
    "level = 4\n",
    "labels_marker = f'mean_gru_emb_tokens_y_{level}'\n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(train['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "train_labels = preprocessing.normalize(np.array(pd.DataFrame(train[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(test['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "test_labels = preprocessing.normalize(np.array(pd.DataFrame(test[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(valid['mean_gru_emb_tokens'])['mean_gru_emb_tokens'].apply(pd.Series)))\n",
    "valid_labels = preprocessing.normalize(np.array(pd.DataFrame(valid[labels_marker])[labels_marker].apply(pd.Series)))\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "277e9aaf58bbfde54e1956aac8f2babd4c41f226386ab1e58381dbe6dfe73f30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
