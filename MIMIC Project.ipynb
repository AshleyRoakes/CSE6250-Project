{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a331de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk import word_tokenize\n",
    "from nltk.internals import find_jars_within_path\n",
    "import nltk\n",
    "import sklearn.model_selection\n",
    "from collections import Counter\n",
    "import sklearn.feature_extraction.text as skt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "#import tensorflow as tf\n",
    "#from tensorflow.keras import layers\n",
    "\n",
    "#import torch\n",
    "#import transformers\n",
    "\n",
    "import pyhealth\n",
    "from pyhealth.medcode import InnerMap\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as STOP_WORDS\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c542936",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/Users/ashleyroakes/Desktop/\"\n",
    "mim_root = root + \"mimic-iii-clinical-database-1.4/\"\n",
    "\n",
    "#root = \"../data/\"\n",
    "#mim_root = \"../data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b7b726",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "## Read in Discharge Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9078a11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-36-44e46582d5d0>:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  notes_df = pd.read_csv(notes, compression='gzip', error_bad_lines=False,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of discharge summaries:  55177\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22532</td>\n",
       "      <td>167853.0</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2151-7-16**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13702</td>\n",
       "      <td>107527.0</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2118-6-2**]       Discharg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13702</td>\n",
       "      <td>167118.0</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2119-5-4**]              D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13702</td>\n",
       "      <td>196489.0</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2124-7-21**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26880</td>\n",
       "      <td>135453.0</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2162-3-3**]              D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID   HADM_ID           CATEGORY DESCRIPTION  \\\n",
       "0       22532  167853.0  Discharge summary      Report   \n",
       "1       13702  107527.0  Discharge summary      Report   \n",
       "2       13702  167118.0  Discharge summary      Report   \n",
       "3       13702  196489.0  Discharge summary      Report   \n",
       "4       26880  135453.0  Discharge summary      Report   \n",
       "\n",
       "                                                TEXT  \n",
       "0  Admission Date:  [**2151-7-16**]       Dischar...  \n",
       "1  Admission Date:  [**2118-6-2**]       Discharg...  \n",
       "2  Admission Date:  [**2119-5-4**]              D...  \n",
       "3  Admission Date:  [**2124-7-21**]              ...  \n",
       "4  Admission Date:  [**2162-3-3**]              D...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes = mim_root + \"NOTEEVENTS.csv.gz\"\n",
    "\n",
    "notes_df = pd.read_csv(notes, compression='gzip', error_bad_lines=False, \n",
    "                       usecols = ['SUBJECT_ID', 'HADM_ID', 'CATEGORY', 'DESCRIPTION','TEXT'])\\\n",
    "                      .query(\"CATEGORY == 'Discharge summary'\")\\\n",
    "                      .query(\"DESCRIPTION == 'Report'\")\n",
    "\n",
    "# Should be 55,177 records\n",
    "print(\"Number of discharge summaries: \", + len(notes_df))\n",
    "\n",
    "notes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757857a9",
   "metadata": {},
   "source": [
    "## Read in Patient Diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e16993f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-37-64ff0258cef1>:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  diag_df = pd.read_csv(diag, compression='gzip', error_bad_lines=False)\\\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>[25013, 3371, 5849, 5780, V5867, 25063, 5363, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>[53100, 2851, 07054, 5715, 45621, 53789, 4019,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100006</td>\n",
       "      <td>[49320, 51881, 486, 20300, 2761, 7850, 3090, V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100007</td>\n",
       "      <td>[56081, 5570, 9973, 486, 4019]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100009</td>\n",
       "      <td>[41401, 99604, 4142, 25000, 27800, V8535, 4148...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                          ICD9_CODE\n",
       "0   100001  [25013, 3371, 5849, 5780, V5867, 25063, 5363, ...\n",
       "1   100003  [53100, 2851, 07054, 5715, 45621, 53789, 4019,...\n",
       "2   100006  [49320, 51881, 486, 20300, 2761, 7850, 3090, V...\n",
       "3   100007                     [56081, 5570, 9973, 486, 4019]\n",
       "4   100009  [41401, 99604, 4142, 25000, 27800, V8535, 4148..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag = mim_root + \"DIAGNOSES_ICD.csv.gz\"\n",
    "\n",
    "diag_df = pd.read_csv(diag, compression='gzip', error_bad_lines=False)\\\n",
    "                    .dropna()\\\n",
    "                    .groupby('HADM_ID')['ICD9_CODE']\\\n",
    "                    .unique()\\\n",
    "                    .reset_index()\n",
    "\n",
    "diag_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5befa018",
   "metadata": {},
   "source": [
    "## Read in ICD9 Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e7e3472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-0e824f7752d2>:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  icd_df = pd.read_csv(icd, compression='gzip', error_bad_lines=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>SHORT_TITLE</th>\n",
       "      <th>LONG_TITLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174</td>\n",
       "      <td>01166</td>\n",
       "      <td>TB pneumonia-oth test</td>\n",
       "      <td>Tuberculous pneumonia [any form], tubercle bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175</td>\n",
       "      <td>01170</td>\n",
       "      <td>TB pneumothorax-unspec</td>\n",
       "      <td>Tuberculous pneumothorax, unspecified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176</td>\n",
       "      <td>01171</td>\n",
       "      <td>TB pneumothorax-no exam</td>\n",
       "      <td>Tuberculous pneumothorax, bacteriological or h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>177</td>\n",
       "      <td>01172</td>\n",
       "      <td>TB pneumothorx-exam unkn</td>\n",
       "      <td>Tuberculous pneumothorax, bacteriological or h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>178</td>\n",
       "      <td>01173</td>\n",
       "      <td>TB pneumothorax-micro dx</td>\n",
       "      <td>Tuberculous pneumothorax, tubercle bacilli fou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROW_ID ICD9_CODE               SHORT_TITLE  \\\n",
       "0     174     01166     TB pneumonia-oth test   \n",
       "1     175     01170    TB pneumothorax-unspec   \n",
       "2     176     01171   TB pneumothorax-no exam   \n",
       "3     177     01172  TB pneumothorx-exam unkn   \n",
       "4     178     01173  TB pneumothorax-micro dx   \n",
       "\n",
       "                                          LONG_TITLE  \n",
       "0  Tuberculous pneumonia [any form], tubercle bac...  \n",
       "1              Tuberculous pneumothorax, unspecified  \n",
       "2  Tuberculous pneumothorax, bacteriological or h...  \n",
       "3  Tuberculous pneumothorax, bacteriological or h...  \n",
       "4  Tuberculous pneumothorax, tubercle bacilli fou...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icd = mim_root + \"D_ICD_DIAGNOSES.csv.gz\"\n",
    "icd_df = pd.read_csv(icd, compression='gzip', error_bad_lines=False)\n",
    "\n",
    "\n",
    "icd_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d7502",
   "metadata": {},
   "source": [
    "## Merge datasets by HADM_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb8a4e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55172"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(diag_df, notes_df, on='HADM_ID', how='inner')\n",
    "\n",
    "# Should be 55177-5 = 55172\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b193cbba",
   "metadata": {},
   "source": [
    "## Substitute special sequences,  Filter HoPI sections, & Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cb0055",
   "metadata": {},
   "source": [
    "### Identify HOPI sections & Substitute Special Sequences: These special sequences were identified and replaced by the first token in the sequence, e.g. \"[**Hospital1 18**]\" was replaced by ‘Hospital1’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f357cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_notes(st):\n",
    "    s  = \"History of Present Illness\"\n",
    "    s1 = \"HISTORY OF PRESENT ILLNESS|HISTORY OF THE PRESENT ILLNESS|\\nHISTORY:|present illness|Present Illness|PRESENT ILLNESS\"\n",
    "    \n",
    "    match  = re.search(s, st)\n",
    "    match1 = re.search(s1, st)\n",
    "    \n",
    "    if (match is not None) or (match1 is not None):\n",
    "        if match is not None:\n",
    "            st = st.split(s, 1)[1]\n",
    "            e = \"\\n\\n\\n\"\n",
    "            n = st.split(e, 1)[0]\n",
    "    \n",
    "        elif match1 is not None: \n",
    "            st = st.split(match1[0], 1)[1]\n",
    "            e = re.search(r\"\\n[\\s\\w]+:\", st)[0]\n",
    "            n = st.split(e, 1)[0]\n",
    "        \n",
    "        # Replace special strings with \"\"\n",
    "        rep = re.findall(r\"\\[\\*\\*([a-zA-Z0-9]*)\", n)\n",
    "        strt = [m.start() for m in re.finditer(\"\\[\\*\\*([a-zA-Z0-9]*)\", n)]\n",
    "        end = [m.end() for m in re.finditer(\"([a-zA-Z0-9]*)\\*\\*\\]\", n)]\n",
    "\n",
    "        for i in range(len(rep)):\n",
    "            n = n[:strt[i]] + rep[i] + \" \" + n[end[i] + 1:]\n",
    "        \n",
    "    else: \n",
    "        n = ''\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "991b1670",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['HOPI'] = df[\"TEXT\"].map(lambda t: process_notes(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548992e0",
   "metadata": {},
   "source": [
    "### Number of missing HOPI sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "165ed5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes without HOPI sections: 1867\n"
     ]
    }
   ],
   "source": [
    "# Detect history of present illness in text (n = 2641 records without HoPI data)\n",
    "df_hopi = df[df[\"HOPI\"] != \"\"].reset_index(drop = True)\n",
    "\n",
    "missing = df[df[\"HOPI\"] == \"\"].reset_index(drop = True)\n",
    "\n",
    "len(missing)\n",
    "\n",
    "# Should be 2641 records\n",
    "print(\"Notes without HOPI sections: \" + str(len(missing)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87147948",
   "metadata": {},
   "source": [
    "### Initialize Stanford Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d43fe6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the jar and model via their path (instead of setting environment variables):\n",
    "jar = root + 'stanford-postagger-full-2020-11-17/stanford-postagger.jar'\n",
    "model = root + 'stanford-postagger-full-2020-11-17/models/english-left3words-distsim.tagger'\n",
    "\n",
    "pos_tagger = StanfordPOSTagger(model, jar, encoding='utf8')\n",
    "\n",
    "# Add other jars from Stanford directory\n",
    "stanford_dir = pos_tagger._stanford_jar.rpartition('/')[0]\n",
    "stanford_jars = find_jars_within_path(stanford_dir)\n",
    "pos_tagger._stanford_jar = ':'.join(stanford_jars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807f5515",
   "metadata": {},
   "source": [
    "### Tokenize, Remove Punctuation and Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da236dca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL WILL BE REMOVED \n",
    "# For testing out other possible HOPI section headers\n",
    "\n",
    "#PREOPERATIVE DIAGNOSIS (n = 6)\n",
    "#ADMISSION DIAGNOSIS(es) (n = 38)(n = 45)\n",
    "#ADMITTING DIAGNOSES (n = 7)\n",
    "#HOSPITAL COURSE\n",
    "#REASON FOR ADMISSION\n",
    "\n",
    "ms = missing[\"TEXT\"].map(lambda t: re.search(\"\\nHISTORY:\", t))\n",
    "m = missing[~ms.isna()].reset_index()\n",
    "\n",
    "cap = 138\n",
    "lw = 15\n",
    "lwlw = 12\n",
    "hpi = 2\n",
    "crs = 951\n",
    "print(len(m))\n",
    "#print(m[\"TEXT\"][1])\n",
    "\n",
    "#missing[\"TEXT\"].to_csv(root + \"no hopi.csv\"\n",
    "\n",
    "#df[\"HOPI\"][8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "62eba9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation, non-characters, etc. \n",
    "df_hopi[\"HOPI\"] = df_hopi[\"HOPI\"].apply(lambda x: re.sub('\\[\\*\\*[^\\]]*\\*\\*\\]', '', x))\n",
    "df_hopi[\"HOPI\"] = df_hopi[\"HOPI\"].apply(lambda x: re.sub('<[^>]*>', '', x))\n",
    "df_hopi[\"HOPI\"] = df_hopi[\"HOPI\"].apply(lambda x: re.sub('[\\W]+', ' ', x))\n",
    "df_hopi[\"HOPI\"] = df_hopi[\"HOPI\"].apply(lambda x: re.sub(\"\\d+\", \" \", x))\n",
    "\n",
    "# Tokenize\n",
    "df_hopi['tokens'] = df_hopi[\"HOPI\"].map(lambda n: word_tokenize(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9900b72c",
   "metadata": {},
   "source": [
    "### Truncate at 500 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d96b2801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of HOPI sections >= 500 tokens: 1656\n"
     ]
    }
   ],
   "source": [
    "# Truncate records with more than 500 tokes (n = 1143)\n",
    "count = df_hopi['tokens'].map(lambda c: len(np.unique(c)))\n",
    "\n",
    "df_hopi[\"trunc\"] = df_hopi['tokens'].map(lambda c: np.unique(c)[0:500])\n",
    "\n",
    "print(\"Number of HOPI sections >= 500 tokens: \" + str(len(df_hopi[count>=500])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b841b468",
   "metadata": {},
   "source": [
    "## Plot a histogram of the Number of tokens in each HoPI document, after data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3b72313d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_hopi[\"trunc_len\"] = df_hopi['trunc'].map(lambda c: len(c))\n",
    "\n",
    "plt.hist(df_hopi[\"trunc_len\"], np.arange(0, 501, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f07c2f",
   "metadata": {},
   "source": [
    "## Split data in train, valid, and test sets\n",
    "### training (38,588 records, 69.9%), validation (5536 records, 10.0%) and testing (11,048 records, 20.0%) folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c9cc17eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random split\n",
    "train, tst = sklearn.model_selection.train_test_split(df_hopi, test_size= 0.3, random_state=42)\n",
    "\n",
    "test, valid = sklearn.model_selection.train_test_split(tst, test_size= 0.33, random_state=42)\n",
    "\n",
    "train = train.reset_index(drop = True).drop([\"CATEGORY\", \"DESCRIPTION\"], axis = 1)\n",
    "test = test.reset_index(drop = True).drop([\"CATEGORY\", \"DESCRIPTION\"], axis = 1)\n",
    "valid = valid.reset_index(drop = True).drop([\"CATEGORY\", \"DESCRIPTION\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c530173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get percentage of data in each set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6f8d07",
   "metadata": {},
   "source": [
    "## Count number of tokens in the training dataset (n = ~92,468 tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "790c81fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurence of tokens that are in the training dataset\n",
    "n = len(np.unique(np.concatenate(train[\"trunc\"].values)))\n",
    "occ = Counter(np.concatenate(train[\"trunc\"].values))\n",
    "             \n",
    "# Tokens that occur >= 5 times are in the study vocabulary (should be ~19,503)\n",
    "vocab = [k for k,v in occ.items() if v >= 5]\n",
    "len(vocab)\n",
    "\n",
    "# Assign a unique integer ID for each token in the study vocabulary \n",
    "vocab_lookup = dict(zip(vocab, np.arange(0, len(vocab), 1)))\n",
    "\n",
    "# Convert each HoPI document to a 1D array of integers using this index\n",
    "#df_hopi[\"trunc_idx\"] = df_hopi[\"trunc\"].map(lambda x: [vocab_lookup.get(i) for i in x if i in vocab_lookup.keys()])\n",
    "\n",
    "train[\"trunc_idx\"] =  train[\"trunc\"].map(lambda x: [vocab_lookup.get(i) for i in x if i in vocab_lookup.keys()])\n",
    "valid[\"trunc_idx\"] =  valid[\"trunc\"].map(lambda x: [vocab_lookup.get(i) for i in x if i in vocab_lookup.keys()])\n",
    "test[\"trunc_idx\"] =  test[\"trunc\"].map(lambda x: [vocab_lookup.get(i) for i in x if i in vocab_lookup.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b180a12e",
   "metadata": {},
   "source": [
    "# Document representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "04e11ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent clinical notes documents as TF-IDF representation\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def tfidf(df):\n",
    "    pipe = Pipeline([('count', CountVectorizer(vocabulary = vocab, lowercase = False)),('tfid', TfidfTransformer(use_idf=True))]).fit(df[\"HOPI\"].values)\n",
    "    pipe['count'].transform(df[\"HOPI\"].values).toarray()\n",
    "    pipe['tfid'].idf_\n",
    "\n",
    "    return pipe.transform(df[\"HOPI\"].values)\n",
    "\n",
    "# Create sparse matrix of size number of docs x words in vocab\n",
    "tfidf_train = tfidf(train)\n",
    "tfidf_valid = tfidf(valid)\n",
    "tfidf_test = tfidf(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e46938b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean embedding representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a05ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb2746c",
   "metadata": {},
   "source": [
    "# Label representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c0afc9",
   "metadata": {},
   "source": [
    "### Identify the hierarchical labels for each icd9 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e7305717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out the main ICD9 code (they are ordered for importance)\n",
    "train[\"ICD_Main\"] = train['ICD9_CODE'].map(lambda x: x[0])\n",
    "valid[\"ICD_Main\"] = valid['ICD9_CODE'].map(lambda x: x[0])\n",
    "test[\"ICD_Main\"] = test['ICD9_CODE'].map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e9714dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pyhealth library to find code hierarchy for the train, test, and valid datasets\n",
    "icd9cm = InnerMap.load(\"ICD9CM\")\n",
    "\n",
    "def define_levels(x):\n",
    "    lvls = []\n",
    "    for l in x: \n",
    "        lvls.append(icd9cm.lookup(l))\n",
    "    return lvls\n",
    "\n",
    "# NEED TO REMOVE '001-999.99' FROM SOME OF THE ANCESTOR LISTS\n",
    "\n",
    "def get_codes(df):\n",
    "    df['icd_desc'] = df['ICD_Main'].map(lambda x: icd9cm.lookup(x))\n",
    "    df['levels'] = df['ICD_Main'].map(lambda x:icd9cm.get_ancestors(x)[::-1])\n",
    "    #df['lvls'] = df['levels'].map(lambda x: drop_top(x))\n",
    "    df['levels_desc'] = df['levels'].map(lambda x: define_levels(x))\n",
    "\n",
    "get_codes(train)\n",
    "get_codes(valid)\n",
    "get_codes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eb02862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED TO REMOVE '001-999.99' FROM SOME OF THE ANCESTOR LISTS\n",
    "\n",
    "#df['lvls'] = df['levels'].map(lambda x: drop_top(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "65bc18b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labs = pd.concat([train, train['levels_desc'].apply(pd.Series)], axis = 1).drop(\"levels_desc\", axis = 1).rename(columns={0: \"l1\", 1: \"l2\", 2: \"l3\", 3: \"l4\" })\n",
    "valid_labs = pd.concat([valid, valid['levels_desc'].apply(pd.Series)], axis = 1).drop(\"levels_desc\", axis = 1).rename(columns={0: \"l1\", 1: \"l2\", 2: \"l3\", 3: \"l4\" })\n",
    "test_labs = pd.concat([test, test['levels_desc'].apply(pd.Series)], axis = 1).drop(\"levels_desc\", axis = 1).rename(columns={0: \"l1\", 1: \"l2\", 2: \"l3\", 3: \"l4\" })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00750564",
   "metadata": {},
   "source": [
    "### TFIDF-Atomic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "32b285c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Logistic Regression from sklearn \n",
    "\n",
    "# L2 values to iterate over\n",
    "L2 = np.logspace(.01, 100, num = 50)\n",
    "\n",
    "# Multinomial Logistic Regression Model\n",
    "tfidf_atomic = LogisticRegression(multi_class = 'multinomial', penalty = 'l2', max_iter = 1500, solver = \"sag\", random_state = 42)\n",
    "\n",
    "#tfidf_atomic_L1 = tfidf.fit(tfidf_train, train['l1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe63853",
   "metadata": {},
   "source": [
    "### Mean-embedding representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8658d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57d4117a",
   "metadata": {},
   "source": [
    "### GRU representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b7ce0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
