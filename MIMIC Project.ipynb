{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a331de94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashleyroakes/opt/anaconda3/envs/Project/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk import word_tokenize\n",
    "from nltk.internals import find_jars_within_path\n",
    "import nltk\n",
    "import sklearn.model_selection\n",
    "from collections import Counter\n",
    "import sklearn.feature_extraction.text as skt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import pyhealth\n",
    "from pyhealth.medcode import InnerMap\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import gensim.models.word2vec as w2v\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import tensorflow as TF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c542936",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/Users/ashleyroakes/Desktop/\"\n",
    "#root = \"./data/\"\n",
    "\n",
    "mim_root = root + \"mimic-iii-clinical-database-1.4/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b7b726",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "## Read in Discharge Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9078a11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of discharge summaries:  55177\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22532</td>\n",
       "      <td>167853.0</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2151-7-16**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13702</td>\n",
       "      <td>107527.0</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2118-6-2**]       Discharg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13702</td>\n",
       "      <td>167118.0</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2119-5-4**]              D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13702</td>\n",
       "      <td>196489.0</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2124-7-21**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26880</td>\n",
       "      <td>135453.0</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2162-3-3**]              D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID   HADM_ID           CATEGORY DESCRIPTION  \\\n",
       "0       22532  167853.0  Discharge summary      Report   \n",
       "1       13702  107527.0  Discharge summary      Report   \n",
       "2       13702  167118.0  Discharge summary      Report   \n",
       "3       13702  196489.0  Discharge summary      Report   \n",
       "4       26880  135453.0  Discharge summary      Report   \n",
       "\n",
       "                                                TEXT  \n",
       "0  Admission Date:  [**2151-7-16**]       Dischar...  \n",
       "1  Admission Date:  [**2118-6-2**]       Discharg...  \n",
       "2  Admission Date:  [**2119-5-4**]              D...  \n",
       "3  Admission Date:  [**2124-7-21**]              ...  \n",
       "4  Admission Date:  [**2162-3-3**]              D...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes = mim_root + \"NOTEEVENTS.csv.gz\"\n",
    "\n",
    "notes_df = pd.read_csv(notes, compression='gzip', error_bad_lines=False, \n",
    "                       usecols = ['SUBJECT_ID', 'HADM_ID', 'CATEGORY', 'DESCRIPTION','TEXT'])\\\n",
    "                      .query(\"CATEGORY == 'Discharge summary'\")\\\n",
    "                      .query(\"DESCRIPTION == 'Report'\")\n",
    "\n",
    "# Should be 55,177 records\n",
    "print(\"Number of discharge summaries: \", + len(notes_df))\n",
    "\n",
    "notes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757857a9",
   "metadata": {},
   "source": [
    "## Read in Patient Diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e16993f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>[25013, 3371, 5849, 5780, V5867, 25063, 5363, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>[53100, 2851, 07054, 5715, 45621, 53789, 4019,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100006</td>\n",
       "      <td>[49320, 51881, 486, 20300, 2761, 7850, 3090, V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100007</td>\n",
       "      <td>[56081, 5570, 9973, 486, 4019]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100009</td>\n",
       "      <td>[41401, 99604, 4142, 25000, 27800, V8535, 4148...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                          ICD9_CODE\n",
       "0   100001  [25013, 3371, 5849, 5780, V5867, 25063, 5363, ...\n",
       "1   100003  [53100, 2851, 07054, 5715, 45621, 53789, 4019,...\n",
       "2   100006  [49320, 51881, 486, 20300, 2761, 7850, 3090, V...\n",
       "3   100007                     [56081, 5570, 9973, 486, 4019]\n",
       "4   100009  [41401, 99604, 4142, 25000, 27800, V8535, 4148..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag = mim_root + \"DIAGNOSES_ICD.csv.gz\"\n",
    "\n",
    "diag_df = pd.read_csv(diag, compression='gzip', error_bad_lines=False)\\\n",
    "                    .dropna()\\\n",
    "                    .groupby('HADM_ID')['ICD9_CODE']\\\n",
    "                    .unique()\\\n",
    "                    .reset_index()\n",
    "\n",
    "diag_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5befa018",
   "metadata": {},
   "source": [
    "## Read in ICD9 Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e7e3472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>SHORT_TITLE</th>\n",
       "      <th>LONG_TITLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174</td>\n",
       "      <td>01166</td>\n",
       "      <td>TB pneumonia-oth test</td>\n",
       "      <td>Tuberculous pneumonia [any form], tubercle bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175</td>\n",
       "      <td>01170</td>\n",
       "      <td>TB pneumothorax-unspec</td>\n",
       "      <td>Tuberculous pneumothorax, unspecified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176</td>\n",
       "      <td>01171</td>\n",
       "      <td>TB pneumothorax-no exam</td>\n",
       "      <td>Tuberculous pneumothorax, bacteriological or h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>177</td>\n",
       "      <td>01172</td>\n",
       "      <td>TB pneumothorx-exam unkn</td>\n",
       "      <td>Tuberculous pneumothorax, bacteriological or h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>178</td>\n",
       "      <td>01173</td>\n",
       "      <td>TB pneumothorax-micro dx</td>\n",
       "      <td>Tuberculous pneumothorax, tubercle bacilli fou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROW_ID ICD9_CODE               SHORT_TITLE  \\\n",
       "0     174     01166     TB pneumonia-oth test   \n",
       "1     175     01170    TB pneumothorax-unspec   \n",
       "2     176     01171   TB pneumothorax-no exam   \n",
       "3     177     01172  TB pneumothorx-exam unkn   \n",
       "4     178     01173  TB pneumothorax-micro dx   \n",
       "\n",
       "                                          LONG_TITLE  \n",
       "0  Tuberculous pneumonia [any form], tubercle bac...  \n",
       "1              Tuberculous pneumothorax, unspecified  \n",
       "2  Tuberculous pneumothorax, bacteriological or h...  \n",
       "3  Tuberculous pneumothorax, bacteriological or h...  \n",
       "4  Tuberculous pneumothorax, tubercle bacilli fou...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icd = mim_root + \"D_ICD_DIAGNOSES.csv.gz\"\n",
    "icd_df = pd.read_csv(icd, compression='gzip', error_bad_lines=False)\n",
    "\n",
    "\n",
    "icd_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d7502",
   "metadata": {},
   "source": [
    "## Merge datasets by HADM_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb8a4e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55172"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(diag_df, notes_df, on='HADM_ID', how='inner')\n",
    "\n",
    "# Should be 55177-5 = 55172\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b193cbba",
   "metadata": {},
   "source": [
    "## Substitute special sequences & Filter HoPI sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cb0055",
   "metadata": {},
   "source": [
    "### Identify HOPI sections & Substitute Special Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f357cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_notes(st):\n",
    "    s  = \"History of Present Illness\"\n",
    "    s1 = \"HISTORY OF PRESENT ILLNESS|HISTORY OF THE PRESENT ILLNESS|\\nHISTORY:|present illness|Present Illness|PRESENT ILLNESS\"\n",
    "    \n",
    "    match  = re.search(s, st)\n",
    "    match1 = re.search(s1, st)\n",
    "    \n",
    "    if (match is not None) or (match1 is not None):\n",
    "        if match is not None:\n",
    "            st = st.split(s, 1)[1]\n",
    "            e = \"\\n\\n\\n\"\n",
    "            n = st.split(e, 1)[0]\n",
    "    \n",
    "        elif match1 is not None: \n",
    "            st = st.split(match1[0], 1)[1]\n",
    "            e = re.search(r\"\\n[\\s\\w]+:\", st)[0]\n",
    "            n = st.split(e, 1)[0]\n",
    "        \n",
    "        # Replace special strings with \"\"\n",
    "        rep = re.findall(r\"\\[\\*\\*([a-zA-Z0-9]*)\", n)\n",
    "        strt = [m.start() for m in re.finditer(\"\\[\\*\\*([a-zA-Z0-9]*)\", n)]\n",
    "        end = [m.end() for m in re.finditer(\"([a-zA-Z0-9]*)\\*\\*\\]\", n)]\n",
    "\n",
    "        for i in range(len(rep)):\n",
    "            n = n[:strt[i]] + rep[i] + \" \" + n[end[i] + 1:]\n",
    "        \n",
    "    else: \n",
    "        n = ''\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "991b1670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs missing HOPI sections: 1867\n"
     ]
    }
   ],
   "source": [
    "df['HOPI'] = df[\"TEXT\"].map(lambda t: process_notes(t))\n",
    "\n",
    "df[\"HOPI\"] = df[\"HOPI\"].apply(lambda x: re.sub('\\[\\*\\*[^\\]]*\\*\\*\\]', '', x))\n",
    "df[\"HOPI\"] = df[\"HOPI\"].apply(lambda x: re.sub('<[^>]*>', '', x))\n",
    "df[\"HOPI\"] = df[\"HOPI\"].apply(lambda x: re.sub('[\\W]+', ' ', x))\n",
    "\n",
    "# Detect history of present illness in text (n = 2641 records without HoPI data)\n",
    "missing = len(df[df['HOPI'] == \"\"])\n",
    "print(\"Number of docs missing HOPI sections: \" + str(missing))\n",
    "\n",
    "df = df[df[\"HOPI\"] != \"\"].reset_index(drop = True)\n",
    "df['SENT_TOKENS'] = df[\"HOPI\"].map(lambda t: [p.lower() for p in nltk.RegexpTokenizer(r'\\w+').tokenize(t) if not p.isnumeric()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a132416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>HOPI</th>\n",
       "      <th>SENT_TOKENS</th>\n",
       "      <th>SENT_TOKENS_COMBO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>[25013, 3371, 5849, 5780, V5867, 25063, 5363, ...</td>\n",
       "      <td>58526</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2117-9-11**]              ...</td>\n",
       "      <td>35F w poorly controlled Type 1 diabetes melli...</td>\n",
       "      <td>[325mg, 35f, 3l, 3rd, 4mg, 5d, a, ag, also, am...</td>\n",
       "      <td>325mg 35f 3l 3rd 4mg 5d a ag also am an and an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>[53100, 2851, 07054, 5715, 45621, 53789, 4019,...</td>\n",
       "      <td>54610</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2150-4-17**]              ...</td>\n",
       "      <td>Mr Known is a 59M w HepC cirrhosis c b grade ...</td>\n",
       "      <td>[40mg, 4l, 59m, a, abdominal, about, abstain, ...</td>\n",
       "      <td>40mg 4l 59m a abdominal about abstain admissio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100006</td>\n",
       "      <td>[49320, 51881, 486, 20300, 2761, 7850, 3090, V...</td>\n",
       "      <td>9895</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2108-4-6**]       Discharg...</td>\n",
       "      <td>This is a 48 year old African American female...</td>\n",
       "      <td>[a, abdominal, abg, admitted, african, ago, al...</td>\n",
       "      <td>a abdominal abg admitted african ago albuterol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100007</td>\n",
       "      <td>[56081, 5570, 9973, 486, 4019]</td>\n",
       "      <td>23018</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2145-3-31**]              ...</td>\n",
       "      <td>Ms Known is a 73 year old female with a histo...</td>\n",
       "      <td>[a, abdominal, and, appendectomy, back, began,...</td>\n",
       "      <td>a abdominal and appendectomy back began bowel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100009</td>\n",
       "      <td>[41401, 99604, 4142, 25000, 27800, V8535, 4148...</td>\n",
       "      <td>533</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2162-5-16**]              ...</td>\n",
       "      <td>60yo man with known coronary disease AMI in 2...</td>\n",
       "      <td>[36mmhg, 60yo, a, admitted, ak, ami, and, angi...</td>\n",
       "      <td>36mmhg 60yo a admitted ak ami and angina anter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                          ICD9_CODE  SUBJECT_ID  \\\n",
       "0   100001  [25013, 3371, 5849, 5780, V5867, 25063, 5363, ...       58526   \n",
       "1   100003  [53100, 2851, 07054, 5715, 45621, 53789, 4019,...       54610   \n",
       "2   100006  [49320, 51881, 486, 20300, 2761, 7850, 3090, V...        9895   \n",
       "3   100007                     [56081, 5570, 9973, 486, 4019]       23018   \n",
       "4   100009  [41401, 99604, 4142, 25000, 27800, V8535, 4148...         533   \n",
       "\n",
       "            CATEGORY DESCRIPTION  \\\n",
       "0  Discharge summary      Report   \n",
       "1  Discharge summary      Report   \n",
       "2  Discharge summary      Report   \n",
       "3  Discharge summary      Report   \n",
       "4  Discharge summary      Report   \n",
       "\n",
       "                                                TEXT  \\\n",
       "0  Admission Date:  [**2117-9-11**]              ...   \n",
       "1  Admission Date:  [**2150-4-17**]              ...   \n",
       "2  Admission Date:  [**2108-4-6**]       Discharg...   \n",
       "3  Admission Date:  [**2145-3-31**]              ...   \n",
       "4  Admission Date:  [**2162-5-16**]              ...   \n",
       "\n",
       "                                                HOPI  \\\n",
       "0   35F w poorly controlled Type 1 diabetes melli...   \n",
       "1   Mr Known is a 59M w HepC cirrhosis c b grade ...   \n",
       "2   This is a 48 year old African American female...   \n",
       "3   Ms Known is a 73 year old female with a histo...   \n",
       "4   60yo man with known coronary disease AMI in 2...   \n",
       "\n",
       "                                         SENT_TOKENS  \\\n",
       "0  [325mg, 35f, 3l, 3rd, 4mg, 5d, a, ag, also, am...   \n",
       "1  [40mg, 4l, 59m, a, abdominal, about, abstain, ...   \n",
       "2  [a, abdominal, abg, admitted, african, ago, al...   \n",
       "3  [a, abdominal, and, appendectomy, back, began,...   \n",
       "4  [36mmhg, 60yo, a, admitted, ak, ami, and, angi...   \n",
       "\n",
       "                                   SENT_TOKENS_COMBO  \n",
       "0  325mg 35f 3l 3rd 4mg 5d a ag also am an and an...  \n",
       "1  40mg 4l 59m a abdominal about abstain admissio...  \n",
       "2  a abdominal abg admitted african ago albuterol...  \n",
       "3  a abdominal and appendectomy back began bowel ...  \n",
       "4  36mmhg 60yo a admitted ak ami and angina anter...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truncate to 500 tokens per HOPI\n",
    "df[\"SENT_TOKENS\"] = df[\"SENT_TOKENS\"].map(lambda c: np.unique(c)[0:500])\n",
    "\n",
    "df['SENT_TOKENS_COMBO'] = df[\"SENT_TOKENS\"].map(lambda t: \" \".join(t))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "194ab1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=200, alpha=0.025>', 'datetime': '2022-11-26T21:29:37.180194', 'gensim': '4.2.0', 'python': '3.8.15 (default, Nov 10 2022, 13:17:42) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 1408781 words, keeping 57207 word types\n",
      "PROGRESS: at sentence #20000, processed 2815360 words, keeping 88177 word types\n",
      "PROGRESS: at sentence #30000, processed 4222394 words, keeping 113580 word types\n",
      "PROGRESS: at sentence #40000, processed 5641322 words, keeping 136651 word types\n",
      "PROGRESS: at sentence #50000, processed 7042111 words, keeping 157550 word types\n",
      "collected 164182 word types from a corpus of 7512332 raw words and 53305 sentences\n",
      "Creating a fresh vocabulary\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 27703 unique words (16.87% of original 164182, drops 136479)', 'datetime': '2022-11-26T21:29:38.196652', 'gensim': '4.2.0', 'python': '3.8.15 (default, Nov 10 2022, 13:17:42) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 7327702 word corpus (97.54% of original 7512332, drops 184630)', 'datetime': '2022-11-26T21:29:38.197102', 'gensim': '4.2.0', 'python': '3.8.15 (default, Nov 10 2022, 13:17:42) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "deleting the raw counts dictionary of 164182 items\n",
      "sample=0.001 downsamples 36 most-common words\n",
      "Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 7011720.183838595 word corpus (95.7%% of prior 7327702)', 'datetime': '2022-11-26T21:29:38.290807', 'gensim': '4.2.0', 'python': '3.8.15 (default, Nov 10 2022, 13:17:42) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "estimated required memory for 27703 words and 200 dimensions: 58176300 bytes\n",
      "resetting layer weights\n",
      "Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-11-26T21:29:38.482898', 'gensim': '4.2.0', 'python': '3.8.15 (default, Nov 10 2022, 13:17:42) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'training model with 4 workers on 27703 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-11-26T21:29:38.483500', 'gensim': '4.2.0', 'python': '3.8.15 (default, Nov 10 2022, 13:17:42) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "EPOCH 0 - PROGRESS: at 31.75% examples, 2223908 words/s, in_qsize 8, out_qsize 0\n",
      "EPOCH 0 - PROGRESS: at 64.41% examples, 2257649 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 0 - PROGRESS: at 97.65% examples, 2278504 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 0: training on 7512332 raw words (7011618 effective words) took 3.1s, 2283807 effective words/s\n",
      "EPOCH 1 - PROGRESS: at 32.79% examples, 2276998 words/s, in_qsize 8, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 65.46% examples, 2278173 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 98.06% examples, 2279056 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 1: training on 7512332 raw words (7010830 effective words) took 3.1s, 2280130 effective words/s\n",
      "EPOCH 2 - PROGRESS: at 32.37% examples, 2269213 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 65.20% examples, 2284871 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 97.79% examples, 2281906 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 2: training on 7512332 raw words (7011027 effective words) took 3.1s, 2285027 effective words/s\n",
      "EPOCH 3 - PROGRESS: at 31.88% examples, 2231724 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 64.66% examples, 2264224 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 97.65% examples, 2273026 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 3: training on 7512332 raw words (7011264 effective words) took 3.1s, 2277161 effective words/s\n",
      "EPOCH 4 - PROGRESS: at 32.12% examples, 2247547 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 4 - PROGRESS: at 61.56% examples, 2153067 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 4 - PROGRESS: at 86.95% examples, 2029481 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 4: training on 7512332 raw words (7011566 effective words) took 3.4s, 2048770 effective words/s\n",
      "Word2Vec lifecycle event {'msg': 'training on 37561660 raw words (35056305 effective words) took 15.7s, 2229377 effective words/s', 'datetime': '2022-11-26T21:29:54.208365', 'gensim': '4.2.0', 'python': '3.8.15 (default, Nov 10 2022, 13:17:42) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "Word2Vec lifecycle event {'fname_or_handle': './model/model.w2v', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-11-26T21:29:54.209004', 'gensim': '4.2.0', 'python': '3.8.15 (default, Nov 10 2022, 13:17:42) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}\n",
      "not storing attribute cum_table\n",
      "saved ./model/model.w2v\n"
     ]
    }
   ],
   "source": [
    "sentences = df['SENT_TOKENS_COMBO'].map(lambda t: t.split()).values\n",
    "\n",
    "model = w2v.Word2Vec(vector_size=200, min_count=5, workers=4, epochs=5)\n",
    "\n",
    "model.build_vocab(sentences)\n",
    "\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "model.save('./model/model.w2v')\n",
    "\n",
    "wv = model.wv\n",
    "\n",
    "vocab = model.wv.key_to_index  \n",
    "\n",
    "ind2w = {i+1:w for i,w in enumerate(sorted(vocab))}\n",
    "w2ind = {w:i for i,w in ind2w.items()}\n",
    "\n",
    "PAD_CHAR = \"**PAD**\"\n",
    "\n",
    "def build_matrix(ind2w, wv):\n",
    "    W = np.zeros((len(ind2w)+1, len(wv.get_vector(wv.index_to_key[0])) ))\n",
    "    words = [PAD_CHAR]\n",
    "    W[0][:] = np.zeros(len(wv.get_vector(wv.index_to_key[0])))\n",
    "    for idx, word in ind2w.items():\n",
    "        if idx >= W.shape[0]:\n",
    "            break    \n",
    "        W[idx][:] = wv.get_vector(word)\n",
    "        words.append(word)\n",
    "    return W, words\n",
    "\n",
    "W, words = build_matrix(ind2w, wv)\n",
    "\n",
    "def save_embeddings(W, words, outfile):\n",
    "    with open(outfile, 'w') as o:\n",
    "        #pad token already included\n",
    "        for i in range(len(words)):\n",
    "            line = [words[i]]\n",
    "            line.extend([str(d) for d in W[i]])\n",
    "            o.write(\" \".join(line) + \"\\n\")\n",
    "\n",
    "outfile = './model/model.embed'\n",
    "save_embeddings(W, words, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4c34972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embed_file):\n",
    "    #also normalizes the embeddings\n",
    "    W = []\n",
    "    with open(embed_file) as ef:\n",
    "        for line in ef:\n",
    "            line = line.rstrip().split()\n",
    "            vec = np.array(line[1:]).astype(float)\n",
    "            vec = vec / float(np.linalg.norm(vec) + 1e-6)\n",
    "            W.append(vec)\n",
    "        vec = np.random.randn(len(W[-1]))\n",
    "        vec = vec / float(np.linalg.norm(vec) + 1e-6)\n",
    "        W.append(vec)\n",
    "    W = np.array(W)\n",
    "    return W\n",
    "\n",
    "loaded_embed = torch.Tensor(load_embeddings(outfile))\n",
    "\n",
    "embed = nn.Embedding(loaded_embed.size()[0], loaded_embed.size()[1], padding_idx=0)\n",
    "embed.weight.data = loaded_embed.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c110107a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>HOPI</th>\n",
       "      <th>SENT_TOKENS</th>\n",
       "      <th>SENT_TOKENS_COMBO</th>\n",
       "      <th>idx_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>[25013, 3371, 5849, 5780, V5867, 25063, 5363, ...</td>\n",
       "      <td>58526</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2117-9-11**]              ...</td>\n",
       "      <td>35F w poorly controlled Type 1 diabetes melli...</td>\n",
       "      <td>[325mg, 35f, 3l, 3rd, 4mg, 5d, a, ag, also, am...</td>\n",
       "      <td>325mg 35f 3l 3rd 4mg 5d a ag also am an and an...</td>\n",
       "      <td>[631, 657, 712, 730, 872, 1003, 1443, 2090, 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>[53100, 2851, 07054, 5715, 45621, 53789, 4019,...</td>\n",
       "      <td>54610</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2150-4-17**]              ...</td>\n",
       "      <td>Mr Known is a 59M w HepC cirrhosis c b grade ...</td>\n",
       "      <td>[40mg, 4l, 59m, a, abdominal, about, abstain, ...</td>\n",
       "      <td>40mg 4l 59m a abdominal about abstain admissio...</td>\n",
       "      <td>[763, 863, 992, 1443, 1592, 1641, 27704, 1973,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100006</td>\n",
       "      <td>[49320, 51881, 486, 20300, 2761, 7850, 3090, V...</td>\n",
       "      <td>9895</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2108-4-6**]       Discharg...</td>\n",
       "      <td>This is a 48 year old African American female...</td>\n",
       "      <td>[a, abdominal, abg, admitted, african, ago, al...</td>\n",
       "      <td>a abdominal abg admitted african ago albuterol...</td>\n",
       "      <td>[1443, 1592, 1606, 1985, 2072, 2126, 2220, 231...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100007</td>\n",
       "      <td>[56081, 5570, 9973, 486, 4019]</td>\n",
       "      <td>23018</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2145-3-31**]              ...</td>\n",
       "      <td>Ms Known is a 73 year old female with a histo...</td>\n",
       "      <td>[a, abdominal, and, appendectomy, back, began,...</td>\n",
       "      <td>a abdominal and appendectomy back began bowel ...</td>\n",
       "      <td>[1443, 1592, 2550, 2886, 3682, 3884, 4456, 561...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100009</td>\n",
       "      <td>[41401, 99604, 4142, 25000, 27800, V8535, 4148...</td>\n",
       "      <td>533</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2162-5-16**]              ...</td>\n",
       "      <td>60yo man with known coronary disease AMI in 2...</td>\n",
       "      <td>[36mmhg, 60yo, a, admitted, ak, ami, and, angi...</td>\n",
       "      <td>36mmhg 60yo a admitted ak ami and angina anter...</td>\n",
       "      <td>[671, 1078, 1443, 1985, 2188, 2390, 2550, 2611...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                          ICD9_CODE  SUBJECT_ID  \\\n",
       "0   100001  [25013, 3371, 5849, 5780, V5867, 25063, 5363, ...       58526   \n",
       "1   100003  [53100, 2851, 07054, 5715, 45621, 53789, 4019,...       54610   \n",
       "2   100006  [49320, 51881, 486, 20300, 2761, 7850, 3090, V...        9895   \n",
       "3   100007                     [56081, 5570, 9973, 486, 4019]       23018   \n",
       "4   100009  [41401, 99604, 4142, 25000, 27800, V8535, 4148...         533   \n",
       "\n",
       "            CATEGORY DESCRIPTION  \\\n",
       "0  Discharge summary      Report   \n",
       "1  Discharge summary      Report   \n",
       "2  Discharge summary      Report   \n",
       "3  Discharge summary      Report   \n",
       "4  Discharge summary      Report   \n",
       "\n",
       "                                                TEXT  \\\n",
       "0  Admission Date:  [**2117-9-11**]              ...   \n",
       "1  Admission Date:  [**2150-4-17**]              ...   \n",
       "2  Admission Date:  [**2108-4-6**]       Discharg...   \n",
       "3  Admission Date:  [**2145-3-31**]              ...   \n",
       "4  Admission Date:  [**2162-5-16**]              ...   \n",
       "\n",
       "                                                HOPI  \\\n",
       "0   35F w poorly controlled Type 1 diabetes melli...   \n",
       "1   Mr Known is a 59M w HepC cirrhosis c b grade ...   \n",
       "2   This is a 48 year old African American female...   \n",
       "3   Ms Known is a 73 year old female with a histo...   \n",
       "4   60yo man with known coronary disease AMI in 2...   \n",
       "\n",
       "                                         SENT_TOKENS  \\\n",
       "0  [325mg, 35f, 3l, 3rd, 4mg, 5d, a, ag, also, am...   \n",
       "1  [40mg, 4l, 59m, a, abdominal, about, abstain, ...   \n",
       "2  [a, abdominal, abg, admitted, african, ago, al...   \n",
       "3  [a, abdominal, and, appendectomy, back, began,...   \n",
       "4  [36mmhg, 60yo, a, admitted, ak, ami, and, angi...   \n",
       "\n",
       "                                   SENT_TOKENS_COMBO  \\\n",
       "0  325mg 35f 3l 3rd 4mg 5d a ag also am an and an...   \n",
       "1  40mg 4l 59m a abdominal about abstain admissio...   \n",
       "2  a abdominal abg admitted african ago albuterol...   \n",
       "3  a abdominal and appendectomy back began bowel ...   \n",
       "4  36mmhg 60yo a admitted ak ami and angina anter...   \n",
       "\n",
       "                                          idx_tokens  \n",
       "0  [631, 657, 712, 730, 872, 1003, 1443, 2090, 23...  \n",
       "1  [763, 863, 992, 1443, 1592, 1641, 27704, 1973,...  \n",
       "2  [1443, 1592, 1606, 1985, 2072, 2126, 2220, 231...  \n",
       "3  [1443, 1592, 2550, 2886, 3682, 3884, 4456, 561...  \n",
       "4  [671, 1078, 1443, 1985, 2188, 2390, 2550, 2611...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding indexes\n",
    "df[\"idx_tokens\"] = df['SENT_TOKENS'].map(lambda t: [int(w2ind[w]) if w in w2ind else len(w2ind)+1 for w in t])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b841b468",
   "metadata": {},
   "source": [
    "## Plot a histogram of the Number of tokens in each HoPI document, after data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b72313d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"trunc_len\"] = df['SENT_TOKENS'].map(lambda c: len(c))\n",
    "\n",
    "plt.hist(df[\"trunc_len\"], np.arange(0, 501, 10))\n",
    "plt.xlabel(\"Number of Tokens\")\n",
    "plt.ylabel(\"Number of Documents\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f07c2f",
   "metadata": {},
   "source": [
    "## Split data in train, valid, and test sets\n",
    "### training (38,588 records, 69.9%), validation (5536 records, 10.0%) and testing (11,048 records, 20.0%) folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9cc17eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training: 37313, length of test: 10714, & length of valid: 5278\n"
     ]
    }
   ],
   "source": [
    "# Random split\n",
    "train, tst = sklearn.model_selection.train_test_split(df, test_size= 0.3, random_state=42)\n",
    "\n",
    "test, valid = sklearn.model_selection.train_test_split(tst, test_size= 0.33, random_state=42)\n",
    "\n",
    "train = train.reset_index(drop = True).drop([\"CATEGORY\", \"DESCRIPTION\"], axis = 1)\n",
    "test = test.reset_index(drop = True).drop([\"CATEGORY\", \"DESCRIPTION\"], axis = 1)\n",
    "valid = valid.reset_index(drop = True).drop([\"CATEGORY\", \"DESCRIPTION\"], axis = 1)\n",
    "\n",
    "print(f\"Length of training: {len(train)}, length of test: {len(test)}, & length of valid: {len(valid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6f8d07",
   "metadata": {},
   "source": [
    "## Count number of tokens in the training dataset (n = ~92,468 tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "790c81fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22744\n"
     ]
    }
   ],
   "source": [
    "# Count occurence of tokens that are in the training dataset\n",
    "n = len(np.unique(np.concatenate(train['SENT_TOKENS'].values)))\n",
    "occ = Counter(np.concatenate(train['SENT_TOKENS'].values))\n",
    "            \n",
    "# Tokens that occur >= 5 times are in the study vocabulary (should be ~19,503)\n",
    "vocab = [k for k,v in occ.items() if v >= 5]\n",
    "print(len(vocab))\n",
    "\n",
    "# Assign a unique integer ID for each token in the study vocabulary \n",
    "vocab_lookup = dict(zip(vocab, np.arange(0, len(vocab), 1)))\n",
    "\n",
    "# Convert each HoPI document to a 1D array of integers using this index\n",
    "train[\"trunc_idx\"] =  train['SENT_TOKENS'].map(lambda x: [vocab_lookup.get(i) for i in x if i in vocab_lookup.keys()])\n",
    "valid[\"trunc_idx\"] =  valid['SENT_TOKENS'].map(lambda x: [vocab_lookup.get(i) for i in x if i in vocab_lookup.keys()])\n",
    "test[\"trunc_idx\"] =  test['SENT_TOKENS'].map(lambda x: [vocab_lookup.get(i) for i in x if i in vocab_lookup.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b180a12e",
   "metadata": {},
   "source": [
    "# Document representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04e11ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent clinical notes documents as TF-IDF weights\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def tfidf(df):\n",
    "    pipe = Pipeline([('count', CountVectorizer(vocabulary = vocab, lowercase = False)),('tfid', TfidfTransformer(use_idf=True))]).fit(df[\"HOPI\"].values)\n",
    "    pipe['count'].transform(df[\"HOPI\"].values).toarray()\n",
    "    pipe['tfid'].idf_\n",
    "\n",
    "    return pipe.transform(df[\"HOPI\"].values)\n",
    "\n",
    "# Create sparse matrix of size number of docs x words in vocab\n",
    "tfidf_train = tfidf(train)\n",
    "tfidf_valid = tfidf(valid)\n",
    "tfidf_test = tfidf(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e46938b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Document embedding representation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m xemb_train \u001b[39m=\u001b[39m train[\u001b[39m'\u001b[39m\u001b[39midx_tokens\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m t: [np\u001b[39m.\u001b[39marray(embed\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdata[w]) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m t])\n\u001b[1;32m      3\u001b[0m xemb_valid \u001b[39m=\u001b[39m valid[\u001b[39m'\u001b[39m\u001b[39midx_tokens\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m t: [np\u001b[39m.\u001b[39marray(embed\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdata[w]) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m t])\n\u001b[1;32m      4\u001b[0m xemb_test \u001b[39m=\u001b[39m test[\u001b[39m'\u001b[39m\u001b[39midx_tokens\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m t: [np\u001b[39m.\u001b[39marray(embed\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdata[w]) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m t])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# Document embedding representation\n",
    "xemb_train = train['idx_tokens'].map(lambda t: [np.array(embed.weight.data[w]) for w in t])\n",
    "xemb_valid = valid['idx_tokens'].map(lambda t: [np.array(embed.weight.data[w]) for w in t])\n",
    "xemb_test = test['idx_tokens'].map(lambda t: [np.array(embed.weight.data[w]) for w in t])\n",
    "\n",
    "# Document mean embedding representation\n",
    "xmean_train = train['idx_tokens'].map(lambda t: np.average([np.array(embed.weight.data[w]) for w in t], axis = 0))\n",
    "xmean_valid = valid['idx_tokens'].map(lambda t: np.average([np.array(embed.weight.data[w]) for w in t], axis = 0))\n",
    "xmean_test = test['idx_tokens'].map(lambda t: np.average([np.array(embed.weight.data[w]) for w in t], axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b50a05ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU representation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb2746c",
   "metadata": {},
   "source": [
    "# Label representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c0afc9",
   "metadata": {},
   "source": [
    "### Identify the hierarchical labels for each icd9 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7305717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out the main ICD9 code (they are ordered for importance)\n",
    "train[\"ICD_Main\"] = train['ICD9_CODE'].map(lambda x: x[0])\n",
    "valid[\"ICD_Main\"] = valid['ICD9_CODE'].map(lambda x: x[0])\n",
    "test[\"ICD_Main\"] = test['ICD9_CODE'].map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9714dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pyhealth library to find code hierarchy for the train, test, and valid datasets\n",
    "icd9cm = InnerMap.load(\"ICD9CM\")\n",
    "\n",
    "def define_levels(x):\n",
    "    lvls = []\n",
    "    for l in x: \n",
    "        lvls.append(icd9cm.lookup(l))\n",
    "    return lvls\n",
    "\n",
    "def drop_top(X):\n",
    "    if '001-999.99' in X:\n",
    "        X.remove('001-999.99')\n",
    "    return X\n",
    "\n",
    "def get_codes(df):\n",
    "    #df['icd_desc'] = df['ICD_Main'].map(lambda x: icd9cm.lookup(x))\n",
    "    df['levels'] = df['ICD_Main'].map(lambda x:icd9cm.get_ancestors(x)[::-1])\n",
    "    df['levels'].map(lambda x: drop_top(x))\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        df['levels'][i].append(df['ICD_Main'][i])\n",
    "\n",
    "    df['levels_desc'] = df['levels'].map(lambda x: define_levels(x))\n",
    "\n",
    "get_codes(train)\n",
    "get_codes(valid)\n",
    "get_codes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65bc18b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the code descriptions to the dfs\n",
    "train_labs = pd.concat([train, train['levels_desc'].apply(pd.Series)], axis = 1).drop(\"levels_desc\", axis = 1)\\\n",
    "    .rename(columns={0: \"y_l1\", 1: \"y_l2\", 2: \"y_l3\", 3: \"y_l4\", 4:\"y_l5\", 5:\"y_l6\"})\n",
    "valid_labs = pd.concat([valid, valid['levels_desc'].apply(pd.Series)], axis = 1).drop(\"levels_desc\", axis = 1)\\\n",
    "    .rename(columns={0: \"y_l1\", 1: \"y_l2\", 2: \"y_l3\", 3: \"y_l4\", 4:\"y_l5\", 5:\"y_l6\"})\n",
    "test_labs = pd.concat([test, test['levels_desc'].apply(pd.Series)], axis = 1).drop(\"levels_desc\", axis = 1)\\\n",
    "    .rename(columns={0: \"y_l1\", 1: \"y_l2\", 2: \"y_l3\", 3: \"y_l4\" , 4:\"y_l5\", 5:\"y_l6\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e04392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean representation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3524d3",
   "metadata": {},
   "source": [
    "## Baseline Model: TFIDF-Atomic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00750564",
   "metadata": {},
   "source": [
    "#### TFIDF document weights were input into a multinomial logistic regression classifier in order to predict the 4 levels of icd hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0df4d0",
   "metadata": {},
   "source": [
    "### Predict level 1 icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "32b285c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress: 100.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6668932390234398,\n",
       " 'recall': 0.6768713832368863,\n",
       " 'f1-score': 0.6521999221367677,\n",
       " 'support': 10714}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multinomial Logistic Regression from sklearn \n",
    "\n",
    "# L2 values to iterate over\n",
    "L2 = np.logspace(.01, 100, num = 50)\n",
    "L2 = [1.023292992280754]\n",
    "\n",
    "# # Enumerate the l1 classes\n",
    "y_l1_dict = dict(zip(np.unique(train_labs['y_l1'].values), np.arange(0, len(np.unique(train_labs['y_l1'].values)), 1)))\n",
    "y_l1_trn = train_labs['y_l1'].map(lambda x: y_l1_dict.get(x))\n",
    "y_l1_val = valid_labs['y_l1'].map(lambda x: y_l1_dict.get(x))\n",
    "y_l1_tst = test_labs['y_l1'].map(lambda x: y_l1_dict.get(x))\n",
    "\n",
    "def log_reg_val(y_trn, y_val, x_trn, x_val, L2):\n",
    "    \n",
    "    # Use the validation data to determine the best model parameter for C\n",
    "    scores = []\n",
    "    i = 1\n",
    "    for C in L2: \n",
    "        if len(L2)%(i) == 0:\n",
    "            print(f\"Validation Progress: {i/len(L2)*100} %\")\n",
    "\n",
    "        tfidf_atomic = LogisticRegression(multi_class = 'multinomial', penalty = 'l2', max_iter = 1500, solver = \"sag\", random_state = 42, C = C)\n",
    "        tfidf_atomic_L1 = tfidf_atomic.fit(x_trn, y_trn)\n",
    "        y_v_pred = tfidf_atomic_L1.predict(x_val)\n",
    "        report = metrics.classification_report(y_val, y_v_pred, digits = 3, output_dict=True)\n",
    "        scores.append((C, report['weighted avg']['recall']))\n",
    "\n",
    "    # C value for best model on valid data\n",
    "    best_C = max(scores,key=lambda x:x[1])[0]\n",
    "    return best_C\n",
    "\n",
    "best_C = log_reg_val(x_trn = tfidf_train, y_trn = y_l1_trn, L2 = L2, y_val = y_l1_val, x_val = tfidf_valid)\n",
    "\n",
    "# Get results on test data for best model\n",
    "tfidf_atomic = LogisticRegression(multi_class = 'multinomial', penalty = 'l2', max_iter = 1500, solver = \"sag\", random_state = 42, C = best_C)\n",
    "tfidf_atomic_l1 = tfidf_atomic.fit(tfidf_train, y_l1_trn)\n",
    "y_pred = tfidf_atomic_l1.predict(tfidf_test)\n",
    "L1_test_report = metrics.classification_report(y_l1_tst, y_pred, digits = 3, output_dict=True)\n",
    "L1_test_report['weighted avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23aa8ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C for tfidf tier1: 1.023292992280754\n"
     ]
    }
   ],
   "source": [
    "#BEST C Found for tfidf tier1: 1.023292992280754\n",
    "print(f'best C for tfidf tier1: {best_C}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0860306",
   "metadata": {},
   "source": [
    "### Predict level 2 icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5f8d7971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress: 100.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.529786295283573,\n",
       " 'recall': 0.5524227429745122,\n",
       " 'f1-score': 0.513553821091097,\n",
       " 'support': 10711}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multinomial Logistic Regression from sklearn \n",
    "\n",
    "# L2 values to iterate over; in order to run all iterations uncomment the line below\n",
    "#L2 = np.logspace(.01, 100, num = 50)\n",
    "L2 = [1.023292992280754] # This was the best C value out of the 50\n",
    "\n",
    "# Enumerate the l2 classes\n",
    "y_l2_dict = dict(zip(np.unique(train_labs['y_l2'].values), np.arange(0, len(np.unique(train_labs['y_l2'].values)), 1)))\n",
    "y_l2_trn = train_labs['y_l2'].map(lambda x: y_l2_dict.get(x)).fillna('Missing')\n",
    "y_l2_val = valid_labs['y_l2'].map(lambda x: y_l2_dict.get(x)).fillna('Missing')\n",
    "y_l2_tst = test_labs['y_l2'].map(lambda x: y_l2_dict.get(x)).fillna('Missing')\n",
    "\n",
    "# Only include lvl2 labels that are in the training set\n",
    "tst_miss = [y_l2_tst != \"Missing\"]\n",
    "tst_idx_2 = np.where(y_l2_tst != 'Missing')\n",
    "y_l2_tst = np.array(y_l2_tst[y_l2_tst != \"Missing\"]).astype(int)\n",
    "\n",
    "val_miss = [y_l2_val != \"Missing\"]\n",
    "val_idx_2 = np.where(y_l2_val != 'Missing')\n",
    "y_l2_val = np.array(y_l2_val[y_l2_val != \"Missing\"]).astype(int)\n",
    "\n",
    "best_C = log_reg_val(x_trn = tfidf_train, y_trn = y_l2_trn, L2 = L2, y_val = y_l2_val, x_val = tfidf_valid[list(val_idx_2[0]), :])\n",
    "\n",
    "# Get results on test data for best model\n",
    "tfidf_atomic = LogisticRegression(multi_class = 'multinomial', penalty = 'l2', max_iter = 1500, solver = \"sag\", random_state = 42, C = best_C)\n",
    "tfidf_atomic_l2 = tfidf_atomic.fit(tfidf_train, y_l2_trn)\n",
    "y_pred = tfidf_atomic_l2.predict(tfidf_test[list(tst_idx_2[0]), :])\n",
    "L2_test_report = metrics.classification_report(y_l2_tst, y_pred, digits = 3, output_dict=True)\n",
    "L2_test_report['weighted avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7fa5e732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C for tfidf tier2: 1.023292992280754\n"
     ]
    }
   ],
   "source": [
    "# BEST C Found for tfidf tier2: 1.023292992280754\n",
    "print(f'best C for tfidf tier2: {best_C}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d4d31",
   "metadata": {},
   "source": [
    "### Predict level 3 icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "22270f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress: 100.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.39221832038164023,\n",
       " 'recall': 0.44016493299597037,\n",
       " 'f1-score': 0.38074887974566596,\n",
       " 'support': 10671}"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multinomial Logistic Regression from sklearn \n",
    "\n",
    "# L2 values to iterate over; in order to run all iterations uncomment the line below\n",
    "# L2 = np.logspace(.01, 100, num = 50)\n",
    "L2 = [1.023292992280754]\n",
    "\n",
    "# Enumerate the l3 classes\n",
    "train_miss = train_labs['y_l3'].isna()\n",
    "train_labs_l3 = train_labs[~train_miss]\n",
    "y_l3_dict = dict(zip(set(train_labs_l3['y_l3'].values), np.arange(0, len(set(train_labs_l3['y_l3'].values)), 1)))\n",
    "\n",
    "y_l3_trn = train_labs_l3['y_l3'].map(lambda x: y_l3_dict.get(x)).fillna('Missing')\n",
    "y_l3_val = valid_labs['y_l3'].map(lambda x: y_l3_dict.get(x)).fillna('Missing')\n",
    "y_l3_tst = test_labs['y_l3'].map(lambda x: y_l3_dict.get(x)).fillna('Missing')\n",
    "\n",
    "# Only include lvl2 labels that are in the training set\n",
    "tst_miss = [y_l3_tst != \"Missing\"]\n",
    "tst_idx_3 = np.where(y_l3_tst != 'Missing')\n",
    "y_l3_tst = np.array(y_l3_tst[y_l3_tst != \"Missing\"]).astype(int)\n",
    "\n",
    "val_miss = [y_l3_val != \"Missing\"]\n",
    "val_idx_3 = np.where(y_l3_val != 'Missing')\n",
    "y_l3_val = np.array(y_l3_val[y_l3_val != \"Missing\"]).astype(int)\n",
    "\n",
    "best_C = log_reg_val(x_trn = tfidf_train[~train_miss,:], y_trn = y_l3_trn, L2 = L2, y_val = y_l3_val, x_val = tfidf_valid[list(val_idx_3[0]), :])\n",
    "\n",
    "# Get results on test data for best model\n",
    "tfidf_atomic = LogisticRegression(multi_class = 'multinomial', penalty = 'l2', max_iter = 1500, solver = \"sag\", random_state = 42, C = best_C)\n",
    "tfidf_atomic_l3 = tfidf_atomic.fit(tfidf_train[~train_miss,:], y_l3_trn)\n",
    "y_pred = tfidf_atomic_l3.predict(tfidf_test[list(tst_idx_3[0]), :])\n",
    "L3_test_report = metrics.classification_report(y_l3_tst, y_pred, digits = 3, output_dict=True)\n",
    "L3_test_report['weighted avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c40b11df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C for tfidf tier3: 1.023292992280754\n"
     ]
    }
   ],
   "source": [
    "# BEST C Found for tfidf tier3:\n",
    "print(f'best C for tfidf tier3: {best_C}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0e964f",
   "metadata": {},
   "source": [
    "### Predict terminal icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "2ec8a3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Progress: 100.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.047441373457230776,\n",
       " 'recall': 0.12740434332988623,\n",
       " 'f1-score': 0.05233019613665538,\n",
       " 'support': 4835}"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multinomial Logistic Regression from sklearn \n",
    "\n",
    "# L2 values to iterate over; in order to run all iterations uncomment the line below\n",
    "# L2 = np.logspace(.01, 100, num = 10)\n",
    "L2 = [1.023292992280754]\n",
    "\n",
    "# Due to long training time; only predict a subset of the level 4 classes\n",
    "train_labs_l4 = train_labs[~train_labs['y_l4'].isna()]\n",
    "\n",
    "# Get top X number of classes in the training data\n",
    "train_top = train_labs_l4['y_l4'].value_counts()[0:32]\n",
    "\n",
    "# 16,869 data points after filtering for the top 32 classes \n",
    "train_labs_l4 = train_labs[train_labs['y_l4'].isin(list(train_top.index))]\n",
    "\n",
    "y_l4_dict = dict(zip(set(train_labs_l4['y_l4'].values), np.arange(0, len(set(train_labs_l4['y_l4'].values)), 1)))\n",
    "\n",
    "train_labs_l4 = train_labs_l4[train_labs['y_l4'].isin(list(train_top.index))]\n",
    "\n",
    "train_idx_4 = np.where(~train_labs_l4['y_l4'].isna())\n",
    "train_labs_l4 = train_labs_l4[~train_labs_l4['y_l4'].isna()]\n",
    "\n",
    "# Remove classes that are not in the dictionary\n",
    "y_l4_trn = train_labs_l4['y_l4'].map(lambda x: y_l4_dict.get(x)).fillna('Missing')\n",
    "y_l4_val = valid_labs['y_l4'].map(lambda x: y_l4_dict.get(x)).fillna('Missing')\n",
    "y_l4_tst = test_labs['y_l4'].map(lambda x: y_l4_dict.get(x)).fillna('Missing')\n",
    "\n",
    "# Only include labels that are in the training set\n",
    "tst_miss = [y_l4_tst != \"Missing\"]\n",
    "tst_idx_4 = np.where(y_l4_tst != 'Missing')\n",
    "y_l4_tst = np.array(y_l4_tst[y_l4_tst != \"Missing\"]).astype(int)\n",
    "\n",
    "# Only include labels that are in the training set\n",
    "val_miss = [y_l4_val != \"Missing\"]\n",
    "val_idx_4 = np.where(y_l4_val != 'Missing')\n",
    "y_l4_val = np.array(y_l4_val[y_l4_val != \"Missing\"]).astype(int)\n",
    "\n",
    "# Test different l2 regularization coefficiants on the validation dataset \n",
    "best_C = log_reg_val(x_trn = tfidf_train[train_idx_4[0],:], y_trn = y_l4_trn, L2 = L2, y_val = y_l4_val, x_val = tfidf_valid[list(val_idx_4[0]), :])\n",
    "\n",
    "# Get results on test data for best model\n",
    "tfidf_atomic = LogisticRegression(multi_class = 'multinomial', penalty = 'l2', max_iter = 1500, solver = \"sag\", random_state = 42, C = best_C)\n",
    "tfidf_atomic_l4 = tfidf_atomic.fit(tfidf_train[~train_idx_4[0],:], y_l4_trn)\n",
    "y_pred = tfidf_atomic_l4.predict(tfidf_test[tst_idx_4[0], :])\n",
    "L4_test_report = metrics.classification_report(y_l4_tst, y_pred, digits = 3, output_dict=True)\n",
    "L4_test_report['weighted avg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe63853",
   "metadata": {},
   "source": [
    "## Mean-atomic\n",
    "\n",
    "### The mean embedding representation for the documents was input into a tensorflow softmax classifier to represent the labels atomically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138db98d",
   "metadata": {},
   "source": [
    "### Precict level 1 icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c5433c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.544\n",
      "\n",
      " Test Precision: 0.553\n",
      "\n",
      " Test Recall: 0.582\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# number of features\n",
    "num_features = 200\n",
    "\n",
    "# number of target labels\n",
    "num_labels = len(np.unique(y_l1_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 5000\n",
    "\n",
    "def one_hot(y, c):\n",
    "    \n",
    "    # y--> label/ground truth.\n",
    "    # c--> Number of classes.\n",
    "    \n",
    "    # A zero matrix of size (m, c)\n",
    "    y_hot = np.zeros((len(y), c))\n",
    "    \n",
    "    # Putting 1 for column where the label is,\n",
    "    # Using multidimensional indexing.\n",
    "    y_hot[np.arange(len(y)), y] = 1\n",
    "    \n",
    "    return y_hot\n",
    "  \n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(xmean_train)['idx_tokens'].apply(pd.Series)))\n",
    "train_labels = one_hot(y_l1_trn, num_labels)\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(xmean_test)['idx_tokens'].apply(pd.Series)))\n",
    "test_labels = one_hot(y_l1_tst, num_labels)\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(xmean_valid)['idx_tokens'].apply(pd.Series)))\n",
    "valid_labels = one_hot(y_l1_val, num_labels)\n",
    "  \n",
    "def nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate):\n",
    "    # initialize a tensorflow graph\n",
    "    graph = tf.Graph()\n",
    "  \n",
    "    with graph.as_default():\n",
    "        \"\"\"\n",
    "        defining all the nodes\n",
    "        \"\"\"\n",
    "    \n",
    "        # Inputs\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, num_features))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "        # Variables.\n",
    "        weights = tf.Variable(tf.truncated_normal([num_features, num_labels]))\n",
    "        biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "        # Training computation.\n",
    "        logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                            labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "        # Optimizer.\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "        # Predictions for the training, validation, and test data.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "        test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "\n",
    "    # utility function to calculate F1, precision, recall\n",
    "    def sm_metrics(predictions, labels):\n",
    "        y_pred = np.argmax(predictions, axis=1)\n",
    "        y_labs = np.argmax(labels, axis=1)\n",
    "        f1 = f1_score(y_labs, y_pred, average = 'weighted')\n",
    "        prec = precision_score(y_labs, y_pred, average = 'weighted')\n",
    "        rec = recall_score(y_labs, y_pred, average = 'weighted')\n",
    "        return f1, prec, rec\n",
    "    \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        # initialize weights and biases\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"Initialized\")\n",
    "    \n",
    "        for step in range(num_steps):\n",
    "            # pick a randomized offset\n",
    "            offset = np.random.randint(0, train_labels.shape[0] - batch_size - 1)\n",
    "    \n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "            # Prepare the feed dict\n",
    "            feed_dict = {tf_train_dataset : batch_data,\n",
    "                        tf_train_labels : batch_labels}\n",
    "    \n",
    "            # run one step of computation\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction],\n",
    "                                            feed_dict=feed_dict)\n",
    "    \n",
    "        print(\"\\n Test F1 Score: {:.3f}\".format(\n",
    "            sm_metrics(test_prediction.eval(), test_labels)[0]))\n",
    "        print(\"\\n Test Precision: {:.3f}\".format(\n",
    "            sm_metrics(test_prediction.eval(), test_labels)[1]))\n",
    "        print(\"\\n Test Recall: {:.3f}\".format(\n",
    "            sm_metrics(test_prediction.eval(), test_labels)[2]))\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e521018",
   "metadata": {},
   "source": [
    "### Precict level 2 icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e85b0200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.362\n",
      "\n",
      " Test Precision: 0.360\n",
      "\n",
      " Test Recall: 0.413\n"
     ]
    }
   ],
   "source": [
    "# number of features\n",
    "num_features = 200\n",
    "\n",
    "# number of target labels\n",
    "num_labels = len(np.unique(y_l2_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 5000\n",
    "  \n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(xmean_train)['idx_tokens'].apply(pd.Series)))\n",
    "train_labels = one_hot(y_l2_trn, num_labels)\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(xmean_test[tst_idx_2[0]])['idx_tokens'].apply(pd.Series)))\n",
    "test_labels = one_hot(y_l2_tst, num_labels)\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(xmean_valid[val_idx_2[0]])['idx_tokens'].apply(pd.Series)))\n",
    "valid_labels = one_hot(y_l2_val, num_labels)\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7770d577",
   "metadata": {},
   "source": [
    "### Precict level 3 icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "06652bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.240\n",
      "\n",
      " Test Precision: 0.236\n",
      "\n",
      " Test Recall: 0.300\n"
     ]
    }
   ],
   "source": [
    "# number of features\n",
    "num_features = 200\n",
    "\n",
    "# number of target labels\n",
    "num_labels = len(np.unique(y_l3_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 5000\n",
    "\n",
    "train_idx = np.where(train_miss != True)\n",
    "  \n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(xmean_train[train_idx[0]])['idx_tokens'].apply(pd.Series)))\n",
    "train_labels = one_hot(y_l3_trn, num_labels)\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(xmean_test[tst_idx_3[0]])['idx_tokens'].apply(pd.Series)))\n",
    "test_labels = one_hot(y_l3_tst, num_labels)\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(xmean_valid[val_idx_3[0]])['idx_tokens'].apply(pd.Series)))\n",
    "valid_labels = one_hot(y_l3_val, num_labels)\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b2f6a",
   "metadata": {},
   "source": [
    "### Precict terminal icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "0ec8b1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      " Test F1 Score: 0.044\n",
      "\n",
      " Test Precision: 0.035\n",
      "\n",
      " Test Recall: 0.139\n"
     ]
    }
   ],
   "source": [
    "# number of features\n",
    "num_features = 200\n",
    "\n",
    "# number of target labels\n",
    "num_labels = len(np.unique(y_l4_trn))\n",
    "\n",
    "# learning rate (alpha)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "num_steps = 5000\n",
    "\n",
    "train_idx_4 = np.where(~train_labs_l4['y_l4'].isna())\n",
    "train_labs_l4 = train_labs_l4[~train_labs_l4['y_l4'].isna()]\n",
    "\n",
    "y_l4_dict = dict(zip(set(train_labs_l4['y_l4'].values), np.arange(0, len(set(train_labs_l4['y_l4'].values)), 1)))\n",
    "\n",
    "# Remove classes that are not in the dictionary\n",
    "y_l4_trn = train_labs_l4['y_l4'].map(lambda x: y_l4_dict.get(x)).fillna('Missing')\n",
    "y_l4_val = valid_labs['y_l4'].map(lambda x: y_l4_dict.get(x)).fillna('Missing')\n",
    "y_l4_tst = test_labs['y_l4'].map(lambda x: y_l4_dict.get(x)).fillna('Missing')\n",
    "\n",
    "# Only include labels that are in the training set\n",
    "tst_idx_4 = np.where(y_l4_tst != 'Missing')\n",
    "y_l4_tst = np.array(y_l4_tst[y_l4_tst != \"Missing\"]).astype(int)\n",
    "\n",
    "# Only include labels that are in the training set\n",
    "val_idx_4 = np.where(y_l4_val != 'Missing')\n",
    "y_l4_val = np.array(y_l4_val[y_l4_val != \"Missing\"]).astype(int)\n",
    "  \n",
    "# input data\n",
    "train_dataset = preprocessing.normalize(np.array(pd.DataFrame(xmean_train[train_idx_4[0]])['idx_tokens'].apply(pd.Series)))\n",
    "train_labels = one_hot(y_l4_trn, num_labels)\n",
    "test_dataset = preprocessing.normalize(np.array(pd.DataFrame(xmean_test[tst_idx_4[0]])['idx_tokens'].apply(pd.Series)))\n",
    "test_labels = one_hot(y_l4_tst, num_labels)\n",
    "valid_dataset = preprocessing.normalize(np.array(pd.DataFrame(xmean_valid[val_idx_4[0]])['idx_tokens'].apply(pd.Series)))\n",
    "valid_labels = one_hot(y_l4_val, num_labels)\n",
    "\n",
    "nn_softmax(train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels, num_features, batch_size, num_labels, num_steps, learning_rate)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38577f9d",
   "metadata": {},
   "source": [
    "## GRU(X) - Mean(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf12f7e",
   "metadata": {},
   "source": [
    "### Predict level 1 icd codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "80df396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean representation of labels "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e522d1",
   "metadata": {},
   "source": [
    "### Precict level 2 icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59806bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "368be6c1",
   "metadata": {},
   "source": [
    "### Precict level 3 icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc75561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60470b85",
   "metadata": {},
   "source": [
    "### Precict terminal icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb717276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57d4117a",
   "metadata": {},
   "source": [
    "## GRU(X) - atomic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6b7ce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU with softmax activation\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Variable(xemb_train, requires_grad = False)\n",
    "\n",
    "# Pad the word embeddings so that they are all the same length\n",
    "t = pad_sequences([torch.tensor(x) for x in df['idx_tokens']], padding='post')\n",
    "df['pad_idx'] = t.tolist()\n",
    "df['emb_tokens'] = df['pad_idx'].map(lambda t: [np.array(embed.weight.data[w]) for w in t])\n",
    "#df[\"stack_emb_tokens\"] = df['emb_tokens'].map(lambda t: torch.stack(t, dim = 0))\n",
    "\n",
    "\n",
    "#len(df['idx_tokens'])\n",
    "# Initialize bi-directional GRU (ref: https://gist.github.com/ceshine/bed2dadca48fe4fe4b4600ccce2fd6e1)\n",
    "#bi_grus = torch.nn.GRU(input_size = len(train), hidden_size = 1, num_layers = 1, batch_first = False, bidirectional = True)\n",
    "\n",
    "#bi_output, bi_hidden = bi_grus(xemb_train)\n",
    "\n",
    "\n",
    "# Using Gated Recurrent Unit\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n",
    "#     tf.keras.layers.Dense(6, activation='relu'),  \n",
    "#     tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "# model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "# num_epochs = 10\n",
    "# history_gru = model.fit(padded, y_train, epochs=num_epochs, validation_data=(testing_padded, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cb0855",
   "metadata": {},
   "source": [
    "## GRU(X) - GRU(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0d9062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0a69b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('Project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "6191cad5e48d799445899377cbbbb6ea318aa4fc4ab311ca83af078a3f071713"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
